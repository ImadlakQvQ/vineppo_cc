[2024-11-24 21:06:03,749] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-11-24 21:06:17,920] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-11-24 21:06:17,920] [INFO] [runner.py:568:main] cmd = /lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank --enable_each_rank_log=None src/treetune/main.py --configs configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet run_iteration_loop
[2024-11-24 21:06:20,919] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-11-24 21:06:22,063] [INFO] [launch.py:138:main] 0 EBVERSIONNCCL=2.18.3
[2024-11-24 21:06:22,064] [INFO] [launch.py:138:main] 0 EBROOTNCCL=/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/CUDA/gcccore/cuda12.2/nccl/2.18.3
[2024-11-24 21:06:22,064] [INFO] [launch.py:138:main] 0 EBDEVELNCCL=/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/CUDA/gcccore/cuda12.2/nccl/2.18.3/easybuild/x86-64-v3-CUDA-gcccore-cuda12.2-nccl-2.18.3-easybuild-devel
[2024-11-24 21:06:22,064] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-11-24 21:06:22,064] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-11-24 21:06:22,064] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-11-24 21:06:22,064] [INFO] [launch.py:163:main] dist_world_size=4
[2024-11-24 21:06:22,064] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-11-24 21:06:22,065] [INFO] [launch.py:253:main] process 3562372 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[2024-11-24 21:06:22,066] [INFO] [launch.py:253:main] process 3562373 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[2024-11-24 21:06:22,067] [INFO] [launch.py:253:main] process 3562374 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[2024-11-24 21:06:22,068] [INFO] [launch.py:253:main] process 3562375 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[INFO|main.py:59:3562375] 2024-11-24 21:06:23,624 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:59:3562372] 2024-11-24 21:06:23,624 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >> ----Config----
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >> {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "analyzers": [
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "inference_strategy": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_depth": 100,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "no_cache": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "temperature": 0.6,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_template": "{query}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "samples": 16,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_num_requests": 1024,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "reward_function": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "math_task": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "load_dataset_dict": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "use_original_format": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "math_reward_function",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "task": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "tokenizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "pretrained"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "type": "mc_value_prediction",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "vllm_server": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "swap_space": 24
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "actor_deepspeed_config": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "bf16": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "enabled": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "prescale_gradients": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "train_batch_size": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "per_device_batch_size": 16,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_num_iterations": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_depth": 100,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "no_cache": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "temperature": 0.6,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_template": "{query}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "samples": 5,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "inference_strategy": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_depth": 100,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "no_cache": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "temperature": 0.6,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_template": "{query}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "samples": 16,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_num_requests": 512,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_num_states": 256,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "reward_function": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "math_task": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "load_dataset_dict": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "use_original_format": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "math_reward_function",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "task": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "tokenizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "pretrained"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "vllm_server": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "swap_space": 24
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     ],
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "directory": "experiments",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "episode_generator": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "answer_prefix": null,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "append_bos_to_query": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "append_eos_to_response": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "fill_missing_episodes": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "inference_strategy": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "answer_extractor": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "node_key_name": "text",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "identity"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "guidance_llm": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "api_base": "none",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "api_key": "EMPTY",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "caching": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_retries": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "openai_vllm"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_depth": 100,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "no_cache": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "node_expander": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "model_context_size": 2047,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "program_kwargs": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_tokens": 1024,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "temperature": 0.6,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "top_p": 0.9
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "tokenizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "pretrained"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "efficient_iid"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "question_field": "query",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "samples": 8,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "type": "cot"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "initial_model_name_or_path": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "max_question_length": 1512,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "max_sequence_length": 2048,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "reward_function": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "math_task": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "type": "math_reward_function",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "task": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "load_dataset_dict": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "type": "gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "use_original_format": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "total_num_iterations": 500,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "answer_extractor": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "node_key_name": "full_text",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "guidance_llm": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "api_base": "none",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "api_key": "EMPTY",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "caching": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_retries": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "openai_vllm"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_depth": 100,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "no_cache": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "node_expander": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "model_context_size": 2047,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "program_kwargs": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_tokens": 1024,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "temperature": 0.6,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "top_p": 0.9
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "tokenizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "pretrained"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "efficient_iid"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "question_field": "query",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "question_template": "{query}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "samples": 9,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "type": "cot"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "vllm_server": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_num_seqs": 512,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "swap_space": 8
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "wait_until_memory_release": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "global_vars": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "debug_mode": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "dirs": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "data": "data",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "experiments": "experiments"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "seed": 2746318213
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "inference_pipelines": [
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "analyzers": [
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "task_performance"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             ],
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "dataset_portion": 1,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "dataset_split": "test",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "inference_strategy": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_key_name": "text",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "identity"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_depth": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "no_cache": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "temperature": 0.35,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "samples": 16,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "seed": 42,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "prompt_library": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "tree": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "expansion": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "task": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "analyzers": [
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "task_performance"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             ],
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "dataset_portion": 1,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "dataset_split": "validation",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "inference_strategy": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_key_name": "text",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "identity"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_depth": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "no_cache": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "temperature": 0.35,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "samples": 16,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "seed": 42,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "prompt_library": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "tree": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "expansion": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "task": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "analyzers": [
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "task_performance"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             ],
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "dataset_split": "train",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "inference_strategy": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_key_name": "text",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "identity"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "max_depth": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "no_cache": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "temperature": 0.35,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "samples": 16,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "seed": 42,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "prompt_library": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "tree": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "expansion": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "task": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     ],
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "num_iterations": 500,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "prompt_library": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "tree": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "expansion": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "tokenizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "type": "pretrained"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "trainer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "actor_deepspeed_config": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "bf16": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "enabled": "auto"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "gradient_clipping": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "optimizer": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "params": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "betas": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "eps": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "lr": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "weight_decay": "auto"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "AdamW"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "scheduler": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "params": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "total_num_steps": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "train_batch_size": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "zero_optimization": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "allgather_partitions": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "contiguous_gradients": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "overlap_comm": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "reduce_scatter": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "stage": 0
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "actor_model": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "disable_dropout": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "pretrained_args": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "use_flash_attention_2": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "critic_model": null,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "general_training_args": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "bf16": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "gradient_checkpointing": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "learning_rate": 1e-06,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "logging_steps": 1,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "max_grad_norm": 1,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "save_steps": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "seed": 2746318213,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "target_train_batch_size": 64,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "weight_decay": 0
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "params": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "cliprange": 0.2,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "cliprange_value": 0.2,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "gamma": 1,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "lam": 1,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "temperature": 0.6,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "use_score_norm": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "use_score_scaling": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "whiten_advantages": true,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "whiten_rewards": false
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "reference_deepspeed_config": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "bf16": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "enabled": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "prescale_gradients": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "train_batch_size": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "wall_clock_breakdown": false
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "reference_model": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "pretrained_args": {
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>                 "use_flash_attention_2": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "report_entropy": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>         "type": "ppo"
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     },
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "type": "policy_iteration",
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >>     "use_deepspeed": true
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >> }
[INFO|main.py:60:3562375] 2024-11-24 21:06:23,624 >> --------------
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >> ----Config----
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >> {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "analyzers": [
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "inference_strategy": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_depth": 100,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "no_cache": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "temperature": 0.6,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_template": "{query}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "samples": 16,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_num_requests": 1024,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "reward_function": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "math_task": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "load_dataset_dict": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "use_original_format": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "math_reward_function",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "task": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "tokenizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "pretrained"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "type": "mc_value_prediction",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "vllm_server": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "swap_space": 24
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "actor_deepspeed_config": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "bf16": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "enabled": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "prescale_gradients": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "train_batch_size": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "per_device_batch_size": 16,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_num_iterations": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_depth": 100,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "no_cache": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "temperature": 0.6,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_template": "{query}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "samples": 5,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "inference_strategy": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_depth": 100,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "no_cache": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "temperature": 0.6,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_template": "{query}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "samples": 16,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_num_requests": 512,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_num_states": 256,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "reward_function": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "math_task": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "load_dataset_dict": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "use_original_format": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "math_reward_function",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "task": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "tokenizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "pretrained"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "vllm_server": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "swap_space": 24
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     ],
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "directory": "experiments",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "episode_generator": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "answer_prefix": null,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "append_bos_to_query": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "append_eos_to_response": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "fill_missing_episodes": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "inference_strategy": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "answer_extractor": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "node_key_name": "text",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "identity"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "guidance_llm": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "api_base": "none",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "api_key": "EMPTY",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "caching": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_retries": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "openai_vllm"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_depth": 100,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "no_cache": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "node_expander": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "model_context_size": 2047,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "program_kwargs": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_tokens": 1024,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "temperature": 0.6,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "top_p": 0.9
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "tokenizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "pretrained"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "efficient_iid"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "question_field": "query",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "samples": 8,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "type": "cot"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "initial_model_name_or_path": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "max_question_length": 1512,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "max_sequence_length": 2048,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "reward_function": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "math_task": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "type": "math_reward_function",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "task": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "load_dataset_dict": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "type": "gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "use_original_format": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "total_num_iterations": 500,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "answer_extractor": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "node_key_name": "full_text",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "guidance_llm": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "api_base": "none",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "api_key": "EMPTY",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "caching": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_retries": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "openai_vllm"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_depth": 100,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "no_cache": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "node_expander": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "model_context_size": 2047,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "program_kwargs": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_tokens": 1024,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "temperature": 0.6,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "top_p": 0.9
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "tokenizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "pretrained"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "efficient_iid"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "question_field": "query",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "question_template": "{query}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "samples": 9,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "type": "cot"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "vllm_server": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_num_seqs": 512,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "swap_space": 8
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "wait_until_memory_release": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "global_vars": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "debug_mode": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "dirs": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "data": "data",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "experiments": "experiments"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "seed": 2746318213
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "inference_pipelines": [
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "analyzers": [
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "task_performance"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             ],
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "dataset_portion": 1,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "dataset_split": "test",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "inference_strategy": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_key_name": "text",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "identity"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_depth": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "no_cache": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "temperature": 0.35,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "samples": 16,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "seed": 42,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "prompt_library": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "tree": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "expansion": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "task": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "analyzers": [
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "task_performance"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             ],
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "dataset_portion": 1,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "dataset_split": "validation",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "inference_strategy": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_key_name": "text",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "identity"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_depth": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "no_cache": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "temperature": 0.35,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "samples": 16,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "seed": 42,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "prompt_library": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "tree": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "expansion": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "task": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "analyzers": [
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "task_performance"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             ],
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "dataset_split": "train",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "inference_strategy": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "answer_extractor": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_key_name": "text",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "identity"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "guidance_llm": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_base": "none",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "caching": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "max_retries": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "max_depth": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "no_cache": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "node_expander": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "program_kwargs": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "temperature": 0.35,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "top_p": 0.9
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "tokenizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "type": "pretrained"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_field": "query",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "samples": 16,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "seed": 42,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "cot"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "prompt_library": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "tree": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "expansion": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "task": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "gsm8k",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "use_original_format": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     ],
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "num_iterations": 500,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "prompt_library": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "tree": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "expansion": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "tokenizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "type": "pretrained"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "trainer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "actor_deepspeed_config": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "bf16": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "enabled": "auto"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "gradient_clipping": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "optimizer": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "params": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "betas": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "eps": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "lr": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "weight_decay": "auto"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "AdamW"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "scheduler": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "params": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "total_num_steps": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "train_batch_size": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "zero_optimization": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "allgather_partitions": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "contiguous_gradients": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "overlap_comm": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "reduce_scatter": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "stage": 0
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "actor_model": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "disable_dropout": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "pretrained_args": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "use_flash_attention_2": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "critic_model": null,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "general_training_args": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "bf16": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "gradient_checkpointing": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "learning_rate": 1e-06,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "logging_steps": 1,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "max_grad_norm": 1,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "save_steps": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "seed": 2746318213,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "target_train_batch_size": 64,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "weight_decay": 0
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "params": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "cliprange": 0.2,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "cliprange_value": 0.2,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "gamma": 1,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "lam": 1,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "temperature": 0.6,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "use_score_norm": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "use_score_scaling": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "whiten_advantages": true,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "whiten_rewards": false
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "reference_deepspeed_config": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "bf16": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "enabled": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "prescale_gradients": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "train_batch_size": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "wall_clock_breakdown": false
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "reference_model": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "pretrained_args": {
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>                 "use_flash_attention_2": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "report_entropy": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>         "type": "ppo"
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     },
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "type": "policy_iteration",
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >>     "use_deepspeed": true
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >> }
[INFO|main.py:60:3562372] 2024-11-24 21:06:23,624 >> --------------
[INFO|main.py:59:3562373] 2024-11-24 21:06:23,625 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:59:3562374] 2024-11-24 21:06:23,625 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >> ----Config----
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >> {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "analyzers": [
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "inference_strategy": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_depth": 100,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "no_cache": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "temperature": 0.6,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_template": "{query}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "samples": 16,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_num_requests": 1024,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "reward_function": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "math_task": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "load_dataset_dict": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "use_original_format": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "math_reward_function",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "task": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "tokenizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "pretrained"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "type": "mc_value_prediction",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "vllm_server": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "swap_space": 24
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "actor_deepspeed_config": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "bf16": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "enabled": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "prescale_gradients": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "train_batch_size": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "per_device_batch_size": 16,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_num_iterations": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_depth": 100,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "no_cache": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "temperature": 0.6,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_template": "{query}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "samples": 5,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "inference_strategy": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_depth": 100,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "no_cache": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "temperature": 0.6,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_template": "{query}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "samples": 16,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_num_requests": 512,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_num_states": 256,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "reward_function": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "math_task": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "load_dataset_dict": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "use_original_format": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "math_reward_function",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "task": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "tokenizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "pretrained"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "vllm_server": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "swap_space": 24
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     ],
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "directory": "experiments",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "episode_generator": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "answer_prefix": null,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "append_bos_to_query": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "append_eos_to_response": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "fill_missing_episodes": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "inference_strategy": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "answer_extractor": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "node_key_name": "text",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "identity"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "guidance_llm": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "api_base": "none",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "api_key": "EMPTY",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "caching": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_retries": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "openai_vllm"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_depth": 100,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "no_cache": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "node_expander": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "model_context_size": 2047,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "program_kwargs": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_tokens": 1024,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "temperature": 0.6,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "top_p": 0.9
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "tokenizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "pretrained"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "efficient_iid"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "question_field": "query",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "samples": 8,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "type": "cot"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "initial_model_name_or_path": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "max_question_length": 1512,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "max_sequence_length": 2048,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "reward_function": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "math_task": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "type": "math_reward_function",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "task": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "load_dataset_dict": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "type": "gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "use_original_format": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "total_num_iterations": 500,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "answer_extractor": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "node_key_name": "full_text",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "guidance_llm": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "api_base": "none",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "api_key": "EMPTY",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "caching": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_retries": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "openai_vllm"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_depth": 100,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "no_cache": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "node_expander": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "model_context_size": 2047,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "program_kwargs": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_tokens": 1024,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "temperature": 0.6,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "top_p": 0.9
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "tokenizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "pretrained"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "efficient_iid"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "question_field": "query",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "question_template": "{query}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "samples": 9,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "type": "cot"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "vllm_server": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_num_seqs": 512,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "swap_space": 8
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "wait_until_memory_release": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "global_vars": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "debug_mode": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "dirs": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "data": "data",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "experiments": "experiments"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "seed": 2746318213
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "inference_pipelines": [
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "analyzers": [
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "task_performance"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             ],
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "dataset_portion": 1,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "dataset_split": "test",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "inference_strategy": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_key_name": "text",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "identity"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_depth": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "no_cache": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "temperature": 0.35,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "samples": 16,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "seed": 42,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "prompt_library": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "tree": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "expansion": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "task": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "analyzers": [
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "task_performance"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             ],
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "dataset_portion": 1,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "dataset_split": "validation",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "inference_strategy": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_key_name": "text",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "identity"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_depth": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "no_cache": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "temperature": 0.35,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "samples": 16,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "seed": 42,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "prompt_library": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "tree": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "expansion": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "task": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "analyzers": [
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "task_performance"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             ],
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "dataset_split": "train",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "inference_strategy": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_key_name": "text",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "identity"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "max_depth": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "no_cache": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "temperature": 0.35,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "samples": 16,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "seed": 42,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "prompt_library": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "tree": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "expansion": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "task": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     ],
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "num_iterations": 500,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "prompt_library": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "tree": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "expansion": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "tokenizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "type": "pretrained"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "trainer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "actor_deepspeed_config": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "bf16": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "enabled": "auto"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "gradient_clipping": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "optimizer": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "params": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "betas": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "eps": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "lr": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "weight_decay": "auto"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "AdamW"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "scheduler": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "params": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "total_num_steps": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "train_batch_size": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "zero_optimization": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "allgather_partitions": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "contiguous_gradients": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "overlap_comm": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "reduce_scatter": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "stage": 0
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "actor_model": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "disable_dropout": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "pretrained_args": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "use_flash_attention_2": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "critic_model": null,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "general_training_args": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "bf16": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "gradient_checkpointing": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "learning_rate": 1e-06,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "logging_steps": 1,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "max_grad_norm": 1,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "save_steps": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "seed": 2746318213,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "target_train_batch_size": 64,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "weight_decay": 0
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "params": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "cliprange": 0.2,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "cliprange_value": 0.2,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "gamma": 1,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "lam": 1,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "temperature": 0.6,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "use_score_norm": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "use_score_scaling": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "whiten_advantages": true,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "whiten_rewards": false
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "reference_deepspeed_config": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "bf16": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "enabled": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "prescale_gradients": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "train_batch_size": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "wall_clock_breakdown": false
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "reference_model": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "pretrained_args": {
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>                 "use_flash_attention_2": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "report_entropy": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>         "type": "ppo"
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     },
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "type": "policy_iteration",
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >>     "use_deepspeed": true
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >> }
[INFO|main.py:60:3562373] 2024-11-24 21:06:23,625 >> --------------
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >> ----Config----
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >> {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "analyzers": [
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "inference_strategy": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_depth": 100,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "no_cache": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "temperature": 0.6,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_template": "{query}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "samples": 16,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_num_requests": 1024,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "reward_function": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "math_task": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "load_dataset_dict": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "use_original_format": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "math_reward_function",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "task": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "tokenizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "pretrained"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "type": "mc_value_prediction",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "vllm_server": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "swap_space": 24
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "actor_deepspeed_config": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "bf16": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "enabled": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "prescale_gradients": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "train_batch_size": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "per_device_batch_size": 16,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_num_iterations": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_depth": 100,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "no_cache": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "temperature": 0.6,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_template": "{query}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "samples": 5,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "inference_strategy": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_key_name": "full_text",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_depth": 100,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "no_cache": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "temperature": 0.6,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_template": "{query}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "samples": 16,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_num_requests": 512,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_num_states": 256,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "reward_function": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "math_task": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "load_dataset_dict": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "use_original_format": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "math_reward_function",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "task": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "tokenizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "pretrained"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "vllm_server": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "swap_space": 24
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     ],
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "directory": "experiments",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "episode_generator": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "answer_prefix": null,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "append_bos_to_query": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "append_eos_to_response": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "fill_missing_episodes": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "inference_strategy": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "answer_extractor": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "node_key_name": "text",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "identity"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "guidance_llm": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "api_base": "none",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "api_key": "EMPTY",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "caching": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_retries": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "openai_vllm"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_depth": 100,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "no_cache": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "node_expander": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "model_context_size": 2047,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "program_kwargs": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_tokens": 1024,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "temperature": 0.6,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "top_p": 0.9
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "tokenizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "pretrained"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "efficient_iid"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "question_field": "query",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "samples": 8,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "type": "cot"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "initial_model_name_or_path": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "max_question_length": 1512,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "max_sequence_length": 2048,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "reward_function": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "math_task": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "type": "math_reward_function",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "task": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "load_dataset_dict": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "type": "gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "use_original_format": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "total_num_iterations": 500,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "answer_extractor": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "node_key_name": "full_text",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "guidance_llm": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "api_base": "none",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "api_key": "EMPTY",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "caching": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_retries": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "openai_vllm"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_depth": 100,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "no_cache": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "node_expander": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "model_context_size": 2047,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "program_kwargs": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_tokens": 1024,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "temperature": 0.6,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "top_p": 0.9
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "tokenizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "pretrained"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "efficient_iid"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "question_field": "query",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "question_template": "{query}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "samples": 9,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "type": "cot"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "vllm_server": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_num_seqs": 512,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "swap_space": 8
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "wait_until_memory_release": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "global_vars": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "debug_mode": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "dirs": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "data": "data",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "experiments": "experiments"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "seed": 2746318213
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "inference_pipelines": [
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "analyzers": [
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "task_performance"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             ],
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "dataset_portion": 1,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "dataset_split": "test",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "inference_strategy": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_key_name": "text",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "identity"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_depth": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "no_cache": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "temperature": 0.35,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "samples": 16,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "seed": 42,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "prompt_library": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "tree": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "expansion": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "task": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "analyzers": [
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "task_performance"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             ],
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "dataset_portion": 1,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "dataset_split": "validation",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "inference_strategy": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_key_name": "text",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "identity"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_depth": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "no_cache": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "temperature": 0.35,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "samples": 16,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "seed": 42,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "prompt_library": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "tree": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "expansion": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "task": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "analyzers": [
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "task_performance"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             ],
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "dataset_split": "train",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "inference_strategy": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "answer_extractor": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_key_name": "text",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "identity"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "guidance_llm": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_base": "none",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "api_key": "EMPTY",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "caching": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "max_retries": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "openai_vllm"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "max_depth": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "no_cache": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "node_expander": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "model_context_size": 2047,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "program_kwargs": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "max_tokens": 1024,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "temperature": 0.35,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "top_p": 0.9
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "tokenizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "type": "pretrained"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "type": "efficient_iid"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_field": "query",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "samples": 16,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "seed": 42,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "cot"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "prompt_library": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "tree": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "expansion": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "task": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "load_dataset_dict": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "gsm8k",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "use_original_format": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     ],
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "num_iterations": 500,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "prompt_library": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "tree": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "expansion": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "tokenizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "type": "pretrained"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "trainer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "actor_deepspeed_config": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "bf16": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "enabled": "auto"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "gradient_clipping": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "optimizer": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "params": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "betas": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "eps": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "lr": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "weight_decay": "auto"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "AdamW"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "scheduler": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "params": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "total_num_steps": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "train_batch_size": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "zero_optimization": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "allgather_partitions": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "contiguous_gradients": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "overlap_comm": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "reduce_scatter": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "stage": 0
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "actor_model": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "disable_dropout": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "pretrained_args": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "use_flash_attention_2": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "critic_model": null,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "general_training_args": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "bf16": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "gradient_checkpointing": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "learning_rate": 1e-06,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "logging_steps": 1,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "max_grad_norm": 1,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "save_steps": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "seed": 2746318213,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "target_train_batch_size": 64,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "weight_decay": 0
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "params": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "cliprange": 0.2,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "cliprange_value": 0.2,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "gamma": 1,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "lam": 1,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "temperature": 0.6,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "use_score_norm": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "use_score_scaling": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "whiten_advantages": true,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "whiten_rewards": false
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "reference_deepspeed_config": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "bf16": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "enabled": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "prescale_gradients": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "train_batch_size": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "wall_clock_breakdown": false
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "reference_model": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "hf_model_name": "/home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "pretrained_args": {
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>                 "use_flash_attention_2": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "report_entropy": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>         "type": "ppo"
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     },
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "type": "policy_iteration",
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >>     "use_deepspeed": true
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >> }
[INFO|main.py:60:3562374] 2024-11-24 21:06:23,625 >> --------------
[2024-11-24 21:06:43,057] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 21:06:43,111] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 21:06:43,139] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 21:06:43,151] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Traceback (most recent call last):
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 110, in <module>
Traceback (most recent call last):
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 110, in <module>
Traceback (most recent call last):
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 110, in <module>
Traceback (most recent call last):
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 110, in <module>
    fire.Fire(EntryPoint)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)    
fire.Fire(EntryPoint)  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire

  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
            component, remaining_args = _CallAndUpdateTrace(fire.Fire(EntryPoint)component_trace = _Fire(component, args, parsed_flag_args, context, name)


  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
      File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
fire.Fire(EntryPoint)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
        component_trace = _Fire(component, args, parsed_flag_args, context, name)component = fn(*varargs, **kwargs)    
    
component, remaining_args = _CallAndUpdateTrace(  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
component_trace = _Fire(component, args, parsed_flag_args, context, name)  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 67, in __init__


  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
        component, remaining_args = _CallAndUpdateTrace(component = fn(*varargs, **kwargs)

  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 67, in __init__
    component = fn(*varargs, **kwargs)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 67, in __init__
        self._runtime = Runtime.from_params(Params({"config_dict": config, **config}))component = fn(*varargs, **kwargs)

  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 809, in from_params
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 67, in __init__
    self._runtime = Runtime.from_params(Params({"config_dict": config, **config}))
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 809, in from_params
    self._runtime = Runtime.from_params(Params({"config_dict": config, **config}))
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 809, in from_params
    self._runtime = Runtime.from_params(Params({"config_dict": config, **config}))
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 809, in from_params
    return retyped_subclass.from_params(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 841, in from_params
    return retyped_subclass.from_params(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 841, in from_params
    return retyped_subclass.from_params(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 841, in from_params
    return retyped_subclass.from_params(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 841, in from_params
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 337, in create_kwargs
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 337, in create_kwargs
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 337, in create_kwargs
    kwargs = create_kwargs(constructor_to_inspect, cls, params, **extras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 337, in create_kwargs
    constructed_arg = pop_and_construct_arg(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 455, in pop_and_construct_arg
    constructed_arg = pop_and_construct_arg(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 455, in pop_and_construct_arg
    constructed_arg = pop_and_construct_arg(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 455, in pop_and_construct_arg
    constructed_arg = pop_and_construct_arg(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 455, in pop_and_construct_arg
    return construct_arg(class_name, name, popped_params, annotation, default, **extras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 521, in construct_arg
    return construct_arg(class_name, name, popped_params, annotation, default, **extras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 521, in construct_arg
    return construct_arg(class_name, name, popped_params, annotation, default, **extras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 521, in construct_arg
    return construct_arg(class_name, name, popped_params, annotation, default, **extras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 521, in construct_arg
    result = annotation.from_params(params=popped_params, **subextras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 809, in from_params
    result = annotation.from_params(params=popped_params, **subextras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 809, in from_params
    result = annotation.from_params(params=popped_params, **subextras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 809, in from_params
    result = annotation.from_params(params=popped_params, **subextras)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 809, in from_params
    return retyped_subclass.from_params(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 843, in from_params
    return retyped_subclass.from_params(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 843, in from_params
    return retyped_subclass.from_params(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 843, in from_params
    return retyped_subclass.from_params(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/from_params.py", line 843, in from_params
    return constructor_to_call(**kwargs)  # type: ignore
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/tokenization_utils/pretrained.py", line 17, in from_di
    return constructor_to_call(**kwargs)  # type: ignore
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/tokenization_utils/pretrained.py", line 17, in from_di
    return constructor_to_call(**kwargs)  # type: ignore
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/tokenization_utils/pretrained.py", line 17, in from_di
    return constructor_to_call(**kwargs)  # type: ignore
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/tokenization_utils/pretrained.py", line 17, in from_di
        tokenizer = AutoTokenizer.from_pretrained(tokenizer = AutoTokenizer.from_pretrained(

  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 782, in from_pretrained
      File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 782, in from_pretrained
tokenizer = AutoTokenizer.from_pretrained(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 782, in from_pretrained
    tokenizer = AutoTokenizer.from_pretrained(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py", line 782, in from_pretrained
            config = AutoConfig.from_pretrained(    config = AutoConfig.from_pretrained(config = AutoConfig.from_pretrained(
config = AutoConfig.from_pretrained(

  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1111, in from_pretrained

  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1111, in from_pretrained
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1111, in from_pretrained
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py", line 1111, in from_pretrained
        config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 633, in get_config_dict

  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 633, in get_config_dict
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 633, in get_config_dict
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 633, in get_config_dict
            config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)


  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 688, in _get_config_dict
      File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 688, in _get_config_dict
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 688, in _get_config_dict
config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/configuration_utils.py", line 688, in _get_config_dict
        resolved_config_file = cached_file(    resolved_config_file = cached_file(
resolved_config_file = cached_file(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 369, in cached_file

  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 369, in cached_file
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 369, in cached_file
    resolved_config_file = cached_file(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/transformers/utils/hub.py", line 369, in cached_file
                raise EnvironmentError(raise EnvironmentError(raise EnvironmentError(raise EnvironmentError(



OSErrorOSErrorOSErrorOSError: : : : /home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K/None' for available files./home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K/None' for available files./home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K/None' for available files./home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K does not appear to have a file named config.json. Checkout 'https://huggingface.co//home/imadlak/.cache/huggingface/hub/models--realtreetune--rho-1b-sft-GSM8K/None' for available files.



[2024-11-24 21:06:54,100] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3562372
[2024-11-24 21:06:54,100] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3562373
[2024-11-24 21:06:54,120] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3562374
[2024-11-24 21:06:54,139] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 3562375
[2024-11-24 21:06:54,156] [ERROR] [launch.py:322:sigkill_handler] ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop'] exits with return code = 1
