[2024-11-24 15:28:33,833] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-11-24 15:29:47,861] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-11-24 15:29:47,862] [INFO] [runner.py:568:main] cmd = /lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank --enable_each_rank_log=None src/treetune/main.py --configs configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet run_iteration_loop
[2024-11-24 15:29:51,530] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-11-24 15:29:53,094] [INFO] [launch.py:138:main] 0 EBVERSIONNCCL=2.18.3
[2024-11-24 15:29:53,094] [INFO] [launch.py:138:main] 0 EBROOTNCCL=/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/CUDA/gcccore/cuda12.2/nccl/2.18.3
[2024-11-24 15:29:53,094] [INFO] [launch.py:138:main] 0 EBDEVELNCCL=/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/CUDA/gcccore/cuda12.2/nccl/2.18.3/easybuild/x86-64-v3-CUDA-gcccore-cuda12.2-nccl-2.18.3-easybuild-devel
[2024-11-24 15:29:53,094] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-11-24 15:29:53,094] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-11-24 15:29:53,094] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-11-24 15:29:53,094] [INFO] [launch.py:163:main] dist_world_size=4
[2024-11-24 15:29:53,094] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-11-24 15:29:53,095] [INFO] [launch.py:253:main] process 4096684 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[2024-11-24 15:29:53,096] [INFO] [launch.py:253:main] process 4096685 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[2024-11-24 15:29:53,097] [INFO] [launch.py:253:main] process 4096686 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[2024-11-24 15:29:53,098] [INFO] [launch.py:253:main] process 4096687 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[INFO|main.py:59:4096686] 2024-11-24 15:29:58,914 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >> ----Config----
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >> {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "analyzers": [
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "inference_strategy": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_depth": 100,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "no_cache": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "temperature": 0.6,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_template": "{query}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "samples": 16,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_num_requests": 1024,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "reward_function": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "math_task": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "load_dataset_dict": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "use_original_format": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "math_reward_function",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "task": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "tokenizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "pretrained"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "type": "mc_value_prediction",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "vllm_server": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "swap_space": 24
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "actor_deepspeed_config": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "bf16": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "enabled": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "prescale_gradients": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "train_batch_size": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "per_device_batch_size": 16,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_num_iterations": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_depth": 100,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "no_cache": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "temperature": 0.6,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_template": "{query}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "samples": 5,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "inference_strategy": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_depth": 100,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "no_cache": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "temperature": 0.6,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_template": "{query}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "samples": 16,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_num_requests": 512,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_num_states": 256,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "reward_function": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "math_task": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "load_dataset_dict": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "use_original_format": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "math_reward_function",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "task": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "tokenizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "pretrained"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "vllm_server": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "swap_space": 24
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     ],
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "directory": "experiments",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "episode_generator": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "answer_prefix": null,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "append_bos_to_query": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "append_eos_to_response": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "fill_missing_episodes": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "inference_strategy": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "answer_extractor": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "node_key_name": "text",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "identity"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "guidance_llm": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "api_base": "none",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "api_key": "EMPTY",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "caching": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_retries": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "openai_vllm"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_depth": 100,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "no_cache": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "node_expander": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "model_context_size": 2047,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "program_kwargs": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_tokens": 1024,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "temperature": 0.6,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "top_p": 0.9
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "tokenizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "pretrained"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "efficient_iid"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "question_field": "query",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "samples": 8,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "type": "cot"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "initial_model_name_or_path": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "max_question_length": 1512,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "max_sequence_length": 2048,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "reward_function": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "math_task": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "type": "math_reward_function",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "task": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "load_dataset_dict": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "type": "gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "use_original_format": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "total_num_iterations": 650,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "answer_extractor": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "node_key_name": "full_text",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "guidance_llm": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "api_base": "none",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "api_key": "EMPTY",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "caching": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_retries": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "openai_vllm"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_depth": 100,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "no_cache": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "node_expander": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "model_context_size": 2047,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "program_kwargs": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_tokens": 1024,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "temperature": 0.6,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "top_p": 0.9
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "tokenizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "pretrained"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "efficient_iid"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "question_field": "query",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "question_template": "{query}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "samples": 9,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "type": "cot"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "vllm_server": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_num_seqs": 512,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "swap_space": 8
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "wait_until_memory_release": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "global_vars": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "debug_mode": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "dirs": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "data": "data",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "experiments": "experiments"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "seed": 2746318213
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "inference_pipelines": [
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "analyzers": [
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "task_performance"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             ],
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "dataset_portion": 1,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "dataset_split": "test",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "inference_strategy": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_key_name": "text",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "identity"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_depth": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "no_cache": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "temperature": 0.35,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "samples": 16,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "seed": 42,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "prompt_library": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "tree": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "expansion": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "task": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "analyzers": [
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "task_performance"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             ],
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "dataset_portion": 1,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "dataset_split": "validation",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "inference_strategy": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_key_name": "text",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "identity"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_depth": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "no_cache": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "temperature": 0.35,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "samples": 16,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "seed": 42,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "prompt_library": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "tree": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "expansion": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "task": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "analyzers": [
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "task_performance"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             ],
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "dataset_split": "train",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "inference_strategy": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_key_name": "text",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "identity"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "max_depth": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "no_cache": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "temperature": 0.35,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "samples": 16,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "seed": 42,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "prompt_library": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "tree": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "expansion": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "task": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     ],
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "num_iterations": 650,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "prompt_library": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "tree": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "expansion": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "tokenizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "type": "pretrained"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "trainer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "actor_deepspeed_config": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "bf16": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "enabled": "auto"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "gradient_clipping": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "optimizer": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "params": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "betas": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "eps": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "lr": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "weight_decay": "auto"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "AdamW"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "scheduler": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "params": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "total_num_steps": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "train_batch_size": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "zero_optimization": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "allgather_partitions": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "contiguous_gradients": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "overlap_comm": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "reduce_scatter": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "stage": 0
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "actor_model": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "disable_dropout": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "pretrained_args": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "use_flash_attention_2": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "critic_model": null,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "general_training_args": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "bf16": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "gradient_checkpointing": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "learning_rate": 1e-06,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "logging_steps": 1,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "max_grad_norm": 1,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "save_steps": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "seed": 2746318213,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "target_train_batch_size": 64,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "weight_decay": 0
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "params": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "cliprange": 0.2,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "cliprange_value": 0.2,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "gamma": 1,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "lam": 1,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "temperature": 0.6,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "use_score_norm": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "use_score_scaling": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "whiten_advantages": true,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "whiten_rewards": false
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "reference_deepspeed_config": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "bf16": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "enabled": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "prescale_gradients": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "train_batch_size": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "wall_clock_breakdown": false
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "reference_model": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "pretrained_args": {
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>                 "use_flash_attention_2": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "report_entropy": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>         "type": "ppo"
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     },
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "type": "policy_iteration",
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >>     "use_deepspeed": true
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >> }
[INFO|main.py:60:4096686] 2024-11-24 15:29:58,914 >> --------------
[INFO|main.py:59:4096685] 2024-11-24 15:29:58,914 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >> ----Config----
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >> {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "analyzers": [
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "inference_strategy": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_depth": 100,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "no_cache": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "temperature": 0.6,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_template": "{query}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "samples": 16,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_num_requests": 1024,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "reward_function": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "math_task": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "load_dataset_dict": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "use_original_format": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "math_reward_function",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "task": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "tokenizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "pretrained"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "type": "mc_value_prediction",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "vllm_server": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "swap_space": 24
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "actor_deepspeed_config": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "bf16": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "enabled": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "prescale_gradients": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "train_batch_size": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "per_device_batch_size": 16,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_num_iterations": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_depth": 100,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "no_cache": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "temperature": 0.6,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_template": "{query}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "samples": 5,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "inference_strategy": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_depth": 100,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "no_cache": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "temperature": 0.6,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_template": "{query}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "samples": 16,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_num_requests": 512,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_num_states": 256,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "reward_function": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "math_task": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "load_dataset_dict": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "use_original_format": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "math_reward_function",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "task": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "tokenizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "pretrained"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "vllm_server": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "swap_space": 24
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     ],
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "directory": "experiments",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "episode_generator": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "answer_prefix": null,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "append_bos_to_query": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "append_eos_to_response": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "fill_missing_episodes": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "inference_strategy": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "answer_extractor": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "node_key_name": "text",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "identity"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "guidance_llm": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "api_base": "none",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "api_key": "EMPTY",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "caching": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_retries": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "openai_vllm"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_depth": 100,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "no_cache": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "node_expander": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "model_context_size": 2047,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "program_kwargs": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_tokens": 1024,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "temperature": 0.6,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "top_p": 0.9
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "tokenizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "pretrained"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "efficient_iid"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "question_field": "query",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "samples": 8,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "type": "cot"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "initial_model_name_or_path": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "max_question_length": 1512,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "max_sequence_length": 2048,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "reward_function": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "math_task": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "type": "math_reward_function",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "task": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "load_dataset_dict": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "type": "gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "use_original_format": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "total_num_iterations": 650,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "answer_extractor": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "node_key_name": "full_text",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "guidance_llm": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "api_base": "none",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "api_key": "EMPTY",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "caching": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_retries": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "openai_vllm"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_depth": 100,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "no_cache": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "node_expander": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "model_context_size": 2047,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "program_kwargs": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_tokens": 1024,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "temperature": 0.6,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "top_p": 0.9
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "tokenizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "pretrained"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "efficient_iid"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "question_field": "query",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "question_template": "{query}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "samples": 9,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "type": "cot"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "vllm_server": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_num_seqs": 512,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "swap_space": 8
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "wait_until_memory_release": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "global_vars": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "debug_mode": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "dirs": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "data": "data",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "experiments": "experiments"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "seed": 2746318213
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "inference_pipelines": [
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "analyzers": [
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "task_performance"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             ],
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "dataset_portion": 1,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "dataset_split": "test",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "inference_strategy": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_key_name": "text",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "identity"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_depth": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "no_cache": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "temperature": 0.35,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "samples": 16,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "seed": 42,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "prompt_library": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "tree": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "expansion": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "task": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "analyzers": [
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "task_performance"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             ],
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "dataset_portion": 1,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "dataset_split": "validation",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "inference_strategy": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_key_name": "text",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "identity"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_depth": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "no_cache": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "temperature": 0.35,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "samples": 16,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "seed": 42,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "prompt_library": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "tree": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "expansion": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "task": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "analyzers": [
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "task_performance"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             ],
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "dataset_split": "train",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "inference_strategy": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "answer_extractor": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_key_name": "text",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "identity"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "guidance_llm": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_base": "none",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "caching": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "max_retries": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "max_depth": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "no_cache": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "node_expander": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "program_kwargs": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "temperature": 0.35,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "top_p": 0.9
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "tokenizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "type": "pretrained"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_field": "query",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "samples": 16,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "seed": 42,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "cot"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "prompt_library": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "tree": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "expansion": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "task": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "gsm8k",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "use_original_format": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     ],
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "num_iterations": 650,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "prompt_library": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "tree": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "expansion": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "tokenizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "type": "pretrained"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "trainer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "actor_deepspeed_config": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "bf16": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "enabled": "auto"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "gradient_clipping": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "optimizer": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "params": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "betas": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "eps": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "lr": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "weight_decay": "auto"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "AdamW"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "scheduler": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "params": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "total_num_steps": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "train_batch_size": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "zero_optimization": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "allgather_partitions": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "contiguous_gradients": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "overlap_comm": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "reduce_scatter": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "stage": 0
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "actor_model": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "disable_dropout": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "pretrained_args": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "use_flash_attention_2": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "critic_model": null,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "general_training_args": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "bf16": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "gradient_checkpointing": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "learning_rate": 1e-06,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "logging_steps": 1,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "max_grad_norm": 1,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "save_steps": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "seed": 2746318213,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "target_train_batch_size": 64,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "weight_decay": 0
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "params": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "cliprange": 0.2,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "cliprange_value": 0.2,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "gamma": 1,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "lam": 1,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "temperature": 0.6,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "use_score_norm": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "use_score_scaling": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "whiten_advantages": true,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "whiten_rewards": false
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "reference_deepspeed_config": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "bf16": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "enabled": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "prescale_gradients": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "train_batch_size": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "wall_clock_breakdown": false
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "reference_model": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "pretrained_args": {
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>                 "use_flash_attention_2": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "report_entropy": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>         "type": "ppo"
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     },
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "type": "policy_iteration",
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >>     "use_deepspeed": true
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >> }
[INFO|main.py:60:4096685] 2024-11-24 15:29:58,914 >> --------------
[INFO|main.py:59:4096687] 2024-11-24 15:29:58,915 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >> ----Config----
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >> {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "analyzers": [
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "inference_strategy": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "answer_extractor": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "guidance_llm": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_base": "none",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "caching": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_retries": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_depth": 100,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "no_cache": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "node_expander": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program_kwargs": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "temperature": 0.6,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "top_p": 0.9
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "tokenizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "type": "pretrained"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_field": "query",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_template": "{query}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "samples": 16,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "cot"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_num_requests": 1024,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "reward_function": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "math_task": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "load_dataset_dict": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "use_original_format": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "math_reward_function",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "task": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "use_original_format": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "tokenizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "pretrained"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "type": "mc_value_prediction",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "vllm_server": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "swap_space": 24
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "actor_deepspeed_config": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "bf16": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "enabled": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "prescale_gradients": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "train_batch_size": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "per_device_batch_size": 16,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_num_iterations": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "answer_extractor": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "guidance_llm": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_base": "none",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "caching": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_retries": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_depth": 100,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "no_cache": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "node_expander": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program_kwargs": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "temperature": 0.6,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "top_p": 0.9
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "tokenizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "type": "pretrained"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_field": "query",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_template": "{query}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "samples": 5,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "cot"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "inference_strategy": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "answer_extractor": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "guidance_llm": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_base": "none",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "caching": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_retries": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_depth": 100,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "no_cache": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "node_expander": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program_kwargs": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "temperature": 0.6,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "top_p": 0.9
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "tokenizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "type": "pretrained"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_field": "query",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_template": "{query}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "samples": 16,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "cot"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_num_requests": 512,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_num_states": 256,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "reward_function": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "math_task": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "load_dataset_dict": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "use_original_format": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "math_reward_function",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "task": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "use_original_format": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "tokenizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "pretrained"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "vllm_server": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "swap_space": 24
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     ],
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "directory": "experiments",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "episode_generator": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "answer_prefix": null,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "append_bos_to_query": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "append_eos_to_response": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "fill_missing_episodes": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "inference_strategy": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "answer_extractor": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "node_key_name": "text",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "identity"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "guidance_llm": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "api_base": "none",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "api_key": "EMPTY",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "caching": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_retries": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "openai_vllm"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_depth": 100,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "no_cache": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "node_expander": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "model_context_size": 2047,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "program_kwargs": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_tokens": 1024,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "temperature": 0.6,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "top_p": 0.9
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "tokenizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "pretrained"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "efficient_iid"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "question_field": "query",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "samples": 8,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "type": "cot"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "initial_model_name_or_path": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "max_question_length": 1512,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "max_sequence_length": 2048,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "reward_function": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "math_task": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "use_original_format": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "type": "math_reward_function",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "task": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "load_dataset_dict": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "type": "gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "use_original_format": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "total_num_iterations": 650,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "answer_extractor": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "node_key_name": "full_text",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "guidance_llm": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "api_base": "none",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "api_key": "EMPTY",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "caching": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_retries": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "openai_vllm"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_depth": 100,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "no_cache": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "node_expander": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "model_context_size": 2047,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "program_kwargs": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_tokens": 1024,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "temperature": 0.6,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "top_p": 0.9
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "tokenizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "pretrained"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "efficient_iid"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "question_field": "query",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "question_template": "{query}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "samples": 9,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "type": "cot"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "vllm_server": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_num_seqs": 512,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "swap_space": 8
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "wait_until_memory_release": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "global_vars": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "debug_mode": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "dirs": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "data": "data",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "experiments": "experiments"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "seed": 2746318213
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "inference_pipelines": [
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "analyzers": [
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "task_performance"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             ],
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "dataset_portion": 1,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "dataset_split": "test",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "inference_strategy": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "answer_extractor": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_key_name": "text",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "identity"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "guidance_llm": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_base": "none",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "caching": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_retries": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_depth": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "no_cache": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "node_expander": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program_kwargs": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "temperature": 0.35,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "top_p": 0.9
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "tokenizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "type": "pretrained"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_field": "query",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "samples": 16,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "seed": 42,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "cot"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "prompt_library": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "tree": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "expansion": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "task": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "use_original_format": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "analyzers": [
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "task_performance"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             ],
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "dataset_portion": 1,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "dataset_split": "validation",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "inference_strategy": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "answer_extractor": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_key_name": "text",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "identity"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "guidance_llm": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_base": "none",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "caching": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_retries": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_depth": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "no_cache": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "node_expander": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program_kwargs": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "temperature": 0.35,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "top_p": 0.9
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "tokenizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "type": "pretrained"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_field": "query",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "samples": 16,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "seed": 42,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "cot"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "prompt_library": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "tree": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "expansion": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "task": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "use_original_format": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "analyzers": [
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "task_performance"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             ],
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "dataset_split": "train",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "inference_strategy": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "answer_extractor": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_key_name": "text",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "identity"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "guidance_llm": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_base": "none",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "caching": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "max_retries": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "max_depth": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "no_cache": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "node_expander": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "program_kwargs": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "temperature": 0.35,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "top_p": 0.9
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "tokenizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "type": "pretrained"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_field": "query",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "samples": 16,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "seed": 42,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "cot"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "prompt_library": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "tree": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "expansion": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "task": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "gsm8k",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "use_original_format": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     ],
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "num_iterations": 650,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "prompt_library": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "tree": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "expansion": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "tokenizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "type": "pretrained"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "trainer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "actor_deepspeed_config": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "bf16": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "enabled": "auto"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "gradient_clipping": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "optimizer": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "params": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "betas": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "eps": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "lr": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "weight_decay": "auto"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "AdamW"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "scheduler": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "params": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "total_num_steps": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "train_batch_size": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "zero_optimization": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "allgather_partitions": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "contiguous_gradients": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "overlap_comm": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "reduce_scatter": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "stage": 0
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "actor_model": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "disable_dropout": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "pretrained_args": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "use_flash_attention_2": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "critic_model": null,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "general_training_args": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "bf16": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "gradient_checkpointing": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "learning_rate": 1e-06,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "logging_steps": 1,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "max_grad_norm": 1,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "save_steps": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "seed": 2746318213,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "target_train_batch_size": 64,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "weight_decay": 0
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "params": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "cliprange": 0.2,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "cliprange_value": 0.2,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "gamma": 1,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "lam": 1,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "temperature": 0.6,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "use_score_norm": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "use_score_scaling": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "whiten_advantages": true,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "whiten_rewards": false
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "reference_deepspeed_config": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "bf16": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "enabled": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "prescale_gradients": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "train_batch_size": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "wall_clock_breakdown": false
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "reference_model": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "pretrained_args": {
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>                 "use_flash_attention_2": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "report_entropy": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>         "type": "ppo"
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     },
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "type": "policy_iteration",
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >>     "use_deepspeed": true
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >> }
[INFO|main.py:60:4096687] 2024-11-24 15:29:58,915 >> --------------
[INFO|main.py:59:4096684] 2024-11-24 15:29:58,916 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >> ----Config----
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >> {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "analyzers": [
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "inference_strategy": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "answer_extractor": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "guidance_llm": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_base": "none",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "caching": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_retries": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_depth": 100,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "no_cache": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "node_expander": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program_kwargs": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "temperature": 0.6,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "top_p": 0.9
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "tokenizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "type": "pretrained"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_field": "query",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_template": "{query}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "samples": 16,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "cot"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_num_requests": 1024,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "reward_function": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "math_task": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "load_dataset_dict": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "use_original_format": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "math_reward_function",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "task": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "use_original_format": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "tokenizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "pretrained"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "type": "mc_value_prediction",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "vllm_server": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "swap_space": 24
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "actor_deepspeed_config": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "bf16": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "enabled": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "prescale_gradients": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "train_batch_size": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "per_device_batch_size": 16,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_num_iterations": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "answer_extractor": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "guidance_llm": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_base": "none",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "caching": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_retries": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_depth": 100,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "no_cache": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "node_expander": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program_kwargs": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "temperature": 0.6,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "top_p": 0.9
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "tokenizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "type": "pretrained"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_field": "query",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_template": "{query}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "samples": 5,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "cot"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "inference_strategy": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "answer_extractor": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_key_name": "full_text",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "guidance_llm": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_base": "none",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "caching": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_retries": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_depth": 100,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "no_cache": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "node_expander": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program_kwargs": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "temperature": 0.6,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "top_p": 0.9
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "tokenizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "type": "pretrained"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_field": "query",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_template": "{query}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "samples": 16,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "cot"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_num_requests": 512,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_num_states": 256,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "reward_function": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "math_task": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "load_dataset_dict": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "use_original_format": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "math_reward_function",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "task": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "use_original_format": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "tokenizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "pretrained"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "vllm_server": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "swap_space": 24
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     ],
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "directory": "experiments",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "episode_generator": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "answer_prefix": null,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "append_bos_to_query": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "append_eos_to_response": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "fill_missing_episodes": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "inference_strategy": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "answer_extractor": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "node_key_name": "text",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "identity"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "guidance_llm": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "api_base": "none",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "api_key": "EMPTY",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "caching": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_retries": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "openai_vllm"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_depth": 100,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "no_cache": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "node_expander": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "model_context_size": 2047,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "program_kwargs": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_tokens": 1024,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "temperature": 0.6,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "top_p": 0.9
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "tokenizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "pretrained"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "efficient_iid"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "question_field": "query",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "samples": 8,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "type": "cot"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "initial_model_name_or_path": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "max_question_length": 1512,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "max_sequence_length": 2048,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "reward_function": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "math_task": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "use_original_format": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "type": "math_reward_function",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "task": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "load_dataset_dict": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "type": "gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "use_original_format": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "total_num_iterations": 650,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "answer_extractor": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "node_key_name": "full_text",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "guidance_llm": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "api_base": "none",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "api_key": "EMPTY",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "caching": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_retries": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "openai_vllm"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_depth": 100,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "no_cache": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "node_expander": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "model_context_size": 2047,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "program_kwargs": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_tokens": 1024,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "temperature": 0.6,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "top_p": 0.9
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "tokenizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "pretrained"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "efficient_iid"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "question_field": "query",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "question_template": "{query}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "samples": 9,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "type": "cot"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "vllm_server": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_num_seqs": 512,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "swap_space": 8
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "wait_until_memory_release": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "global_vars": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "debug_mode": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "dirs": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "data": "data",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "experiments": "experiments"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "seed": 2746318213
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "inference_pipelines": [
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "analyzers": [
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "task_performance"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             ],
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "dataset_portion": 1,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "dataset_split": "test",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "inference_strategy": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "answer_extractor": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_key_name": "text",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "identity"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "guidance_llm": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_base": "none",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "caching": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_retries": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_depth": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "no_cache": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "node_expander": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program_kwargs": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "temperature": 0.35,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "top_p": 0.9
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "tokenizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "type": "pretrained"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_field": "query",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "samples": 16,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "seed": 42,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "cot"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "prompt_library": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "tree": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "expansion": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "task": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "use_original_format": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "analyzers": [
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "task_performance"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             ],
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "dataset_portion": 1,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "dataset_split": "validation",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "inference_strategy": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "answer_extractor": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_key_name": "text",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "identity"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "guidance_llm": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_base": "none",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "caching": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_retries": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_depth": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "no_cache": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "node_expander": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program_kwargs": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "temperature": 0.35,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "top_p": 0.9
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "tokenizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "type": "pretrained"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_field": "query",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "samples": 16,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "seed": 42,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "cot"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "prompt_library": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "tree": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "expansion": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "task": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "use_original_format": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "analyzers": [
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "task_performance"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             ],
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "dataset_split": "train",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "inference_strategy": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "answer_extractor": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_key_name": "text",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "identity"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "guidance_llm": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_base": "none",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "api_key": "EMPTY",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "caching": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "max_retries": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "openai_vllm"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "max_depth": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "no_cache": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "node_expander": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "model_context_size": 2047,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "program_kwargs": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "max_tokens": 1024,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "temperature": 0.35,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "top_p": 0.9
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "tokenizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "type": "pretrained"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "type": "efficient_iid"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_field": "query",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "samples": 16,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "seed": 42,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "cot"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "prompt_library": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "tree": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "expansion": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "task": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "load_dataset_dict": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "gsm8k",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "use_original_format": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     ],
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "num_iterations": 650,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "prompt_library": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "tree": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "expansion": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "tokenizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "type": "pretrained"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "trainer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "actor_deepspeed_config": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "bf16": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "enabled": "auto"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "gradient_clipping": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "optimizer": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "params": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "betas": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "eps": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "lr": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "weight_decay": "auto"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "AdamW"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "scheduler": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "params": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "total_num_steps": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "train_batch_size": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "zero_optimization": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "allgather_partitions": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "contiguous_gradients": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "overlap_comm": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "reduce_scatter": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "stage": 0
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "actor_model": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "disable_dropout": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "pretrained_args": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "use_flash_attention_2": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "critic_model": null,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "general_training_args": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "bf16": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "gradient_checkpointing": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "learning_rate": 1e-06,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "logging_steps": 1,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "max_grad_norm": 1,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "save_steps": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "seed": 2746318213,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "target_train_batch_size": 64,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "weight_decay": 0
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "params": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "cliprange": 0.2,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "cliprange_value": 0.2,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "gamma": 1,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "lam": 1,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "temperature": 0.6,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "use_score_norm": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "use_score_scaling": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "whiten_advantages": true,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "whiten_rewards": false
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "reference_deepspeed_config": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "bf16": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "enabled": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "prescale_gradients": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "train_batch_size": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "wall_clock_breakdown": false
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "reference_model": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "hf_model_name": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "pretrained_args": {
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>                 "use_flash_attention_2": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "report_entropy": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>         "type": "ppo"
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     },
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "type": "policy_iteration",
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >>     "use_deepspeed": true
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >> }
[INFO|main.py:60:4096684] 2024-11-24 15:29:58,916 >> --------------
[2024-11-24 15:31:11,373] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 15:31:11,398] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 15:31:11,470] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 15:31:11,516] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum[93m [WARNING] [0m async_io: please install the libaio-devel package with yum

[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH

[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2024-11-24 15:32:32,385] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-24 15:32:32,385] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-24 15:32:32,387] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-24 15:32:32,387] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-24 15:32:32,387] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[INFO|base_runtime.py:202:4096684] 2024-11-24 15:32:32,837 >> Setting seed = 2746318213
[INFO|base_runtime.py:202:4096685] 2024-11-24 15:32:33,389 >> Setting seed = 2746318213
[INFO|base_runtime.py:202:4096686] 2024-11-24 15:32:33,390 >> Setting seed = 2746318213
[INFO|base_runtime.py:202:4096687] 2024-11-24 15:32:33,410 >> Setting seed = 2746318213
wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 2.
wandb: Tracking run with wandb version 0.16.3
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Device: 0
Device: 1
Device: 2
Device: 3
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >> GPUs Info: 
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >> [
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>     {
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "name": "NVIDIA A100-SXM4-40GB",
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "capability": [
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>             8,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>             0
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         ],
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "cores": 108,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "cuda_core": "unknown",
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "threads": 221184,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "clock": 1410.0,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "memory_clock": 1215.0,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "total_memory": 4095.9999990463257,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "free_memory": 3922.0
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>     },
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>     {
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "name": "NVIDIA A100-SXM4-40GB",
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "capability": [
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>             8,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>             0
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         ],
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "cores": 108,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "cuda_core": "unknown",
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "threads": 221184,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "clock": 1410.0,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "memory_clock": 1215.0,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "total_memory": 4095.9999990463257,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "free_memory": 3922.0
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>     },
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>     {
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "name": "NVIDIA A100-SXM4-40GB",
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "capability": [
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>             8,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>             0
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         ],
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "cores": 108,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "cuda_core": "unknown",
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "threads": 221184,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "clock": 1410.0,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "memory_clock": 1215.0,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "total_memory": 4095.9999990463257,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "free_memory": 3922.0
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>     },
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>     {
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "name": "NVIDIA A100-SXM4-40GB",
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "capability": [
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>             8,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>             0
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         ],
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "cores": 108,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "cuda_core": "unknown",
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "threads": 221184,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "clock": 1410.0,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "memory_clock": 1215.0,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "total_memory": 4095.9999990463257,
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>         "free_memory": 3922.0
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >>     }
[INFO|base_runtime.py:78:4096684] 2024-11-24 15:32:37,870 >> ]
[INFO|base_runtime.py:93:4096684] 2024-11-24 15:32:37,958 >> Application ENV: APP_SEED=2746318213
wandb: WARNING Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 15:32:39,297 >> Using default temp_dir_root: experiments/polIter_rho1bSft2_vineppo_GSM8K/temp_episodes
[INFO|on_policy_episode_generator.py:122:4096684] 2024-11-24 15:32:41,300 >> Found free ports: [28206, 62405, 19317, 47648]
[INFO|on_policy_episode_generator.py:132:4096684] 2024-11-24 15:32:41,695 >> Rank 0 using vLLM port 28206
[INFO|on_policy_episode_generator.py:132:4096686] 2024-11-24 15:32:42,167 >> Rank 2 using vLLM port 19317
[INFO|on_policy_episode_generator.py:132:4096685] 2024-11-24 15:32:42,171 >> Rank 1 using vLLM port 62405
[INFO|on_policy_episode_generator.py:132:4096687] 2024-11-24 15:32:42,173 >> Rank 3 using vLLM port 47648
[ERROR|math_episode_generator.py:92:4096684] 2024-11-24 16:16:45,050 >> Failed to load BLEU metric: Couldn't find a module script at /lustre06/project/6002409/imadlak/program/VinePPO/bleu/bleu.py. Module 'bleu' doesn't exist on the Hugging Face Hub either.
[INFO|ppo_trainer.py:310:4096684] 2024-11-24 16:16:45,053 >> Per device batch size: 16
[INFO|ppo_trainer.py:311:4096684] 2024-11-24 16:16:45,053 >> Gradient accumulation steps: 1
[INFO|ppo_trainer.py:314:4096684] 2024-11-24 16:16:45,053 >> Num of total processes: 4
[INFO|ppo_trainer.py:315:4096684] 2024-11-24 16:16:45,054 >> Global batch size (w. parallel, distributed & accumulation): 64
[INFO|ppo_trainer.py:318:4096684] 2024-11-24 16:16:45,054 >> Total number of training steps (Gradient Updates): 10400
[INFO|ppo_trainer.py:188:4096684] 2024-11-24 16:16:45,054 >> No critic model provided. We then assume values are provided in the episodes.
[INFO|ppo_trainer.py:229:4096684] 2024-11-24 16:16:45,054 >> No temporary checkpoint directory provided. Using /lustre06/project/6002409/imadlak/program/VinePPO/temp_ppo_checkpoints
[INFO|policy_iteration_runtime.py:217:4096684] 2024-11-24 16:16:45,057 >> ********************************************************************************
[INFO|policy_iteration_runtime.py:218:4096684] 2024-11-24 16:16:45,058 >> Running iteration 0
[INFO|policy_iteration_runtime.py:219:4096684] 2024-11-24 16:16:45,058 >> ********************************************************************************
[ERROR|math_episode_generator.py:92:4096685] 2024-11-24 16:16:45,125 >> Failed to load BLEU metric: Couldn't find a module script at /lustre06/project/6002409/imadlak/program/VinePPO/bleu/bleu.py. Module 'bleu' doesn't exist on the Hugging Face Hub either.
[ERROR|math_episode_generator.py:92:4096686] 2024-11-24 16:16:45,194 >> Failed to load BLEU metric: Couldn't find a module script at /lustre06/project/6002409/imadlak/program/VinePPO/bleu/bleu.py. Module 'bleu' doesn't exist on the Hugging Face Hub either.
[ERROR|math_episode_generator.py:92:4096687] 2024-11-24 16:16:45,693 >> Failed to load BLEU metric: Couldn't find a module script at /lustre06/project/6002409/imadlak/program/VinePPO/bleu/bleu.py. Module 'bleu' doesn't exist on the Hugging Face Hub either.
[INFO|policy_iteration_runtime.py:731:4096684] 2024-11-24 16:16:45,696 >> --------------------------------------------------------------------------------
[INFO|policy_iteration_runtime.py:732:4096684] 2024-11-24 16:16:45,696 >> Episode at 0 does not exist. Generating episodes...
[INFO|policy_iteration_runtime.py:735:4096684] 2024-11-24 16:16:45,696 >> --------------------------------------------------------------------------------
[INFO|on_policy_episode_generator.py:213:4096687] 2024-11-24 16:16:45,853 >> Need at least 10240. Waiting for GPU3 used memory to be below 30086.375 MB. Total GPU memory: 40326.375 MB.
[INFO|on_policy_episode_generator.py:213:4096685] 2024-11-24 16:16:45,853 >> Need at least 10240. Waiting for GPU1 used memory to be below 30086.375 MB. Total GPU memory: 40326.375 MB.
[INFO|on_policy_episode_generator.py:213:4096684] 2024-11-24 16:16:45,854 >> Need at least 10240. Waiting for GPU0 used memory to be below 30086.375 MB. Total GPU memory: 40326.375 MB.
[INFO|on_policy_episode_generator.py:213:4096686] 2024-11-24 16:16:45,855 >> Need at least 10240. Waiting for GPU2 used memory to be below 30086.375 MB. Total GPU memory: 40326.375 MB.
[INFO|gpu_utils.py:236:4096687] 2024-11-24 16:16:45,909 >> GPU 3 has less than 30086.375 MB used. Continuing...
[INFO|gpu_utils.py:236:4096685] 2024-11-24 16:16:45,916 >> GPU 1 has less than 30086.375 MB used. Continuing...
[INFO|gpu_utils.py:236:4096684] 2024-11-24 16:16:45,916 >> GPU 0 has less than 30086.375 MB used. Continuing...
[INFO|gpu_utils.py:236:4096686] 2024-11-24 16:16:45,936 >> GPU 2 has less than 30086.375 MB used. Continuing...
[2024-11-24 16:16:46,111] [INFO] [utils.py:772:see_memory_usage] Before generating episodes
[2024-11-24 16:16:46,112] [INFO] [utils.py:773:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-11-24 16:16:46,113] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 18.4 GB, percent = 3.7%
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,481 >> Initial Dataset Size: 7100
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,833 >> Filtered out 0 long questions from 7100 questions.
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,903 >> Dataset Size(portion=None): 64
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >> Dataset Examples: [
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>   {
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "_final_answer": "90",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "_solution": "He spent $2.00 on seeds and $8.00 on soil for a total of 2+8 = $10.00\nHe sells each of the 20 basil plants for $5.00 so he makes 20*5 = $100.00\nHe made $100.00 from selling basil plants and he spent $10.00 to buy and grow the seeds. His net profit is 100-10 = $90.00",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "_treetune__idx": 4768,
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "answer": "90",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "answer_without_calculator": "He spent $2.00 on seeds and $8.00 on soil for a total of 2+8 = $10.00\nHe sells each of the 20 basil plants for $5.00 so he makes 20*5 = $100.00\nHe made $100.00 from selling basil plants and he spent $10.00 to buy and grow the seeds. His net profit is 100-10 = $90.00\n#### 90",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "problem": "Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil.  The packet of seeds yielded 20 basil plants.  He sells each basil plant for $5.00 at the local farmer's market.  What is the net profit from his basil plants?",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "query": "Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil.  The packet of seeds yielded 20 basil plants.  He sells each basil plant for $5.00 at the local farmer's market.  What is the net profit from his basil plants?",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "question": "Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil.  The packet of seeds yielded 20 basil plants.  He sells each basil plant for $5.00 at the local farmer's market.  What is the net profit from his basil plants?",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "solution": "He spent $2.00 on seeds and $8.00 on soil for a total of 2+8 = $10.00\nHe sells each of the 20 basil plants for $5.00 so he makes 20*5 = $100.00\nHe made $100.00 from selling basil plants and he spent $10.00 to buy and grow the seeds. His net profit is 100-10 = $90.00\n#### 90"
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>   },
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>   {
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "_final_answer": "95",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "_solution": "Two fifths of $500 is (2/5)*$500 = $200\nShe needed $200 more than $500 which is $200+ $500 = $700\n15% of $700 is (15/100)*$700 = $105\nShe was given a $105 discount so she has to pay $700-$105 = $595\nShe would still need $595-$500= $95",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "_treetune__idx": 871,
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "answer": "95",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "answer_without_calculator": "Two fifths of $500 is (2/5)*$500 = $200\nShe needed $200 more than $500 which is $200+ $500 = $700\n15% of $700 is (15/100)*$700 = $105\nShe was given a $105 discount so she has to pay $700-$105 = $595\nShe would still need $595-$500= $95\n#### 95",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "problem": "Mrs. Smith wanted to buy wears worth $500. She went to a boutique with the $500 but by the time she had picked out everything she liked, she realized that she would need two-fifths more money than she had. If the shop owner gave her a discount of 15%, how much more money will she still need?",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "query": "Mrs. Smith wanted to buy wears worth $500. She went to a boutique with the $500 but by the time she had picked out everything she liked, she realized that she would need two-fifths more money than she had. If the shop owner gave her a discount of 15%, how much more money will she still need?",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "question": "Mrs. Smith wanted to buy wears worth $500. She went to a boutique with the $500 but by the time she had picked out everything she liked, she realized that she would need two-fifths more money than she had. If the shop owner gave her a discount of 15%, how much more money will she still need?",
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>     "solution": "Two fifths of $500 is (2/5)*$500 = $200\nShe needed $200 more than $500 which is $200+ $500 = $700\n15% of $700 is (15/100)*$700 = $105\nShe was given a $105 discount so she has to pay $700-$105 = $595\nShe would still need $595-$500= $95\n#### 95"
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >>   }
[INFO|on_policy_episode_generator.py:138:4096684] 2024-11-24 16:16:48,911 >> ]
Saving the dataset (0/1 shards):   0%|          | 0/64 [00:00<?, ? examples/s]Saving the dataset (0/1 shards):   0%|          | 0/64 [00:00<?, ? examples/s]Saving the dataset (0/1 shards):   0%|          | 0/64 [00:00<?, ? examples/s]Saving the dataset (0/1 shards):   0%|          | 0/64 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 1421.74 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 1011.48 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 1416.33 examples/s]
Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 1001.82 examples/s]
Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 1206.21 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 1202.67 examples/s]
Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 766.28 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 764.79 examples/s]
[INFO|on_policy_episode_generator.py:122:4096684] 2024-11-24 16:16:51,047 >> Found free ports: [4779, 60991, 57413, 8417]
[INFO|on_policy_episode_generator.py:132:4096686] 2024-11-24 16:16:51,251 >> Rank 2 using vLLM port 57413
[INFO|on_policy_episode_generator.py:132:4096684] 2024-11-24 16:16:51,252 >> Rank 0 using vLLM port 4779
[INFO|on_policy_episode_generator.py:132:4096685] 2024-11-24 16:16:51,258 >> Rank 1 using vLLM port 60991
[INFO|on_policy_episode_generator.py:132:4096687] 2024-11-24 16:16:51,263 >> Rank 3 using vLLM port 8417
[INFO|on_policy_episode_generator.py:508:4096684] 2024-11-24 16:16:51,307 >> GPU #0 Auto-computed vLLM GPU memory utilization: 0.87. Currently Allocated: 1353 MB, Total: 40326.375 MB, Remaining: 35076.0375 MB.
#####################################################################################
 # Sample Trajectories from the current policy 从当前策略采样
 #####################################################################################
[INFO|on_policy_episode_generator.py:508:4096685] 2024-11-24 16:16:51,309 >> GPU #1 Auto-computed vLLM GPU memory utilization: 0.87. Currently Allocated: 1499 MB, Total: 40326.375 MB, Remaining: 34944.637500000004 MB.

#####################################################################################
 # Sample Trajectories from the current policy 从当前策略采样
 #####################################################################################

[INFO|math_episode_generator_with_mc_advantages.py:72:4096685] 2024-11-24 16:16:51,309 >> Always generating from scratch
[INFO|math_episode_generator_with_mc_advantages.py:72:4096684] 2024-11-24 16:16:51,309 >> Always generating from scratch
[INFO|on_policy_episode_generator.py:518:4096685] 2024-11-24 16:16:51,309 >> Rank #1 starting vLLM: model=realtreetune/rho-1b-sft-GSM8K   port=60991   seed=2746318313
[INFO|on_policy_episode_generator.py:518:4096684] 2024-11-24 16:16:51,309 >> Rank #0 starting vLLM: model=realtreetune/rho-1b-sft-GSM8K   port=4779   seed=2746318213
[INFO|on_policy_episode_generator.py:508:4096686] 2024-11-24 16:16:51,320 >> GPU #2 Auto-computed vLLM GPU memory utilization: 0.87. Currently Allocated: 1499 MB, Total: 40326.375 MB, Remaining: 34944.637500000004 MB.
#####################################################################################
 # Sample Trajectories from the current policy 从当前策略采样
 #####################################################################################

[INFO|math_episode_generator_with_mc_advantages.py:72:4096686] 2024-11-24 16:16:51,320 >> Always generating from scratch
[INFO|on_policy_episode_generator.py:518:4096686] 2024-11-24 16:16:51,320 >> Rank #2 starting vLLM: model=realtreetune/rho-1b-sft-GSM8K   port=57413   seed=2746318413
[INFO|on_policy_episode_generator.py:508:4096687] 2024-11-24 16:16:51,330 >> GPU #3 Auto-computed vLLM GPU memory utilization: 0.87. Currently Allocated: 1427 MB, Total: 40326.375 MB, Remaining: 35009.4375 MB.
#####################################################################################
 # Sample Trajectories from the current policy 从当前策略采样
 #####################################################################################

[INFO|math_episode_generator_with_mc_advantages.py:72:4096687] 2024-11-24 16:16:51,331 >> Always generating from scratch
[INFO|on_policy_episode_generator.py:518:4096687] 2024-11-24 16:16:51,331 >> Rank #3 starting vLLM: model=realtreetune/rho-1b-sft-GSM8K   port=8417   seed=2746318513
[INFO|vllm_server.py:243:4096685] 2024-11-24 16:16:51,509 >> Server started with PID 4125081 on port 60991
[INFO|vllm_server.py:243:4096687] 2024-11-24 16:16:51,509 >> Server started with PID 4125082 on port 8417
[INFO|vllm_server.py:243:4096686] 2024-11-24 16:16:51,509 >> Server started with PID 4125083 on port 57413
[INFO|vllm_server.py:243:4096684] 2024-11-24 16:16:51,509 >> Server started with PID 4125084 on port 4779
[ERROR|vllm_server.py:207:4096687] 2024-11-24 16:19:14,000 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096686] 2024-11-24 16:19:14,000 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096685] 2024-11-24 16:19:14,001 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096684] 2024-11-24 16:19:14,001 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >> INFO 11-24 16:17:47 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >> INFO 11-24 16:17:47 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >> INFO 11-24 16:18:27 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     engine = cls(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:19:14,002 >> 
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >> INFO 11-24 16:17:47 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >> INFO 11-24 16:17:47 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >> INFO 11-24 16:18:27 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     engine = cls(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:19:14,002 >> 
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >> INFO 11-24 16:17:47 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >> INFO 11-24 16:17:47 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >> INFO 11-24 16:18:27 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     engine = cls(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:19:14,002 >> 
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >> INFO 11-24 16:17:47 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >> INFO 11-24 16:17:47 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >> INFO 11-24 16:18:27 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     engine = cls(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:19:14,003 >> 
[ERROR|vllm_server.py:207:4096687] 2024-11-24 16:20:41,295 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096685] 2024-11-24 16:20:41,295 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >> INFO 11-24 16:19:19 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >> INFO 11-24 16:19:19 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >> INFO 11-24 16:19:59 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     engine = cls(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:20:41,296 >> 
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >> INFO 11-24 16:19:19 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >> INFO 11-24 16:19:19 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >> INFO 11-24 16:19:59 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     engine = cls(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:20:41,296 >> 
[ERROR|vllm_server.py:207:4096684] 2024-11-24 16:20:41,296 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >> INFO 11-24 16:19:19 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >> INFO 11-24 16:19:19 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >> INFO 11-24 16:19:59 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     engine = cls(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:20:41,297 >> 
[ERROR|vllm_server.py:207:4096686] 2024-11-24 16:20:41,297 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >> INFO 11-24 16:19:19 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >> INFO 11-24 16:19:19 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >> INFO 11-24 16:19:59 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     engine = cls(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:20:41,298 >> 
[ERROR|vllm_server.py:207:4096685] 2024-11-24 16:22:07,588 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >> INFO 11-24 16:20:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >> INFO 11-24 16:20:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >> INFO 11-24 16:21:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     engine = cls(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:22:07,589 >> 
[ERROR|vllm_server.py:207:4096686] 2024-11-24 16:22:08,590 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096684] 2024-11-24 16:22:08,590 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >> INFO 11-24 16:20:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >> INFO 11-24 16:20:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >> INFO 11-24 16:21:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     engine = cls(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:22:08,590 >> 
[ERROR|vllm_server.py:207:4096687] 2024-11-24 16:22:08,591 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >> INFO 11-24 16:20:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >> INFO 11-24 16:20:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >> INFO 11-24 16:21:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     engine = cls(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:22:08,591 >> 
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >> INFO 11-24 16:20:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >> INFO 11-24 16:20:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >> INFO 11-24 16:21:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     engine = cls(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:22:08,592 >> 
[ERROR|vllm_server.py:207:4096685] 2024-11-24 16:23:33,879 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >> INFO 11-24 16:22:12 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >> INFO 11-24 16:22:12 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >> INFO 11-24 16:22:52 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     engine = cls(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:23:33,880 >> 
[ERROR|vllm_server.py:207:4096687] 2024-11-24 16:23:34,882 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096686] 2024-11-24 16:23:34,883 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >> INFO 11-24 16:22:13 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >> INFO 11-24 16:22:13 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >> INFO 11-24 16:22:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     engine = cls(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:23:34,883 >> 
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >> INFO 11-24 16:22:13 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >> INFO 11-24 16:22:13 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >> INFO 11-24 16:22:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     engine = cls(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:23:34,883 >> 
[ERROR|vllm_server.py:207:4096684] 2024-11-24 16:23:34,884 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >> INFO 11-24 16:22:13 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >> INFO 11-24 16:22:13 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >> INFO 11-24 16:22:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     engine = cls(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:23:34,885 >> 
[ERROR|vllm_server.py:207:4096684] 2024-11-24 16:25:01,173 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096687] 2024-11-24 16:25:01,173 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >> INFO 11-24 16:23:39 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >> INFO 11-24 16:23:39 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >> INFO 11-24 16:24:19 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     engine = cls(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:25:01,173 >> 
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >> INFO 11-24 16:23:39 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >> INFO 11-24 16:23:39 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >> INFO 11-24 16:24:19 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     engine = cls(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:25:01,174 >> 
[ERROR|vllm_server.py:207:4096685] 2024-11-24 16:25:01,174 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096686] 2024-11-24 16:25:01,174 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >> INFO 11-24 16:23:38 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >> INFO 11-24 16:23:38 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >> INFO 11-24 16:24:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     engine = cls(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:25:01,175 >> 
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >> INFO 11-24 16:23:39 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >> INFO 11-24 16:23:39 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >> INFO 11-24 16:24:19 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     engine = cls(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:25:01,175 >> 
[ERROR|vllm_server.py:207:4096686] 2024-11-24 16:26:28,464 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096685] 2024-11-24 16:26:28,464 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >> INFO 11-24 16:25:06 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >> INFO 11-24 16:25:06 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >> INFO 11-24 16:25:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     engine = cls(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:26:28,464 >> 
[ERROR|vllm_server.py:207:4096687] 2024-11-24 16:26:28,464 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >> INFO 11-24 16:25:06 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >> INFO 11-24 16:25:06 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >> INFO 11-24 16:25:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     engine = cls(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:26:28,464 >> 
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >> INFO 11-24 16:25:06 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >> INFO 11-24 16:25:06 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >> INFO 11-24 16:25:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     engine = cls(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:26:28,465 >> 
[ERROR|vllm_server.py:207:4096684] 2024-11-24 16:26:28,465 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >> INFO 11-24 16:25:06 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >> INFO 11-24 16:25:06 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >> INFO 11-24 16:25:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     engine = cls(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:26:28,466 >> 
[ERROR|vllm_server.py:207:4096687] 2024-11-24 16:27:54,758 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096684] 2024-11-24 16:27:54,758 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >> INFO 11-24 16:26:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >> INFO 11-24 16:26:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >> INFO 11-24 16:27:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     engine = cls(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:27:54,759 >> 
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >> INFO 11-24 16:26:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >> INFO 11-24 16:26:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >> INFO 11-24 16:27:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     engine = cls(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:27:54,759 >> 
[ERROR|vllm_server.py:207:4096686] 2024-11-24 16:27:54,759 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >> INFO 11-24 16:26:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >> INFO 11-24 16:26:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >> INFO 11-24 16:27:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     engine = cls(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:27:54,760 >> 
[ERROR|vllm_server.py:207:4096685] 2024-11-24 16:27:54,761 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >> INFO 11-24 16:26:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >> INFO 11-24 16:26:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >> INFO 11-24 16:27:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     engine = cls(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:27:54,762 >> 
[ERROR|vllm_server.py:207:4096686] 2024-11-24 16:29:21,047 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096687] 2024-11-24 16:29:21,047 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:4096685] 2024-11-24 16:29:21,048 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >> INFO 11-24 16:27:59 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >> INFO 11-24 16:27:59 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >> INFO 11-24 16:28:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     engine = cls(
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096686] 2024-11-24 16:29:21,048 >> 
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >> INFO 11-24 16:27:59 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >> INFO 11-24 16:27:59 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >> INFO 11-24 16:28:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     engine = cls(
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096687] 2024-11-24 16:29:21,048 >> 
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >> INFO 11-24 16:27:59 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >> INFO 11-24 16:27:59 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >> INFO 11-24 16:28:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     engine = cls(
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096685] 2024-11-24 16:29:21,049 >> 
[ERROR|vllm_server.py:207:4096684] 2024-11-24 16:29:21,052 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >> vLLM Server log:
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >> INFO 11-24 16:27:59 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >> INFO 11-24 16:27:59 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   warnings.warn(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >> INFO 11-24 16:28:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 157, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     engine = cls(
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     self._init_worker()
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 45, in _init_worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     from vllm.worker.worker import Worker
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 21, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     from vllm.worker.model_runner import ModelRunner
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 17, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     from vllm.model_executor.model_loader import get_model
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 10, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     from vllm.model_executor.models.llava import LlavaForConditionalGeneration
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llava.py", line 11, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     from vllm.model_executor.layers.activation import get_act_fn
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/layers/activation.py", line 9, in <module>
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >>     from vllm._C import ops
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >> ImportError: /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/_C.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c106detail14torchCheckFailEPKcS2_jRKSs
[ERROR|vllm_server.py:211:4096684] 2024-11-24 16:29:21,053 >> 
[ERROR|vllm_server.py:200:4096684] 2024-11-24 16:30:12,220 >> Timeout waiting for the server to start.
[ERROR|vllm_server.py:200:4096685] 2024-11-24 16:30:12,220 >> Timeout waiting for the server to start.
[ERROR|vllm_server.py:200:4096687] 2024-11-24 16:30:12,220 >> Timeout waiting for the server to start.
[ERROR|vllm_server.py:200:4096686] 2024-11-24 16:30:12,220 >> Timeout waiting for the server to start.
[ERROR|vllm_server.py:252:4096686] 2024-11-24 16:30:15,447 >> vLLM Server log:
[ERROR|vllm_server.py:252:4096686] 2024-11-24 16:30:15,447 >> INFO 11-24 16:29:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:252:4096686] 2024-11-24 16:30:15,447 >> INFO 11-24 16:29:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:252:4096686] 2024-11-24 16:30:15,447 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:252:4096686] 2024-11-24 16:30:15,447 >>   warnings.warn(
[ERROR|vllm_server.py:252:4096686] 2024-11-24 16:30:15,447 >> INFO 11-24 16:30:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:252:4096686] 2024-11-24 16:30:15,447 >> 
Traceback (most recent call last):
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 110, in <module>
    fire.Fire(EntryPoint)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/runtime/policy_iteration_runtime.py", line 224, in run_iteration_loop
[ERROR|vllm_server.py:252:4096685] 2024-11-24 16:30:15,450 >> vLLM Server log:
[ERROR|vllm_server.py:252:4096685] 2024-11-24 16:30:15,450 >> INFO 11-24 16:29:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:252:4096685] 2024-11-24 16:30:15,450 >> INFO 11-24 16:29:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:252:4096685] 2024-11-24 16:30:15,450 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:252:4096685] 2024-11-24 16:30:15,450 >>   warnings.warn(
[ERROR|vllm_server.py:252:4096685] 2024-11-24 16:30:15,450 >> INFO 11-24 16:30:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:252:4096685] 2024-11-24 16:30:15,450 >> 
Traceback (most recent call last):
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 110, in <module>
    episodes = self._generate_episodes(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/runtime/policy_iteration_runtime.py", line 746, in _generate_episodes
    fire.Fire(EntryPoint)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    episodes = self.episode_generator.generate(    
component_trace = _Fire(component, args, parsed_flag_args, context, name)  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/on_policy_episode_generator.py", line 330, in generate

  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/runtime/policy_iteration_runtime.py", line 224, in run_iteration_loop
    episodes = self._generate_episodes(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/runtime/policy_iteration_runtime.py", line 746, in _generate_episodes
    episodes = self.episode_generator.generate(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/on_policy_episode_generator.py", line 330, in generate
    infer_results = self._run_inference(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/math_episode_generator_with_mc_advantages.py", line 85, in _run_inference
    infer_results = self._run_inference(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/math_episode_generator_with_mc_advantages.py", line 85, in _run_inference
[ERROR|vllm_server.py:252:4096684] 2024-11-24 16:30:15,466 >> vLLM Server log:
[ERROR|vllm_server.py:252:4096684] 2024-11-24 16:30:15,466 >> INFO 11-24 16:29:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:252:4096684] 2024-11-24 16:30:15,466 >> INFO 11-24 16:29:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:252:4096684] 2024-11-24 16:30:15,466 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:252:4096684] 2024-11-24 16:30:15,466 >>   warnings.warn(
[ERROR|vllm_server.py:252:4096684] 2024-11-24 16:30:15,466 >> INFO 11-24 16:30:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:252:4096684] 2024-11-24 16:30:15,466 >> 
[ERROR|vllm_server.py:252:4096687] 2024-11-24 16:30:15,467 >> vLLM Server log:
[ERROR|vllm_server.py:252:4096687] 2024-11-24 16:30:15,467 >> INFO 11-24 16:29:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:252:4096687] 2024-11-24 16:30:15,467 >> INFO 11-24 16:29:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='realtreetune/rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:252:4096687] 2024-11-24 16:30:15,467 >> /lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
[ERROR|vllm_server.py:252:4096687] 2024-11-24 16:30:15,467 >>   warnings.warn(
[ERROR|vllm_server.py:252:4096687] 2024-11-24 16:30:15,467 >> INFO 11-24 16:30:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='realtreetune/rho-1b-sft-GSM8K', tokenizer='realtreetune/rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:252:4096687] 2024-11-24 16:30:15,467 >> 
Traceback (most recent call last):
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 110, in <module>
    fire.Fire(EntryPoint)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/runtime/policy_iteration_runtime.py", line 224, in run_iteration_loop
    episodes = self._generate_episodes(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/runtime/policy_iteration_runtime.py", line 746, in _generate_episodes
    episodes = self.episode_generator.generate(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/on_policy_episode_generator.py", line 330, in generate
    infer_results = self._run_inference(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/math_episode_generator_with_mc_advantages.py", line 85, in _run_inference
            _, guidance_llm_kwargs = get_vllm_server()_, guidance_llm_kwargs = get_vllm_server()_, guidance_llm_kwargs = get_vllm_server()


  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/math_episode_generator_with_mc_advantages.py", line 53, in get_vllm_server
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/math_episode_generator_with_mc_advantages.py", line 53, in get_vllm_server
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/math_episode_generator_with_mc_advantages.py", line 53, in get_vllm_server
    out = vllm_init_fn()
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/on_policy_episode_generator.py", line 528, in _init
    out = vllm_init_fn()
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/on_policy_episode_generator.py", line 528, in _init
    out = vllm_init_fn()
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/on_policy_episode_generator.py", line 528, in _init
    server_url = vllm_server.start_server(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/vllm_server.py", line 253, in start_server
    server_url = vllm_server.start_server(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/vllm_server.py", line 253, in start_server
    server_url = vllm_server.start_server(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/vllm_server.py", line 253, in start_server
            raise RuntimeError("Server did not start within the expected time.")raise RuntimeError("Server did not start within the expected time.")raise RuntimeError("Server did not start within the expected time.")


RuntimeErrorRuntimeErrorRuntimeError: : : Server did not start within the expected time.Traceback (most recent call last):
Server did not start within the expected time.Server did not start within the expected time.


  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/main.py", line 110, in <module>
    fire.Fire(EntryPoint)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 475, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/fire/core.py", line 691, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/runtime/policy_iteration_runtime.py", line 224, in run_iteration_loop
    episodes = self._generate_episodes(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/runtime/policy_iteration_runtime.py", line 746, in _generate_episodes
    episodes = self.episode_generator.generate(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/on_policy_episode_generator.py", line 330, in generate
    infer_results = self._run_inference(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/math_episode_generator_with_mc_advantages.py", line 85, in _run_inference
    _, guidance_llm_kwargs = get_vllm_server()
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/math_episode_generator_with_mc_advantages.py", line 53, in get_vllm_server
    out = vllm_init_fn()
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/episode_generators/on_policy_episode_generator.py", line 528, in _init
    server_url = vllm_server.start_server(
  File "/lustre06/project/6002409/imadlak/program/VinePPO/src/treetune/common/vllm_server.py", line 253, in start_server
    raise RuntimeError("Server did not start within the expected time.")
RuntimeError: Server did not start within the expected time.
wandb: 
wandb: Run history:
wandb: timing/total/init_policy_iteration ▁
wandb: 
wandb: Run summary:
wandb: timing/total/init_policy_iteration 2645.76074
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /lustre06/project/6002409/imadlak/program/VinePPO/wandb/offline-run-20241124_153235-2
wandb: Find logs at: ./wandb/offline-run-20241124_153235-2/logs
[2024-11-24 16:30:17,832] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 4096684
[2024-11-24 16:30:18,067] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 4096685
[2024-11-24 16:30:18,090] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 4096686
[2024-11-24 16:30:18,090] [INFO] [launch.py:316:sigkill_handler] Killing subprocess 4096687
[2024-11-24 16:30:18,112] [ERROR] [launch.py:322:sigkill_handler] ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop'] exits with return code = 1
