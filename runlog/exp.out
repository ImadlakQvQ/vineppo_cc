[2024-11-24 21:53:28,564] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-11-24 21:54:05,870] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2024-11-24 21:54:05,871] [INFO] [runner.py:568:main] cmd = /lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --no_local_rank --enable_each_rank_log=None src/treetune/main.py --configs configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet run_iteration_loop
[2024-11-24 21:54:09,314] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[2024-11-24 21:54:10,735] [INFO] [launch.py:138:main] 0 EBVERSIONNCCL=2.18.3
[2024-11-24 21:54:10,735] [INFO] [launch.py:138:main] 0 EBROOTNCCL=/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/CUDA/gcccore/cuda12.2/nccl/2.18.3
[2024-11-24 21:54:10,735] [INFO] [launch.py:138:main] 0 EBDEVELNCCL=/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/CUDA/gcccore/cuda12.2/nccl/2.18.3/easybuild/x86-64-v3-CUDA-gcccore-cuda12.2-nccl-2.18.3-easybuild-devel
[2024-11-24 21:54:10,735] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2024-11-24 21:54:10,735] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=4, node_rank=0
[2024-11-24 21:54:10,735] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2024-11-24 21:54:10,735] [INFO] [launch.py:163:main] dist_world_size=4
[2024-11-24 21:54:10,735] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2024-11-24 21:54:10,737] [INFO] [launch.py:253:main] process 2471911 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[2024-11-24 21:54:10,737] [INFO] [launch.py:253:main] process 2471912 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[2024-11-24 21:54:10,738] [INFO] [launch.py:253:main] process 2471913 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[2024-11-24 21:54:10,739] [INFO] [launch.py:253:main] process 2471914 spawned with command: ['/lustre06/project/6002409/imadlak/program/VinePPO/venv/bin/python', '-u', 'src/treetune/main.py', '--configs', 'configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet', 'run_iteration_loop']
[INFO|main.py:59:2471914] 2024-11-24 21:54:13,443 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:59:2471911] 2024-11-24 21:54:13,443 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >> ----Config----
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >> {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "analyzers": [
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "inference_strategy": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_depth": 100,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "no_cache": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "temperature": 0.6,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_template": "{query}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "samples": 16,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_num_requests": 1024,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "reward_function": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "math_task": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "load_dataset_dict": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "use_original_format": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "math_reward_function",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "task": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "tokenizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "pretrained"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "type": "mc_value_prediction",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "vllm_server": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "swap_space": 24
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "actor_deepspeed_config": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "bf16": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "enabled": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "prescale_gradients": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "train_batch_size": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "per_device_batch_size": 16,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_num_iterations": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_depth": 100,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "no_cache": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "temperature": 0.6,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_template": "{query}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "samples": 5,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "inference_strategy": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_depth": 100,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "no_cache": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "temperature": 0.6,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_template": "{query}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "samples": 16,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_num_requests": 512,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_num_states": 256,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "reward_function": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "math_task": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "load_dataset_dict": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "use_original_format": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "math_reward_function",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "task": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "tokenizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "pretrained"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "vllm_server": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "swap_space": 24
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     ],
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "directory": "experiments",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "episode_generator": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "answer_prefix": null,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "append_bos_to_query": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "append_eos_to_response": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "fill_missing_episodes": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "inference_strategy": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "answer_extractor": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "node_key_name": "text",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "identity"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "guidance_llm": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "api_base": "none",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "api_key": "EMPTY",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "caching": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_retries": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "openai_vllm"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_depth": 100,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "no_cache": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "node_expander": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "model_context_size": 2047,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "program_kwargs": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_tokens": 1024,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "temperature": 0.6,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "top_p": 0.9
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "tokenizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "pretrained"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "efficient_iid"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "question_field": "query",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "samples": 8,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "type": "cot"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "initial_model_name_or_path": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "max_question_length": 1512,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "max_sequence_length": 2048,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "reward_function": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "math_task": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "type": "math_reward_function",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "task": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "load_dataset_dict": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "type": "gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "use_original_format": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "total_num_iterations": 500,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "answer_extractor": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "node_key_name": "full_text",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "guidance_llm": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "api_base": "none",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "api_key": "EMPTY",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "caching": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_retries": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "openai_vllm"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_depth": 100,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "no_cache": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "node_expander": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "model_context_size": 2047,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "program_kwargs": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_tokens": 1024,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "temperature": 0.6,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "top_p": 0.9
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "tokenizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "pretrained"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "efficient_iid"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "question_field": "query",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "question_template": "{query}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "samples": 9,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "type": "cot"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "vllm_server": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_num_seqs": 512,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "swap_space": 8
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "wait_until_memory_release": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "global_vars": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "debug_mode": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "dirs": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "data": "data",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "experiments": "experiments"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "seed": 2746318213
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "inference_pipelines": [
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "analyzers": [
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "task_performance"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             ],
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "dataset_portion": 1,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "dataset_split": "test",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "inference_strategy": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_key_name": "text",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "identity"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_depth": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "no_cache": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "temperature": 0.35,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "samples": 16,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "seed": 42,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "prompt_library": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "tree": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "expansion": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "task": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "analyzers": [
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "task_performance"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             ],
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "dataset_portion": 1,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "dataset_split": "validation",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "inference_strategy": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_key_name": "text",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "identity"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_depth": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "no_cache": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "temperature": 0.35,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "samples": 16,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "seed": 42,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "prompt_library": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "tree": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "expansion": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "task": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "analyzers": [
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "task_performance"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             ],
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "dataset_split": "train",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "inference_strategy": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_key_name": "text",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "identity"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "max_depth": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "no_cache": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "temperature": 0.35,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "samples": 16,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "seed": 42,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "prompt_library": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "tree": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "expansion": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "task": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     ],
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "num_iterations": 500,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "prompt_library": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "tree": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "expansion": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "tokenizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "type": "pretrained"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "trainer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "actor_deepspeed_config": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "bf16": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "enabled": "auto"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "gradient_clipping": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "optimizer": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "params": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "betas": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "eps": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "lr": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "weight_decay": "auto"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "AdamW"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "scheduler": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "params": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "total_num_steps": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "train_batch_size": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "zero_optimization": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "allgather_partitions": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "contiguous_gradients": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "overlap_comm": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "reduce_scatter": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "stage": 0
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "actor_model": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "disable_dropout": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "pretrained_args": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "use_flash_attention_2": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "critic_model": null,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "general_training_args": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "bf16": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "gradient_checkpointing": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "learning_rate": 1e-06,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "logging_steps": 1,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "max_grad_norm": 1,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "save_steps": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "seed": 2746318213,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "target_train_batch_size": 64,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "weight_decay": 0
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "params": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "cliprange": 0.2,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "cliprange_value": 0.2,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "gamma": 1,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "lam": 1,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "temperature": 0.6,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "use_score_norm": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "use_score_scaling": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "whiten_advantages": true,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "whiten_rewards": false
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "reference_deepspeed_config": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "bf16": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "enabled": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "prescale_gradients": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "train_batch_size": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "wall_clock_breakdown": false
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "reference_model": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "pretrained_args": {
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>                 "use_flash_attention_2": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "report_entropy": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>         "type": "ppo"
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     },
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "type": "policy_iteration",
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >>     "use_deepspeed": true
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >> }
[INFO|main.py:60:2471914] 2024-11-24 21:54:13,444 >> --------------
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >> ----Config----
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >> {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "analyzers": [
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "inference_strategy": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_depth": 100,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "no_cache": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "temperature": 0.6,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_template": "{query}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "samples": 16,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_num_requests": 1024,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "reward_function": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "math_task": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "load_dataset_dict": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "use_original_format": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "math_reward_function",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "task": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "tokenizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "pretrained"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "type": "mc_value_prediction",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "vllm_server": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "swap_space": 24
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "actor_deepspeed_config": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "bf16": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "enabled": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "prescale_gradients": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "train_batch_size": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "per_device_batch_size": 16,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_num_iterations": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_depth": 100,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "no_cache": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "temperature": 0.6,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_template": "{query}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "samples": 5,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "inference_strategy": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_depth": 100,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "no_cache": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "temperature": 0.6,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_template": "{query}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "samples": 16,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_num_requests": 512,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_num_states": 256,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "reward_function": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "math_task": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "load_dataset_dict": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "use_original_format": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "math_reward_function",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "task": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "tokenizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "pretrained"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "vllm_server": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "swap_space": 24
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     ],
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "directory": "experiments",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "episode_generator": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "answer_prefix": null,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "append_bos_to_query": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "append_eos_to_response": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "fill_missing_episodes": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "inference_strategy": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "answer_extractor": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "node_key_name": "text",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "identity"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "guidance_llm": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "api_base": "none",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "api_key": "EMPTY",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "caching": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_retries": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "openai_vllm"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_depth": 100,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "no_cache": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "node_expander": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "model_context_size": 2047,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "program_kwargs": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_tokens": 1024,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "temperature": 0.6,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "top_p": 0.9
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "tokenizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "pretrained"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "efficient_iid"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "question_field": "query",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "samples": 8,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "type": "cot"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "initial_model_name_or_path": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "max_question_length": 1512,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "max_sequence_length": 2048,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "reward_function": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "math_task": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "type": "math_reward_function",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "task": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "load_dataset_dict": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "type": "gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "use_original_format": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "total_num_iterations": 500,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "answer_extractor": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "node_key_name": "full_text",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "guidance_llm": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "api_base": "none",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "api_key": "EMPTY",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "caching": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_retries": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "openai_vllm"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_depth": 100,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "no_cache": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "node_expander": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "model_context_size": 2047,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "program_kwargs": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_tokens": 1024,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "temperature": 0.6,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "top_p": 0.9
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "tokenizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "pretrained"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "efficient_iid"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "question_field": "query",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "question_template": "{query}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "samples": 9,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "type": "cot"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "vllm_server": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_num_seqs": 512,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "swap_space": 8
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "wait_until_memory_release": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "global_vars": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "debug_mode": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "dirs": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "data": "data",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "experiments": "experiments"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "seed": 2746318213
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "inference_pipelines": [
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "analyzers": [
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "task_performance"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             ],
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "dataset_portion": 1,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "dataset_split": "test",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "inference_strategy": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_key_name": "text",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "identity"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_depth": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "no_cache": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "temperature": 0.35,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "samples": 16,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "seed": 42,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "prompt_library": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "tree": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "expansion": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "task": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "analyzers": [
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "task_performance"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             ],
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "dataset_portion": 1,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "dataset_split": "validation",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "inference_strategy": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_key_name": "text",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "identity"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_depth": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "no_cache": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "temperature": 0.35,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "samples": 16,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "seed": 42,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "prompt_library": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "tree": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "expansion": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "task": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "analyzers": [
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "task_performance"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             ],
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "dataset_split": "train",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "inference_strategy": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "answer_extractor": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_key_name": "text",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "identity"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "guidance_llm": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_base": "none",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "caching": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "max_retries": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "max_depth": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "no_cache": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "node_expander": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "program_kwargs": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "temperature": 0.35,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "top_p": 0.9
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "tokenizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "type": "pretrained"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_field": "query",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "samples": 16,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "seed": 42,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "cot"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "prompt_library": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "tree": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "expansion": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "task": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "gsm8k",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "use_original_format": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     ],
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "num_iterations": 500,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "prompt_library": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "tree": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "expansion": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "tokenizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "type": "pretrained"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "trainer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "actor_deepspeed_config": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "bf16": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "enabled": "auto"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "gradient_clipping": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "optimizer": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "params": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "betas": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "eps": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "lr": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "weight_decay": "auto"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "AdamW"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "scheduler": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "params": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "total_num_steps": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "train_batch_size": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "zero_optimization": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "allgather_partitions": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "contiguous_gradients": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "overlap_comm": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "reduce_scatter": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "stage": 0
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "actor_model": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "disable_dropout": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "pretrained_args": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "use_flash_attention_2": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "critic_model": null,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "general_training_args": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "bf16": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "gradient_checkpointing": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "learning_rate": 1e-06,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "logging_steps": 1,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "max_grad_norm": 1,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "save_steps": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "seed": 2746318213,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "target_train_batch_size": 64,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "weight_decay": 0
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "params": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "cliprange": 0.2,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "cliprange_value": 0.2,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "gamma": 1,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "lam": 1,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "temperature": 0.6,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "use_score_norm": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "use_score_scaling": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "whiten_advantages": true,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "whiten_rewards": false
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "reference_deepspeed_config": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "bf16": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "enabled": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "prescale_gradients": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "train_batch_size": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "wall_clock_breakdown": false
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "reference_model": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "pretrained_args": {
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>                 "use_flash_attention_2": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "report_entropy": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>         "type": "ppo"
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     },
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "type": "policy_iteration",
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >>     "use_deepspeed": true
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >> }
[INFO|main.py:60:2471911] 2024-11-24 21:54:13,444 >> --------------
[INFO|main.py:59:2471913] 2024-11-24 21:54:13,445 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >> ----Config----
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >> {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "analyzers": [
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "inference_strategy": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "answer_extractor": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "guidance_llm": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_base": "none",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "caching": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_retries": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_depth": 100,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "no_cache": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "node_expander": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program_kwargs": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "temperature": 0.6,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "top_p": 0.9
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "tokenizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "type": "pretrained"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_field": "query",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_template": "{query}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "samples": 16,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "cot"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_num_requests": 1024,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "reward_function": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "math_task": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "load_dataset_dict": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "use_original_format": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "math_reward_function",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "task": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "use_original_format": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "tokenizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "pretrained"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "type": "mc_value_prediction",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "vllm_server": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "swap_space": 24
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "actor_deepspeed_config": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "bf16": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "enabled": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "prescale_gradients": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "train_batch_size": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "per_device_batch_size": 16,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_num_iterations": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "answer_extractor": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "guidance_llm": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_base": "none",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "caching": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_retries": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_depth": 100,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "no_cache": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "node_expander": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program_kwargs": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "temperature": 0.6,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "top_p": 0.9
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "tokenizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "type": "pretrained"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_field": "query",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_template": "{query}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "samples": 5,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "cot"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "inference_strategy": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "answer_extractor": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "guidance_llm": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_base": "none",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "caching": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_retries": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_depth": 100,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "no_cache": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "node_expander": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program_kwargs": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "temperature": 0.6,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "top_p": 0.9
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "tokenizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "type": "pretrained"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_field": "query",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_template": "{query}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "samples": 16,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "cot"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_num_requests": 512,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_num_states": 256,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "reward_function": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "math_task": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "load_dataset_dict": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "use_original_format": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "math_reward_function",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "task": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "use_original_format": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "tokenizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "pretrained"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "vllm_server": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "swap_space": 24
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     ],
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "directory": "experiments",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "episode_generator": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "answer_prefix": null,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "append_bos_to_query": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "append_eos_to_response": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "fill_missing_episodes": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "inference_strategy": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "answer_extractor": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "node_key_name": "text",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "identity"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "guidance_llm": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "api_base": "none",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "api_key": "EMPTY",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "caching": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_retries": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "openai_vllm"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_depth": 100,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "no_cache": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "node_expander": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "model_context_size": 2047,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "program_kwargs": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_tokens": 1024,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "temperature": 0.6,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "top_p": 0.9
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "tokenizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "pretrained"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "efficient_iid"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "question_field": "query",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "samples": 8,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "type": "cot"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "initial_model_name_or_path": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "max_question_length": 1512,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "max_sequence_length": 2048,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "reward_function": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "math_task": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "use_original_format": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "type": "math_reward_function",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "task": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "load_dataset_dict": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "type": "gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "use_original_format": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "total_num_iterations": 500,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "answer_extractor": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "node_key_name": "full_text",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "guidance_llm": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "api_base": "none",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "api_key": "EMPTY",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "caching": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_retries": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "openai_vllm"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_depth": 100,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "no_cache": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "node_expander": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "model_context_size": 2047,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "program_kwargs": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_tokens": 1024,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "temperature": 0.6,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "top_p": 0.9
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "tokenizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "pretrained"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "efficient_iid"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "question_field": "query",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "question_template": "{query}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "samples": 9,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "type": "cot"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "vllm_server": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_num_seqs": 512,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "swap_space": 8
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "wait_until_memory_release": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "global_vars": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "debug_mode": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "dirs": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "data": "data",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "experiments": "experiments"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "seed": 2746318213
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "inference_pipelines": [
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "analyzers": [
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "task_performance"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             ],
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "dataset_portion": 1,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "dataset_split": "test",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "inference_strategy": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "answer_extractor": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_key_name": "text",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "identity"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "guidance_llm": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_base": "none",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "caching": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_retries": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_depth": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "no_cache": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "node_expander": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program_kwargs": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "temperature": 0.35,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "top_p": 0.9
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "tokenizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "type": "pretrained"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_field": "query",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "samples": 16,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "seed": 42,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "cot"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "prompt_library": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "tree": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "expansion": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "task": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "use_original_format": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "analyzers": [
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "task_performance"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             ],
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "dataset_portion": 1,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "dataset_split": "validation",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "inference_strategy": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "answer_extractor": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_key_name": "text",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "identity"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "guidance_llm": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_base": "none",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "caching": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_retries": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_depth": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "no_cache": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "node_expander": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program_kwargs": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "temperature": 0.35,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "top_p": 0.9
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "tokenizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "type": "pretrained"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_field": "query",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "samples": 16,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "seed": 42,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "cot"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "prompt_library": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "tree": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "expansion": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "task": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "use_original_format": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "analyzers": [
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "task_performance"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             ],
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "dataset_split": "train",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "inference_strategy": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "answer_extractor": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_key_name": "text",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "identity"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "guidance_llm": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_base": "none",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "caching": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "max_retries": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "max_depth": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "no_cache": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "node_expander": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "program_kwargs": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "temperature": 0.35,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "top_p": 0.9
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "tokenizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "type": "pretrained"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_field": "query",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "samples": 16,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "seed": 42,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "cot"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "prompt_library": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "tree": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "expansion": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "task": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "gsm8k",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "use_original_format": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     ],
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "num_iterations": 500,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "prompt_library": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "tree": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "expansion": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "tokenizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "type": "pretrained"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "trainer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "actor_deepspeed_config": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "bf16": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "enabled": "auto"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "gradient_clipping": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "optimizer": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "params": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "betas": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "eps": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "lr": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "weight_decay": "auto"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "AdamW"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "scheduler": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "params": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "total_num_steps": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "train_batch_size": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "zero_optimization": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "allgather_partitions": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "contiguous_gradients": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "overlap_comm": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "reduce_scatter": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "stage": 0
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "actor_model": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "disable_dropout": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "pretrained_args": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "use_flash_attention_2": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "critic_model": null,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "general_training_args": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "bf16": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "gradient_checkpointing": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "learning_rate": 1e-06,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "logging_steps": 1,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "max_grad_norm": 1,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "save_steps": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "seed": 2746318213,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "target_train_batch_size": 64,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "weight_decay": 0
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "params": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "cliprange": 0.2,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "cliprange_value": 0.2,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "gamma": 1,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "lam": 1,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "temperature": 0.6,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "use_score_norm": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "use_score_scaling": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "whiten_advantages": true,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "whiten_rewards": false
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "reference_deepspeed_config": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "bf16": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "enabled": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "prescale_gradients": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "train_batch_size": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "wall_clock_breakdown": false
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "reference_model": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "pretrained_args": {
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>                 "use_flash_attention_2": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "report_entropy": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>         "type": "ppo"
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     },
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "type": "policy_iteration",
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >>     "use_deepspeed": true
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >> }
[INFO|main.py:60:2471913] 2024-11-24 21:54:13,445 >> --------------
[INFO|main.py:59:2471912] 2024-11-24 21:54:13,448 >> Config files: ['configs/polIter_rho1bSft2_vineppo_GSM8K.jsonnet']
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >> ----Config----
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >> {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "analyzers": [
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "inference_strategy": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "answer_extractor": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "guidance_llm": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_base": "none",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "caching": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_retries": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_depth": 100,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "no_cache": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "node_expander": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program_kwargs": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "temperature": 0.6,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "top_p": 0.9
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "tokenizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "type": "pretrained"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_field": "query",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_template": "{query}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "samples": 16,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "cot"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_num_requests": 1024,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "reward_function": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "math_task": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "load_dataset_dict": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "use_original_format": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "math_reward_function",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "task": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "use_original_format": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "tokenizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "pretrained"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "type": "mc_value_prediction",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "vllm_server": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "swap_space": 24
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "actor_deepspeed_config": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "bf16": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "enabled": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "prescale_gradients": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "train_batch_size": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "train_micro_batch_size_per_gpu": 16,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "wall_clock_breakdown": false
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "num_bootstrap_runs": 32,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "num_bootstrap_samples": 32,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "per_device_batch_size": 16,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "store_rolling_aggregates_on_cpu": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "type": "ppo_gradient_variance"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_num_iterations": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "type": "mc_advantage_distribution"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "alternative_continuation_inference_strategy": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "answer_extractor": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "guidance_llm": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_base": "none",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "caching": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_retries": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_depth": 100,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "no_cache": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "node_expander": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "num_expansion_rounds": 1,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program_kwargs": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "temperature": 0.6,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "top_p": 0.9
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "tokenizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "type": "pretrained"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_field": "query",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_template": "{query}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "samples": 5,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "cot"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "inference_strategy": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "answer_extractor": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_key_name": "full_text",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "guidance_llm": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_base": "none",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "caching": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_retries": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_programs": 128,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_depth": 100,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "no_cache": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "node_expander": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "num_expansion_rounds": 16,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program_kwargs": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "temperature": 0.6,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "top_p": 0.9
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "tokenizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "type": "pretrained"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_field": "query",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_template": "{query}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "samples": 16,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "cot"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_num_checkpoints": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_num_requests": 512,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_num_states": 256,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "min_num_alternative_actions": 3,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "num_mc_rollouts": 9,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "reward_function": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "math_task": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "load_dataset_dict": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "remove_calculator_expressions": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "use_original_format": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "penalize_unfinished_response": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "math_reward_function",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "unfinished_response_penalty": 0
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "task": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "use_original_format": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "tokenizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "pretrained"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "type": "mc_value_action_ranking",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "vllm_server": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "enable_prefix_caching": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "swap_space": 24
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     ],
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "directory": "experiments",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "episode_generator": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "answer_prefix": null,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "append_bos_to_query": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "append_eos_to_response": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "dataset_num_samples_per_iteration": 64,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "dataset_sample_with_replacement": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "dataset_shuffle_before_portion": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "dataset_shuffle_on_each_iteration": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "fill_missing_episodes": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "inference_strategy": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "answer_extractor": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "node_key_name": "text",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "identity"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "guidance_llm": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "api_base": "none",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "api_key": "EMPTY",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "caching": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_retries": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "openai_vllm"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_concurrent_generations": 64,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_concurrent_programs": 128,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_depth": 100,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "no_cache": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "node_expander": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "model_context_size": 2047,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "program_kwargs": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_tokens": 1024,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "temperature": 0.6,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "top_p": 0.9
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "tokenizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "pretrained"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "efficient_iid"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "question_field": "query",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "samples": 8,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "type": "cot"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "initial_model_name_or_path": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "max_question_length": 1512,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "max_sequence_length": 2048,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "max_step_for_value_estimation": 25,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "reasoning_step_delimiter": "",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "reward_function": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "math_task": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "use_original_format": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "penalize_unfinished_response": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "type": "math_reward_function",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "unfinished_response_penalty": 0
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "task": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "load_dataset_dict": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "remove_calculator_expressions": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "type": "gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "use_original_format": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "total_num_iterations": 500,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "type": "math_episode_generator_w_mc_advantages",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "value_estimation_inference_strategy": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "answer_extractor": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "node_key_name": "full_text",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "solution_prefix": "\nSolution:",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "identity_with_solution_prefix"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "guidance_llm": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "api_base": "none",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "api_key": "EMPTY",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "caching": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_calls_per_min": 1000000,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_retries": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "model": "realtreetune/rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "openai_vllm"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_concurrent_generations": 512,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_concurrent_programs": 512,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_depth": 100,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "no_cache": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "node_expander": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "model_context_size": 2047,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "program_kwargs": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_tokens": 1024,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "temperature": 0.6,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "top_p": 0.9
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "tokenizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "pretrained"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "efficient_iid"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "question_field": "query",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "question_template": "{query}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "samples": 9,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "type": "cot"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "vllm_gpu_memory_utilization": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "vllm_min_available_gpu_memory_mb": 10240,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "vllm_server": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_num_seqs": 512,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "swap_space": 8
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "wait_until_memory_release": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "episodes_cloud_log_steps": 50,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "evaluation_vllm_server": {},
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "exp_name": "polIter_rho1bSft2_vineppo_GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "global_vars": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "debug_mode": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "dirs": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "data": "data",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "experiments": "experiments"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "seed": 2746318213
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "inference_pipelines": [
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "analyzers": [
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "task_performance"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             ],
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "dataset_portion": 1,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "dataset_split": "test",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "inference_name": "gsm8k_test",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "inference_strategy": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "answer_extractor": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_key_name": "text",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "identity"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "guidance_llm": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_base": "none",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "caching": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_retries": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_depth": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "no_cache": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "node_expander": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program_kwargs": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "temperature": 0.35,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "top_p": 0.9
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "tokenizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "type": "pretrained"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_field": "query",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "samples": 16,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "seed": 42,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "cot"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "prompt_library": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "tree": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "expansion": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "task": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "use_original_format": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "analyzers": [
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "task_performance"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             ],
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "dataset_portion": 1,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "dataset_split": "validation",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "inference_name": "gsm8k_validation",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "inference_strategy": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "answer_extractor": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_key_name": "text",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "identity"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "guidance_llm": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_base": "none",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "caching": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_retries": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_depth": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "no_cache": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "node_expander": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program_kwargs": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "temperature": 0.35,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "top_p": 0.9
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "tokenizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "type": "pretrained"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_field": "query",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "samples": 16,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "seed": 42,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "cot"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "prompt_library": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "tree": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "expansion": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "task": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "use_original_format": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "analyzers": [
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "task_performance"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             ],
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "dataset_portion": 0.05253521,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "dataset_shuffle_before_portion": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "dataset_split": "train",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "inference_name": "gsm8k_train",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "inference_strategy": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "answer_extractor": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_key_name": "text",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "identity"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "guidance_llm": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_base": "none",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "api_key": "EMPTY",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "caching": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_calls_per_min": 1000000,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "max_retries": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "tokenizer_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "openai_vllm"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_generations": 128,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_concurrent_programs": 512,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "max_depth": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "no_cache": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "node_expander": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "model_context_size": 2047,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "node_text_template": "{chain_of_thought}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "program_kwargs": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "max_tokens": 1024,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "stop": "\"\n\n\nProblem:\"",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "temperature": 0.35,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "top_p": 0.9
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "tokenizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "hf_model_name": "microsoft/rho-math-1b-v0.1",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "type": "pretrained"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "type": "efficient_iid"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_field": "query",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "samples": 16,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "seed": 42,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "cot"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "prompt_library": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "tree": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "expansion": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                         "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "task": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "dataset_dict_path": "data/gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "load_dataset_dict": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "remove_calculator_expressions": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "gsm8k",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "use_original_format": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     ],
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "num_episodes_per_iteration": 512,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "num_iterations": 500,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "prompt_library": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "tree": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "expansion": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "iid": "{{prefix}}{{gen \"chain_of_thought\" temperature={temperature} top_p={top_p} max_tokens={max_tokens} seed={seed} save_stop_text=\"stop_text\" stop={stop} n={num_samples}}}"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "question_template": "[MATH_TASK] Problem:\n{query}\n\nSolution:"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "tokenizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "type": "pretrained"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "trainer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "actor_deepspeed_config": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "bf16": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "enabled": "auto"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "gradient_clipping": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "optimizer": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "params": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "betas": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "eps": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "lr": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "weight_decay": "auto"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "AdamW"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "scheduler": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "params": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "last_batch_iteration": -1,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "total_num_steps": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "warmup_max_lr": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "warmup_min_lr": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                     "warmup_num_steps": "auto"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "type": "WarmupDecayLR"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "train_batch_size": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "zero_allow_untested_optimizer": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "zero_optimization": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "allgather_bucket_size": 500000000,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "allgather_partitions": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "contiguous_gradients": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "overlap_comm": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "reduce_bucket_size": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "reduce_scatter": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "stage": 0
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "actor_model": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "disable_dropout": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "pretrained_args": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "use_flash_attention_2": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "cache_deepspeed_engines": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "critic_deepspeed_config": null,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "critic_model": null,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "general_training_args": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "bf16": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "checkpoint_keep_steps": 40,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "dataloader_num_workers": 1,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "dataloader_pin_memory": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "gradient_accumulation_steps": 1,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "gradient_checkpointing": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "learning_rate": 1e-06,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "logging_steps": 1,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "max_grad_norm": 1,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "per_device_train_batch_size": null,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "save_steps": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "seed": 2746318213,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "target_train_batch_size": 64,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "warmup_ratio": 0.03,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "weight_decay": 0
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "move_reference_model_to_cpu": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "num_epochs_per_iteration": 2,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "params": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "adap_kl_ctrl": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "cliprange": 0.2,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "cliprange_value": 0.2,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "gamma": 1,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "init_kl_coef": 0.0001,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "kl_penalty_loss_clip_max": 10,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "kl_penalty_loss_clip_min": 0,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "kl_penalty_loss_type": "control_variate",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "lam": 1,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "temperature": 0.6,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "use_score_norm": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "use_score_scaling": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "whiten_advantages": true,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "whiten_rewards": false
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "reference_deepspeed_config": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "bf16": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "enabled": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "gradient_accumulation_steps": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "prescale_gradients": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "train_batch_size": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "train_micro_batch_size_per_gpu": "auto",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "wall_clock_breakdown": false
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "reference_model": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "hf_model_name": "rho-1b-sft-GSM8K",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "pretrained_args": {
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>                 "use_flash_attention_2": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>             "type": "pretrained_causal_lm"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "report_entropy": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "save_hf_critic_checkpoint": false,
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>         "type": "ppo"
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     },
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "type": "policy_iteration",
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >>     "use_deepspeed": true
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >> }
[INFO|main.py:60:2471912] 2024-11-24 21:54:13,448 >> --------------
[2024-11-24 21:54:52,051] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 21:54:52,124] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 21:54:52,163] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-11-24 21:54:52,175] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-devel package with yum
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.1
[93m [WARNING] [0m using untested triton version (2.1.0), only 1.0.0 is known to be compatible
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
[2024-11-24 21:55:12,248] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-24 21:55:12,248] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-24 21:55:12,248] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-11-24 21:55:12,248] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-11-24 21:55:12,249] [INFO] [comm.py:637:init_distributed] cdb=None
[INFO|base_runtime.py:202:2471911] 2024-11-24 21:55:12,626 >> Setting seed = 2746318213
[INFO|base_runtime.py:202:2471914] 2024-11-24 21:55:13,081 >> Setting seed = 2746318213
[INFO|base_runtime.py:202:2471913] 2024-11-24 21:55:13,082 >> Setting seed = 2746318213
[INFO|base_runtime.py:202:2471912] 2024-11-24 21:55:13,095 >> Setting seed = 2746318213
wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 2.
wandb: Tracking run with wandb version 0.16.3
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Device: 0
Device: 1
Device: 2
Device: 3
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >> GPUs Info: 
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >> [
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>     {
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "name": "NVIDIA A100-SXM4-40GB",
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "capability": [
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>             8,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>             0
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         ],
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "cores": 108,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "cuda_core": "unknown",
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "threads": 221184,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "clock": 1410.0,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "memory_clock": 1215.0,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "total_memory": 4095.9999990463257,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "free_memory": 3922.0
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>     },
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>     {
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "name": "NVIDIA A100-SXM4-40GB",
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "capability": [
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>             8,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>             0
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         ],
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "cores": 108,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "cuda_core": "unknown",
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "threads": 221184,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "clock": 1410.0,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "memory_clock": 1215.0,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "total_memory": 4095.9999990463257,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "free_memory": 3922.0
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>     },
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>     {
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "name": "NVIDIA A100-SXM4-40GB",
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "capability": [
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>             8,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>             0
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         ],
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "cores": 108,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "cuda_core": "unknown",
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "threads": 221184,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "clock": 1410.0,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "memory_clock": 1215.0,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "total_memory": 4095.9999990463257,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "free_memory": 3922.0
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>     },
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>     {
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "name": "NVIDIA A100-SXM4-40GB",
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "capability": [
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>             8,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>             0
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         ],
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "cores": 108,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "cuda_core": "unknown",
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "threads": 221184,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "clock": 1410.0,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "memory_clock": 1215.0,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "total_memory": 4095.9999990463257,
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>         "free_memory": 3922.0
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >>     }
[INFO|base_runtime.py:78:2471911] 2024-11-24 21:55:16,631 >> ]
[INFO|base_runtime.py:93:2471911] 2024-11-24 21:55:16,701 >> Application ENV: APP_SEED=2746318213
wandb: WARNING Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 21:55:19,076 >> Using default temp_dir_root: experiments/polIter_rho1bSft2_vineppo_GSM8K/temp_episodes
[INFO|on_policy_episode_generator.py:122:2471911] 2024-11-24 21:55:21,079 >> Found free ports: [28206, 62405, 19317, 47648]
[INFO|on_policy_episode_generator.py:132:2471911] 2024-11-24 21:55:21,557 >> Rank 0 using vLLM port 28206
[INFO|on_policy_episode_generator.py:132:2471914] 2024-11-24 21:55:21,676 >> Rank 3 using vLLM port 47648
[INFO|on_policy_episode_generator.py:132:2471913] 2024-11-24 21:55:21,679 >> Rank 2 using vLLM port 19317
[INFO|on_policy_episode_generator.py:132:2471912] 2024-11-24 21:55:21,680 >> Rank 1 using vLLM port 62405
[ERROR|math_episode_generator.py:92:2471913] 2024-11-24 22:39:25,065 >> Failed to load BLEU metric: Couldn't find a module script at /lustre06/project/6002409/imadlak/program/VinePPO/bleu/bleu.py. Module 'bleu' doesn't exist on the Hugging Face Hub either.
[ERROR|math_episode_generator.py:92:2471911] 2024-11-24 22:39:25,146 >> Failed to load BLEU metric: Couldn't find a module script at /lustre06/project/6002409/imadlak/program/VinePPO/bleu/bleu.py. Module 'bleu' doesn't exist on the Hugging Face Hub either.
[INFO|ppo_trainer.py:310:2471911] 2024-11-24 22:39:25,149 >> Per device batch size: 16
[INFO|ppo_trainer.py:311:2471911] 2024-11-24 22:39:25,149 >> Gradient accumulation steps: 1
[INFO|ppo_trainer.py:314:2471911] 2024-11-24 22:39:25,149 >> Num of total processes: 4
[INFO|ppo_trainer.py:315:2471911] 2024-11-24 22:39:25,149 >> Global batch size (w. parallel, distributed & accumulation): 64
[INFO|ppo_trainer.py:318:2471911] 2024-11-24 22:39:25,149 >> Total number of training steps (Gradient Updates): 8000
[INFO|ppo_trainer.py:188:2471911] 2024-11-24 22:39:25,149 >> No critic model provided. We then assume values are provided in the episodes.
[INFO|ppo_trainer.py:229:2471911] 2024-11-24 22:39:25,150 >> No temporary checkpoint directory provided. Using /lustre06/project/6002409/imadlak/program/VinePPO/temp_ppo_checkpoints
[INFO|policy_iteration_runtime.py:217:2471911] 2024-11-24 22:39:25,151 >> ********************************************************************************
[INFO|policy_iteration_runtime.py:218:2471911] 2024-11-24 22:39:25,151 >> Running iteration 0
[INFO|policy_iteration_runtime.py:219:2471911] 2024-11-24 22:39:25,151 >> ********************************************************************************
[ERROR|math_episode_generator.py:92:2471914] 2024-11-24 22:39:25,212 >> Failed to load BLEU metric: Couldn't find a module script at /lustre06/project/6002409/imadlak/program/VinePPO/bleu/bleu.py. Module 'bleu' doesn't exist on the Hugging Face Hub either.
[ERROR|math_episode_generator.py:92:2471912] 2024-11-24 22:39:25,346 >> Failed to load BLEU metric: Couldn't find a module script at /lustre06/project/6002409/imadlak/program/VinePPO/bleu/bleu.py. Module 'bleu' doesn't exist on the Hugging Face Hub either.
[INFO|policy_iteration_runtime.py:731:2471911] 2024-11-24 22:39:25,348 >> --------------------------------------------------------------------------------
[INFO|policy_iteration_runtime.py:732:2471911] 2024-11-24 22:39:25,348 >> Episode at 0 does not exist. Generating episodes...
[INFO|policy_iteration_runtime.py:735:2471911] 2024-11-24 22:39:25,348 >> --------------------------------------------------------------------------------
[INFO|on_policy_episode_generator.py:213:2471914] 2024-11-24 22:39:25,504 >> Need at least 10240. Waiting for GPU3 used memory to be below 30086.375 MB. Total GPU memory: 40326.375 MB.
[INFO|on_policy_episode_generator.py:213:2471913] 2024-11-24 22:39:25,504 >> Need at least 10240. Waiting for GPU2 used memory to be below 30086.375 MB. Total GPU memory: 40326.375 MB.
[INFO|on_policy_episode_generator.py:213:2471911] 2024-11-24 22:39:25,506 >> Need at least 10240. Waiting for GPU0 used memory to be below 30086.375 MB. Total GPU memory: 40326.375 MB.
[INFO|on_policy_episode_generator.py:213:2471912] 2024-11-24 22:39:25,507 >> Need at least 10240. Waiting for GPU1 used memory to be below 30086.375 MB. Total GPU memory: 40326.375 MB.
[INFO|gpu_utils.py:236:2471912] 2024-11-24 22:39:25,562 >> GPU 1 has less than 30086.375 MB used. Continuing...
[INFO|gpu_utils.py:236:2471911] 2024-11-24 22:39:25,575 >> GPU 0 has less than 30086.375 MB used. Continuing...
[INFO|gpu_utils.py:236:2471913] 2024-11-24 22:39:25,578 >> GPU 2 has less than 30086.375 MB used. Continuing...
[INFO|gpu_utils.py:236:2471914] 2024-11-24 22:39:25,579 >> GPU 3 has less than 30086.375 MB used. Continuing...
[2024-11-24 22:39:25,739] [INFO] [utils.py:772:see_memory_usage] Before generating episodes
[2024-11-24 22:39:25,740] [INFO] [utils.py:773:see_memory_usage] MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
[2024-11-24 22:39:25,741] [INFO] [utils.py:780:see_memory_usage] CPU Virtual Memory:  used = 18.84 GB, percent = 3.7%
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:27,849 >> Initial Dataset Size: 7100
Filtering long questions (num_proc=4):   0%|          | 0/7100 [00:00<?, ? examples/s]Filtering long questions (num_proc=4):  14%|█▍        | 1000/7100 [00:00<00:02, 2269.10 examples/s]Filtering long questions (num_proc=4):  53%|█████▎    | 3775/7100 [00:00<00:00, 8049.80 examples/s]Filtering long questions (num_proc=4):  89%|████████▉ | 6325/7100 [00:00<00:00, 11103.40 examples/s]Filtering long questions (num_proc=4): 100%|██████████| 7100/7100 [00:00<00:00, 7957.94 examples/s] 
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,040 >> Filtered out 0 long questions from 7100 questions.
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,146 >> Dataset Size(portion=None): 64
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >> Dataset Examples: [
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>   {
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "_final_answer": "90",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "_solution": "He spent $2.00 on seeds and $8.00 on soil for a total of 2+8 = $10.00\nHe sells each of the 20 basil plants for $5.00 so he makes 20*5 = $100.00\nHe made $100.00 from selling basil plants and he spent $10.00 to buy and grow the seeds. His net profit is 100-10 = $90.00",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "_treetune__idx": 4768,
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "answer": "90",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "answer_without_calculator": "He spent $2.00 on seeds and $8.00 on soil for a total of 2+8 = $10.00\nHe sells each of the 20 basil plants for $5.00 so he makes 20*5 = $100.00\nHe made $100.00 from selling basil plants and he spent $10.00 to buy and grow the seeds. His net profit is 100-10 = $90.00\n#### 90",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "problem": "Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil.  The packet of seeds yielded 20 basil plants.  He sells each basil plant for $5.00 at the local farmer's market.  What is the net profit from his basil plants?",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "query": "Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil.  The packet of seeds yielded 20 basil plants.  He sells each basil plant for $5.00 at the local farmer's market.  What is the net profit from his basil plants?",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "question": "Burt spent $2.00 on a packet of basil seeds and $8.00 on potting soil.  The packet of seeds yielded 20 basil plants.  He sells each basil plant for $5.00 at the local farmer's market.  What is the net profit from his basil plants?",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "solution": "He spent $2.00 on seeds and $8.00 on soil for a total of 2+8 = $10.00\nHe sells each of the 20 basil plants for $5.00 so he makes 20*5 = $100.00\nHe made $100.00 from selling basil plants and he spent $10.00 to buy and grow the seeds. His net profit is 100-10 = $90.00\n#### 90"
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>   },
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>   {
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "_final_answer": "95",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "_solution": "Two fifths of $500 is (2/5)*$500 = $200\nShe needed $200 more than $500 which is $200+ $500 = $700\n15% of $700 is (15/100)*$700 = $105\nShe was given a $105 discount so she has to pay $700-$105 = $595\nShe would still need $595-$500= $95",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "_treetune__idx": 871,
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "answer": "95",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "answer_without_calculator": "Two fifths of $500 is (2/5)*$500 = $200\nShe needed $200 more than $500 which is $200+ $500 = $700\n15% of $700 is (15/100)*$700 = $105\nShe was given a $105 discount so she has to pay $700-$105 = $595\nShe would still need $595-$500= $95\n#### 95",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "problem": "Mrs. Smith wanted to buy wears worth $500. She went to a boutique with the $500 but by the time she had picked out everything she liked, she realized that she would need two-fifths more money than she had. If the shop owner gave her a discount of 15%, how much more money will she still need?",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "query": "Mrs. Smith wanted to buy wears worth $500. She went to a boutique with the $500 but by the time she had picked out everything she liked, she realized that she would need two-fifths more money than she had. If the shop owner gave her a discount of 15%, how much more money will she still need?",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "question": "Mrs. Smith wanted to buy wears worth $500. She went to a boutique with the $500 but by the time she had picked out everything she liked, she realized that she would need two-fifths more money than she had. If the shop owner gave her a discount of 15%, how much more money will she still need?",
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>     "solution": "Two fifths of $500 is (2/5)*$500 = $200\nShe needed $200 more than $500 which is $200+ $500 = $700\n15% of $700 is (15/100)*$700 = $105\nShe was given a $105 discount so she has to pay $700-$105 = $595\nShe would still need $595-$500= $95\n#### 95"
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >>   }
[INFO|on_policy_episode_generator.py:138:2471911] 2024-11-24 22:39:29,147 >> ]
Saving the dataset (0/1 shards):   0%|          | 0/64 [00:00<?, ? examples/s]Saving the dataset (0/1 shards):   0%|          | 0/64 [00:00<?, ? examples/s]Saving the dataset (0/1 shards):   0%|          | 0/64 [00:00<?, ? examples/s]Saving the dataset (0/1 shards):   0%|          | 0/64 [00:00<?, ? examples/s]Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 2711.20 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 2693.70 examples/s]
Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 3083.27 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 3059.86 examples/s]
Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 2528.16 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 2484.92 examples/s]
Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 1859.00 examples/s]Saving the dataset (1/1 shards): 100%|██████████| 64/64 [00:00<00:00, 1850.72 examples/s]
[INFO|on_policy_episode_generator.py:122:2471911] 2024-11-24 22:39:31,262 >> Found free ports: [4779, 60991, 57413, 8417]
[INFO|on_policy_episode_generator.py:132:2471911] 2024-11-24 22:39:31,441 >> Rank 0 using vLLM port 4779
[INFO|on_policy_episode_generator.py:132:2471913] 2024-11-24 22:39:31,445 >> Rank 2 using vLLM port 57413
[INFO|on_policy_episode_generator.py:132:2471912] 2024-11-24 22:39:31,448 >> Rank 1 using vLLM port 60991
[INFO|on_policy_episode_generator.py:132:2471914] 2024-11-24 22:39:31,448 >> Rank 3 using vLLM port 8417
[INFO|on_policy_episode_generator.py:508:2471911] 2024-11-24 22:39:31,504 >> GPU #0 Auto-computed vLLM GPU memory utilization: 0.87. Currently Allocated: 1353 MB, Total: 40326.375 MB, Remaining: 35076.0375 MB.
#####################################################################################
 # Sample Trajectories from the current policy 从当前策略采样
 #####################################################################################

[INFO|math_episode_generator_with_mc_advantages.py:72:2471911] 2024-11-24 22:39:31,505 >> Always generating from scratch
[INFO|on_policy_episode_generator.py:518:2471911] 2024-11-24 22:39:31,505 >> Rank #0 starting vLLM: model=rho-1b-sft-GSM8K   port=4779   seed=2746318213
[INFO|on_policy_episode_generator.py:508:2471914] 2024-11-24 22:39:31,515 >> GPU #3 Auto-computed vLLM GPU memory utilization: 0.87. Currently Allocated: 1427 MB, Total: 40326.375 MB, Remaining: 35009.4375 MB.
#####################################################################################
 # Sample Trajectories from the current policy 从当前策略采样
 #####################################################################################

[INFO|math_episode_generator_with_mc_advantages.py:72:2471914] 2024-11-24 22:39:31,516 >> Always generating from scratch
[INFO|on_policy_episode_generator.py:518:2471914] 2024-11-24 22:39:31,516 >> Rank #3 starting vLLM: model=rho-1b-sft-GSM8K   port=8417   seed=2746318513
[INFO|on_policy_episode_generator.py:508:2471913] 2024-11-24 22:39:31,518 >> GPU #2 Auto-computed vLLM GPU memory utilization: 0.87. Currently Allocated: 1499 MB, Total: 40326.375 MB, Remaining: 34944.637500000004 MB.
#####################################################################################
 # Sample Trajectories from the current policy 从当前策略采样
 #####################################################################################

[INFO|math_episode_generator_with_mc_advantages.py:72:2471913] 2024-11-24 22:39:31,519 >> Always generating from scratch
[INFO|on_policy_episode_generator.py:518:2471913] 2024-11-24 22:39:31,519 >> Rank #2 starting vLLM: model=rho-1b-sft-GSM8K   port=57413   seed=2746318413
[INFO|on_policy_episode_generator.py:508:2471912] 2024-11-24 22:39:31,519 >> GPU #1 Auto-computed vLLM GPU memory utilization: 0.87. Currently Allocated: 1499 MB, Total: 40326.375 MB, Remaining: 34944.637500000004 MB.
#####################################################################################
 # Sample Trajectories from the current policy 从当前策略采样
 #####################################################################################

[INFO|math_episode_generator_with_mc_advantages.py:72:2471912] 2024-11-24 22:39:31,519 >> Always generating from scratch
[INFO|on_policy_episode_generator.py:518:2471912] 2024-11-24 22:39:31,519 >> Rank #1 starting vLLM: model=rho-1b-sft-GSM8K   port=60991   seed=2746318313
[INFO|vllm_server.py:243:2471912] 2024-11-24 22:39:31,682 >> Server started with PID 2493847 on port 60991
[INFO|vllm_server.py:243:2471913] 2024-11-24 22:39:31,682 >> Server started with PID 2493848 on port 57413
[INFO|vllm_server.py:243:2471911] 2024-11-24 22:39:31,682 >> Server started with PID 2493846 on port 4779
[INFO|vllm_server.py:243:2471914] 2024-11-24 22:39:31,682 >> Server started with PID 2493845 on port 8417
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:40:31,873 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:40:31,873 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:40:31,874 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:40:31,875 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:19 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:19 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> ERROR 11-24 22:40:20 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:24 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:31,876 >> 
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:19 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:19 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> ERROR 11-24 22:40:20 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:24 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:31,876 >> 
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:19 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:19 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> ERROR 11-24 22:40:20 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:24 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:31,876 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:19 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:19 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> ERROR 11-24 22:40:20 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:20 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> INFO 11-24 22:40:24 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:31,876 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:40:40,906 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> INFO 11-24 22:40:36 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> INFO 11-24 22:40:36 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> INFO 11-24 22:40:36 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> ERROR 11-24 22:40:36 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> INFO 11-24 22:40:36 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> INFO 11-24 22:40:36 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> INFO 11-24 22:40:37 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:40,907 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:40:40,907 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> INFO 11-24 22:40:36 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> INFO 11-24 22:40:36 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> INFO 11-24 22:40:36 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> ERROR 11-24 22:40:36 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> INFO 11-24 22:40:36 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> INFO 11-24 22:40:36 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> INFO 11-24 22:40:37 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:40,908 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:40:41,909 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:40:41,909 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> INFO 11-24 22:40:36 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> INFO 11-24 22:40:36 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> INFO 11-24 22:40:36 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> ERROR 11-24 22:40:36 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> INFO 11-24 22:40:36 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> INFO 11-24 22:40:36 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> INFO 11-24 22:40:37 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:41,910 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> INFO 11-24 22:40:36 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> INFO 11-24 22:40:36 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> INFO 11-24 22:40:36 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> ERROR 11-24 22:40:36 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> INFO 11-24 22:40:36 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> INFO 11-24 22:40:36 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> INFO 11-24 22:40:37 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:41,911 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:40:48,933 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> INFO 11-24 22:40:45 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> INFO 11-24 22:40:45 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> INFO 11-24 22:40:45 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> ERROR 11-24 22:40:45 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> INFO 11-24 22:40:45 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> INFO 11-24 22:40:45 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> INFO 11-24 22:40:45 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:48,934 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:40:48,935 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> INFO 11-24 22:40:45 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> INFO 11-24 22:40:45 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> INFO 11-24 22:40:45 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> ERROR 11-24 22:40:45 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> INFO 11-24 22:40:45 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> INFO 11-24 22:40:45 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> INFO 11-24 22:40:45 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:48,936 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:40:49,937 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> INFO 11-24 22:40:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> INFO 11-24 22:40:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> INFO 11-24 22:40:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> ERROR 11-24 22:40:46 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> INFO 11-24 22:40:46 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> INFO 11-24 22:40:46 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> INFO 11-24 22:40:46 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:49,938 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:40:49,939 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> INFO 11-24 22:40:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> INFO 11-24 22:40:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> INFO 11-24 22:40:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> ERROR 11-24 22:40:46 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> INFO 11-24 22:40:46 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> INFO 11-24 22:40:46 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> INFO 11-24 22:40:47 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:49,940 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:40:56,960 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:40:56,961 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> ERROR 11-24 22:40:53 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:40:56,961 >> 
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> ERROR 11-24 22:40:53 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> INFO 11-24 22:40:53 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:40:56,961 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:40:57,965 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:40:57,965 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:54 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:54 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> ERROR 11-24 22:40:54 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:54 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:54 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:55 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:40:57,966 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:54 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:54 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> ERROR 11-24 22:40:54 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:54 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:54 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> INFO 11-24 22:40:55 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:40:57,966 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:41:04,990 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> INFO 11-24 22:41:01 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> INFO 11-24 22:41:01 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> INFO 11-24 22:41:01 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> ERROR 11-24 22:41:01 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> INFO 11-24 22:41:01 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> INFO 11-24 22:41:01 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> INFO 11-24 22:41:01 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:04,990 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:41:04,990 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> INFO 11-24 22:41:01 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> INFO 11-24 22:41:01 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> INFO 11-24 22:41:01 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> ERROR 11-24 22:41:01 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> INFO 11-24 22:41:01 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> INFO 11-24 22:41:01 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> INFO 11-24 22:41:01 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:04,991 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:41:05,992 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> INFO 11-24 22:41:02 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> INFO 11-24 22:41:02 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> INFO 11-24 22:41:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> ERROR 11-24 22:41:02 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> INFO 11-24 22:41:02 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> INFO 11-24 22:41:02 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> INFO 11-24 22:41:03 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:05,993 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:41:05,995 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> INFO 11-24 22:41:02 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> INFO 11-24 22:41:02 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> INFO 11-24 22:41:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> ERROR 11-24 22:41:02 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> INFO 11-24 22:41:02 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> INFO 11-24 22:41:02 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> INFO 11-24 22:41:03 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:05,996 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:41:13,015 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> INFO 11-24 22:41:09 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> INFO 11-24 22:41:09 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> INFO 11-24 22:41:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> ERROR 11-24 22:41:09 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> INFO 11-24 22:41:09 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> INFO 11-24 22:41:09 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> INFO 11-24 22:41:09 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:13,016 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:41:13,018 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> INFO 11-24 22:41:09 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> INFO 11-24 22:41:09 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> INFO 11-24 22:41:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> ERROR 11-24 22:41:09 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> INFO 11-24 22:41:09 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> INFO 11-24 22:41:09 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> INFO 11-24 22:41:09 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:13,018 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:41:14,020 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> INFO 11-24 22:41:10 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> INFO 11-24 22:41:10 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> INFO 11-24 22:41:10 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> ERROR 11-24 22:41:10 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> INFO 11-24 22:41:10 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> INFO 11-24 22:41:10 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> INFO 11-24 22:41:11 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:14,020 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:41:14,028 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> INFO 11-24 22:41:10 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> INFO 11-24 22:41:10 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> INFO 11-24 22:41:10 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> ERROR 11-24 22:41:10 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> INFO 11-24 22:41:10 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> INFO 11-24 22:41:10 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> INFO 11-24 22:41:11 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:14,029 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:41:21,043 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> INFO 11-24 22:41:17 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> INFO 11-24 22:41:17 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> INFO 11-24 22:41:17 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> ERROR 11-24 22:41:17 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> INFO 11-24 22:41:17 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> INFO 11-24 22:41:17 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> INFO 11-24 22:41:18 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:21,044 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:41:21,049 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> INFO 11-24 22:41:17 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> INFO 11-24 22:41:17 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> INFO 11-24 22:41:17 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> ERROR 11-24 22:41:17 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> INFO 11-24 22:41:17 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> INFO 11-24 22:41:17 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> INFO 11-24 22:41:18 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:21,050 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:41:22,047 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> INFO 11-24 22:41:18 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> INFO 11-24 22:41:18 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> INFO 11-24 22:41:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> ERROR 11-24 22:41:18 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> INFO 11-24 22:41:18 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> INFO 11-24 22:41:18 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> INFO 11-24 22:41:19 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:22,047 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:41:22,054 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> INFO 11-24 22:41:18 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> INFO 11-24 22:41:18 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> INFO 11-24 22:41:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> ERROR 11-24 22:41:18 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> INFO 11-24 22:41:18 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> INFO 11-24 22:41:18 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> INFO 11-24 22:41:19 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:22,055 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:41:29,072 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> INFO 11-24 22:41:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> INFO 11-24 22:41:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> INFO 11-24 22:41:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> ERROR 11-24 22:41:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> INFO 11-24 22:41:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> INFO 11-24 22:41:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> INFO 11-24 22:41:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:29,072 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:41:29,073 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> INFO 11-24 22:41:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> INFO 11-24 22:41:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> INFO 11-24 22:41:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> ERROR 11-24 22:41:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> INFO 11-24 22:41:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> INFO 11-24 22:41:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> INFO 11-24 22:41:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:29,073 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:41:30,075 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> INFO 11-24 22:41:26 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> INFO 11-24 22:41:26 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> INFO 11-24 22:41:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> ERROR 11-24 22:41:26 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> INFO 11-24 22:41:26 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> INFO 11-24 22:41:26 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> INFO 11-24 22:41:27 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:30,076 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:41:30,078 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> INFO 11-24 22:41:26 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> INFO 11-24 22:41:26 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> INFO 11-24 22:41:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> ERROR 11-24 22:41:26 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> INFO 11-24 22:41:26 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> INFO 11-24 22:41:26 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> INFO 11-24 22:41:27 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:30,079 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:41:37,100 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> INFO 11-24 22:41:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> INFO 11-24 22:41:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> INFO 11-24 22:41:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> ERROR 11-24 22:41:33 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> INFO 11-24 22:41:33 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> INFO 11-24 22:41:33 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> INFO 11-24 22:41:34 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:37,101 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:41:37,103 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> INFO 11-24 22:41:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> INFO 11-24 22:41:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> INFO 11-24 22:41:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> ERROR 11-24 22:41:33 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> INFO 11-24 22:41:33 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> INFO 11-24 22:41:33 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> INFO 11-24 22:41:34 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:37,104 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:41:38,103 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> INFO 11-24 22:41:34 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> INFO 11-24 22:41:34 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> INFO 11-24 22:41:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> ERROR 11-24 22:41:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> INFO 11-24 22:41:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> INFO 11-24 22:41:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> INFO 11-24 22:41:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:38,103 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:41:38,108 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> INFO 11-24 22:41:34 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> INFO 11-24 22:41:34 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> INFO 11-24 22:41:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> ERROR 11-24 22:41:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> INFO 11-24 22:41:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> INFO 11-24 22:41:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> INFO 11-24 22:41:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:38,109 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:41:45,128 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> INFO 11-24 22:41:41 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> INFO 11-24 22:41:41 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> INFO 11-24 22:41:41 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> ERROR 11-24 22:41:41 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> INFO 11-24 22:41:41 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> INFO 11-24 22:41:41 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> INFO 11-24 22:41:42 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:45,129 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:41:45,131 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> INFO 11-24 22:41:41 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> INFO 11-24 22:41:41 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> INFO 11-24 22:41:41 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> ERROR 11-24 22:41:41 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> INFO 11-24 22:41:41 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> INFO 11-24 22:41:41 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> INFO 11-24 22:41:42 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:45,132 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:41:46,130 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> INFO 11-24 22:41:42 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> INFO 11-24 22:41:42 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> INFO 11-24 22:41:42 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> ERROR 11-24 22:41:42 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> INFO 11-24 22:41:42 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> INFO 11-24 22:41:42 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> INFO 11-24 22:41:43 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:46,131 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:41:46,137 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> INFO 11-24 22:41:42 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> INFO 11-24 22:41:42 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> INFO 11-24 22:41:42 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> ERROR 11-24 22:41:42 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> INFO 11-24 22:41:42 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> INFO 11-24 22:41:42 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> INFO 11-24 22:41:43 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:46,138 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:41:53,154 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> INFO 11-24 22:41:49 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> INFO 11-24 22:41:49 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> INFO 11-24 22:41:49 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> ERROR 11-24 22:41:49 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> INFO 11-24 22:41:49 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> INFO 11-24 22:41:49 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> INFO 11-24 22:41:50 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:41:53,155 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:41:53,156 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> INFO 11-24 22:41:49 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> INFO 11-24 22:41:49 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> INFO 11-24 22:41:49 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> ERROR 11-24 22:41:49 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> INFO 11-24 22:41:49 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> INFO 11-24 22:41:49 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> INFO 11-24 22:41:50 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:41:53,157 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:41:54,159 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> INFO 11-24 22:41:50 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> INFO 11-24 22:41:50 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> INFO 11-24 22:41:50 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> ERROR 11-24 22:41:50 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> INFO 11-24 22:41:50 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> INFO 11-24 22:41:50 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> INFO 11-24 22:41:51 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:41:54,160 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:41:54,173 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> INFO 11-24 22:41:50 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> INFO 11-24 22:41:50 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> INFO 11-24 22:41:50 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> ERROR 11-24 22:41:50 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> INFO 11-24 22:41:50 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> INFO 11-24 22:41:50 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> INFO 11-24 22:41:51 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:41:54,174 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:42:01,182 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> INFO 11-24 22:41:57 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> INFO 11-24 22:41:57 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> INFO 11-24 22:41:57 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> ERROR 11-24 22:41:57 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> INFO 11-24 22:41:57 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> INFO 11-24 22:41:57 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> INFO 11-24 22:41:58 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:01,182 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:42:01,183 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> INFO 11-24 22:41:57 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> INFO 11-24 22:41:57 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> INFO 11-24 22:41:57 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> ERROR 11-24 22:41:57 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> INFO 11-24 22:41:57 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> INFO 11-24 22:41:57 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> INFO 11-24 22:41:58 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:01,183 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:42:02,185 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> INFO 11-24 22:41:58 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> INFO 11-24 22:41:58 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> INFO 11-24 22:41:58 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> ERROR 11-24 22:41:58 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> INFO 11-24 22:41:58 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> INFO 11-24 22:41:58 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> INFO 11-24 22:41:59 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:02,186 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:42:02,200 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> INFO 11-24 22:41:58 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> INFO 11-24 22:41:58 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> INFO 11-24 22:41:58 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> ERROR 11-24 22:41:58 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> INFO 11-24 22:41:58 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> INFO 11-24 22:41:58 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> INFO 11-24 22:41:59 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:02,201 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:42:09,212 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:42:09,213 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:05 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:05 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> ERROR 11-24 22:42:05 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:05 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:05 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:06 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:09,214 >> 
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:05 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:05 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> ERROR 11-24 22:42:05 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:05 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:05 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> INFO 11-24 22:42:06 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:09,214 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:42:10,216 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> INFO 11-24 22:42:06 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> INFO 11-24 22:42:06 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> INFO 11-24 22:42:06 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> ERROR 11-24 22:42:06 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> INFO 11-24 22:42:06 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> INFO 11-24 22:42:06 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> INFO 11-24 22:42:07 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:10,217 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:42:10,229 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> INFO 11-24 22:42:06 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> INFO 11-24 22:42:06 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> INFO 11-24 22:42:06 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> ERROR 11-24 22:42:06 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> INFO 11-24 22:42:06 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> INFO 11-24 22:42:06 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> INFO 11-24 22:42:07 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:10,230 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:42:17,239 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> INFO 11-24 22:42:13 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> INFO 11-24 22:42:13 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> INFO 11-24 22:42:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> ERROR 11-24 22:42:13 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> INFO 11-24 22:42:13 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> INFO 11-24 22:42:13 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> INFO 11-24 22:42:14 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:17,240 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:42:17,241 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> INFO 11-24 22:42:13 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> INFO 11-24 22:42:13 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> INFO 11-24 22:42:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> ERROR 11-24 22:42:13 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> INFO 11-24 22:42:13 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> INFO 11-24 22:42:13 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> INFO 11-24 22:42:14 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:17,242 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:42:18,242 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> INFO 11-24 22:42:14 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> INFO 11-24 22:42:14 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> INFO 11-24 22:42:14 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> ERROR 11-24 22:42:14 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> INFO 11-24 22:42:14 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> INFO 11-24 22:42:14 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> INFO 11-24 22:42:15 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:18,243 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:42:18,255 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> INFO 11-24 22:42:14 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> INFO 11-24 22:42:14 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> INFO 11-24 22:42:14 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> ERROR 11-24 22:42:14 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> INFO 11-24 22:42:14 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> INFO 11-24 22:42:14 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> INFO 11-24 22:42:15 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:18,256 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:42:25,268 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> INFO 11-24 22:42:21 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> INFO 11-24 22:42:21 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> INFO 11-24 22:42:21 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> ERROR 11-24 22:42:21 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> INFO 11-24 22:42:21 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> INFO 11-24 22:42:21 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> INFO 11-24 22:42:22 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:25,269 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:42:25,269 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> INFO 11-24 22:42:21 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> INFO 11-24 22:42:21 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> INFO 11-24 22:42:21 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> ERROR 11-24 22:42:21 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> INFO 11-24 22:42:21 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> INFO 11-24 22:42:21 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> INFO 11-24 22:42:22 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:25,270 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:42:26,270 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> INFO 11-24 22:42:22 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> INFO 11-24 22:42:22 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> INFO 11-24 22:42:22 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> ERROR 11-24 22:42:22 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> INFO 11-24 22:42:22 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> INFO 11-24 22:42:22 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> INFO 11-24 22:42:23 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:26,271 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:42:26,284 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> INFO 11-24 22:42:22 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> INFO 11-24 22:42:22 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> INFO 11-24 22:42:22 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> ERROR 11-24 22:42:22 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> INFO 11-24 22:42:22 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> INFO 11-24 22:42:22 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> INFO 11-24 22:42:23 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:26,285 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:42:33,295 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:29 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:29 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:29 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> ERROR 11-24 22:42:29 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:29 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:29 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:30 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:33,296 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:42:33,296 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:29 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:29 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:29 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> ERROR 11-24 22:42:29 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:29 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:29 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> INFO 11-24 22:42:30 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:33,296 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:42:34,297 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> INFO 11-24 22:42:30 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> INFO 11-24 22:42:30 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> INFO 11-24 22:42:30 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> ERROR 11-24 22:42:30 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> INFO 11-24 22:42:30 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> INFO 11-24 22:42:30 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> INFO 11-24 22:42:31 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:34,298 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:42:34,309 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> INFO 11-24 22:42:30 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> INFO 11-24 22:42:30 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> INFO 11-24 22:42:30 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> ERROR 11-24 22:42:30 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> INFO 11-24 22:42:30 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> INFO 11-24 22:42:30 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> INFO 11-24 22:42:31 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:34,310 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:42:41,321 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> INFO 11-24 22:42:37 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> INFO 11-24 22:42:37 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> INFO 11-24 22:42:37 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> ERROR 11-24 22:42:37 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> INFO 11-24 22:42:37 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> INFO 11-24 22:42:37 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> INFO 11-24 22:42:38 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:41,321 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:42:41,323 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> INFO 11-24 22:42:37 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> INFO 11-24 22:42:37 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> INFO 11-24 22:42:37 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> ERROR 11-24 22:42:37 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> INFO 11-24 22:42:37 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> INFO 11-24 22:42:37 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> INFO 11-24 22:42:38 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:41,324 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:42:42,324 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> INFO 11-24 22:42:38 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> INFO 11-24 22:42:38 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> INFO 11-24 22:42:38 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> ERROR 11-24 22:42:38 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> INFO 11-24 22:42:38 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> INFO 11-24 22:42:38 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> INFO 11-24 22:42:39 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:42,325 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:42:42,336 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> INFO 11-24 22:42:38 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> INFO 11-24 22:42:38 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> INFO 11-24 22:42:38 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> ERROR 11-24 22:42:38 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> INFO 11-24 22:42:38 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> INFO 11-24 22:42:38 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> INFO 11-24 22:42:39 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:42,337 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:42:49,348 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> INFO 11-24 22:42:45 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> INFO 11-24 22:42:45 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> INFO 11-24 22:42:45 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> ERROR 11-24 22:42:45 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> INFO 11-24 22:42:45 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> INFO 11-24 22:42:45 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> INFO 11-24 22:42:46 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:49,349 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:42:49,349 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> INFO 11-24 22:42:45 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> INFO 11-24 22:42:45 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> INFO 11-24 22:42:45 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> ERROR 11-24 22:42:45 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> INFO 11-24 22:42:45 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> INFO 11-24 22:42:45 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> INFO 11-24 22:42:46 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:49,350 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:42:50,352 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> INFO 11-24 22:42:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> INFO 11-24 22:42:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> INFO 11-24 22:42:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> ERROR 11-24 22:42:46 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> INFO 11-24 22:42:46 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> INFO 11-24 22:42:46 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> INFO 11-24 22:42:47 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:50,353 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:42:50,362 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> INFO 11-24 22:42:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> INFO 11-24 22:42:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> INFO 11-24 22:42:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> ERROR 11-24 22:42:46 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> INFO 11-24 22:42:46 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> INFO 11-24 22:42:46 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> INFO 11-24 22:42:47 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:50,363 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:42:57,376 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> INFO 11-24 22:42:53 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> INFO 11-24 22:42:53 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> INFO 11-24 22:42:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> ERROR 11-24 22:42:53 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> INFO 11-24 22:42:53 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> INFO 11-24 22:42:53 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> INFO 11-24 22:42:54 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:42:57,377 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:42:57,378 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> INFO 11-24 22:42:53 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> INFO 11-24 22:42:53 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> INFO 11-24 22:42:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> ERROR 11-24 22:42:53 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> INFO 11-24 22:42:53 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> INFO 11-24 22:42:53 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> INFO 11-24 22:42:54 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:42:57,379 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:42:58,379 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> INFO 11-24 22:42:54 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> INFO 11-24 22:42:54 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> INFO 11-24 22:42:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> ERROR 11-24 22:42:54 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> INFO 11-24 22:42:54 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> INFO 11-24 22:42:54 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> INFO 11-24 22:42:55 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:42:58,380 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:42:58,386 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> INFO 11-24 22:42:54 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> INFO 11-24 22:42:54 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> INFO 11-24 22:42:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> ERROR 11-24 22:42:54 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> INFO 11-24 22:42:54 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> INFO 11-24 22:42:54 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> INFO 11-24 22:42:55 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:42:58,387 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:43:05,404 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:43:05,404 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:01 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:01 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:01 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> ERROR 11-24 22:43:01 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:01 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:01 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:02 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:05,405 >> 
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:01 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:01 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:01 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> ERROR 11-24 22:43:01 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:01 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:01 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> INFO 11-24 22:43:02 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:05,405 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:43:06,408 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> INFO 11-24 22:43:02 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> INFO 11-24 22:43:02 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> INFO 11-24 22:43:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> ERROR 11-24 22:43:02 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> INFO 11-24 22:43:02 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> INFO 11-24 22:43:02 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> INFO 11-24 22:43:03 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:06,409 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:43:06,410 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> INFO 11-24 22:43:02 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> INFO 11-24 22:43:02 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> INFO 11-24 22:43:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> ERROR 11-24 22:43:02 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> INFO 11-24 22:43:02 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> INFO 11-24 22:43:02 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> INFO 11-24 22:43:03 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:06,413 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:43:13,433 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> INFO 11-24 22:43:09 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> INFO 11-24 22:43:09 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> INFO 11-24 22:43:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> ERROR 11-24 22:43:09 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> INFO 11-24 22:43:09 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> INFO 11-24 22:43:09 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> INFO 11-24 22:43:10 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:13,434 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:43:13,436 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> INFO 11-24 22:43:09 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> INFO 11-24 22:43:09 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> INFO 11-24 22:43:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> ERROR 11-24 22:43:09 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> INFO 11-24 22:43:09 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> INFO 11-24 22:43:09 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> INFO 11-24 22:43:10 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:13,437 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:43:14,435 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> INFO 11-24 22:43:10 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> INFO 11-24 22:43:10 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> INFO 11-24 22:43:10 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> ERROR 11-24 22:43:10 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> INFO 11-24 22:43:10 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> INFO 11-24 22:43:10 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> INFO 11-24 22:43:11 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:14,436 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:43:14,437 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> INFO 11-24 22:43:10 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> INFO 11-24 22:43:10 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> INFO 11-24 22:43:10 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> ERROR 11-24 22:43:10 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> INFO 11-24 22:43:10 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> INFO 11-24 22:43:10 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> INFO 11-24 22:43:11 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:14,438 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:43:21,459 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:43:21,461 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:17 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:17 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:17 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> ERROR 11-24 22:43:17 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:17 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:17 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:18 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:21,462 >> 
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:17 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:17 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:17 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> ERROR 11-24 22:43:17 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:17 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:17 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> INFO 11-24 22:43:18 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:21,462 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:43:22,463 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:43:22,463 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:18 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:18 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> ERROR 11-24 22:43:18 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:18 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:18 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:19 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:22,464 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:18 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:18 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> ERROR 11-24 22:43:18 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:18 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:18 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> INFO 11-24 22:43:19 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:22,464 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:43:29,488 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> INFO 11-24 22:43:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> INFO 11-24 22:43:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> INFO 11-24 22:43:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> ERROR 11-24 22:43:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> INFO 11-24 22:43:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> INFO 11-24 22:43:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> INFO 11-24 22:43:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:29,491 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:43:29,493 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> INFO 11-24 22:43:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> INFO 11-24 22:43:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> INFO 11-24 22:43:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> ERROR 11-24 22:43:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> INFO 11-24 22:43:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> INFO 11-24 22:43:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> INFO 11-24 22:43:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:29,494 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:43:30,490 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:43:30,491 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:26 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:26 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> ERROR 11-24 22:43:26 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:26 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:26 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:27 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:30,491 >> 
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:26 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:26 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> ERROR 11-24 22:43:26 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:26 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:26 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> INFO 11-24 22:43:27 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:30,491 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:43:37,516 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> INFO 11-24 22:43:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> INFO 11-24 22:43:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> INFO 11-24 22:43:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> ERROR 11-24 22:43:33 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> INFO 11-24 22:43:33 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> INFO 11-24 22:43:33 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> INFO 11-24 22:43:34 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:37,517 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:43:37,518 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> INFO 11-24 22:43:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> INFO 11-24 22:43:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> INFO 11-24 22:43:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> ERROR 11-24 22:43:33 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> INFO 11-24 22:43:33 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> INFO 11-24 22:43:33 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> INFO 11-24 22:43:34 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:37,518 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:43:38,517 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> INFO 11-24 22:43:34 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> INFO 11-24 22:43:34 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> INFO 11-24 22:43:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> ERROR 11-24 22:43:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> INFO 11-24 22:43:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> INFO 11-24 22:43:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> INFO 11-24 22:43:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:38,518 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:43:38,519 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> INFO 11-24 22:43:34 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> INFO 11-24 22:43:34 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> INFO 11-24 22:43:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> ERROR 11-24 22:43:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> INFO 11-24 22:43:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> INFO 11-24 22:43:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> INFO 11-24 22:43:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:38,520 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:43:45,541 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> INFO 11-24 22:43:41 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> INFO 11-24 22:43:41 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> INFO 11-24 22:43:41 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> ERROR 11-24 22:43:41 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> INFO 11-24 22:43:41 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> INFO 11-24 22:43:41 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> INFO 11-24 22:43:42 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:45,542 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:43:45,543 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> INFO 11-24 22:43:41 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> INFO 11-24 22:43:41 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> INFO 11-24 22:43:41 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> ERROR 11-24 22:43:41 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> INFO 11-24 22:43:41 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> INFO 11-24 22:43:41 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> INFO 11-24 22:43:42 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:45,543 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:43:46,544 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> INFO 11-24 22:43:42 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> INFO 11-24 22:43:42 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> INFO 11-24 22:43:42 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> ERROR 11-24 22:43:42 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> INFO 11-24 22:43:42 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> INFO 11-24 22:43:42 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> INFO 11-24 22:43:43 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:46,545 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:43:46,547 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> INFO 11-24 22:43:42 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> INFO 11-24 22:43:42 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> INFO 11-24 22:43:42 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> ERROR 11-24 22:43:42 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> INFO 11-24 22:43:42 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> INFO 11-24 22:43:42 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> INFO 11-24 22:43:43 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:46,548 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:43:53,568 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:43:53,570 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:49 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:49 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:49 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> ERROR 11-24 22:43:49 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:49 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:49 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:50 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:43:53,571 >> 
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:49 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:49 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:49 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> ERROR 11-24 22:43:49 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:49 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:49 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> INFO 11-24 22:43:50 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:43:53,571 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:43:54,571 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> INFO 11-24 22:43:50 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> INFO 11-24 22:43:50 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> INFO 11-24 22:43:50 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> ERROR 11-24 22:43:50 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> INFO 11-24 22:43:50 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> INFO 11-24 22:43:50 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> INFO 11-24 22:43:51 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:43:54,572 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:43:54,573 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> INFO 11-24 22:43:50 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> INFO 11-24 22:43:50 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> INFO 11-24 22:43:50 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> ERROR 11-24 22:43:50 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> INFO 11-24 22:43:50 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> INFO 11-24 22:43:50 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> INFO 11-24 22:43:51 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:43:54,574 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:44:01,596 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> INFO 11-24 22:43:57 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> INFO 11-24 22:43:57 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> INFO 11-24 22:43:57 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> ERROR 11-24 22:43:57 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> INFO 11-24 22:43:57 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> INFO 11-24 22:43:57 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> INFO 11-24 22:43:58 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:01,596 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:44:01,607 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> INFO 11-24 22:43:57 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> INFO 11-24 22:43:57 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> INFO 11-24 22:43:57 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> ERROR 11-24 22:43:57 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> INFO 11-24 22:43:57 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> INFO 11-24 22:43:57 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> INFO 11-24 22:43:58 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:01,608 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:44:02,598 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> INFO 11-24 22:43:58 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> INFO 11-24 22:43:58 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> INFO 11-24 22:43:58 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> ERROR 11-24 22:43:58 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> INFO 11-24 22:43:58 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> INFO 11-24 22:43:58 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> INFO 11-24 22:43:59 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:02,599 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:44:02,600 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> INFO 11-24 22:43:58 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> INFO 11-24 22:43:58 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> INFO 11-24 22:43:58 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> ERROR 11-24 22:43:58 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> INFO 11-24 22:43:58 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> INFO 11-24 22:43:58 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> INFO 11-24 22:43:59 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:02,600 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:44:09,623 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> INFO 11-24 22:44:05 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> INFO 11-24 22:44:05 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> INFO 11-24 22:44:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> ERROR 11-24 22:44:05 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> INFO 11-24 22:44:05 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> INFO 11-24 22:44:05 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> INFO 11-24 22:44:06 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:09,624 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:44:09,632 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> INFO 11-24 22:44:05 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> INFO 11-24 22:44:05 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> INFO 11-24 22:44:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> ERROR 11-24 22:44:05 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> INFO 11-24 22:44:05 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> INFO 11-24 22:44:05 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> INFO 11-24 22:44:06 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:09,632 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:44:10,627 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:44:10,628 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:06 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:06 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:06 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> ERROR 11-24 22:44:06 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:06 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:06 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:07 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:10,630 >> 
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:06 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:06 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:06 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> ERROR 11-24 22:44:06 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:06 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:06 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> INFO 11-24 22:44:07 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:10,630 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:44:17,650 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> INFO 11-24 22:44:13 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> INFO 11-24 22:44:13 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> INFO 11-24 22:44:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> ERROR 11-24 22:44:13 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> INFO 11-24 22:44:13 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> INFO 11-24 22:44:13 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> INFO 11-24 22:44:14 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:17,651 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:44:17,656 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> INFO 11-24 22:44:13 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> INFO 11-24 22:44:13 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> INFO 11-24 22:44:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> ERROR 11-24 22:44:13 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> INFO 11-24 22:44:13 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> INFO 11-24 22:44:13 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> INFO 11-24 22:44:14 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:17,657 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:44:18,655 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:44:18,656 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:14 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:14 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:14 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> ERROR 11-24 22:44:14 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:14 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:14 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:15 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:18,656 >> 
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:14 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:14 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:14 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> ERROR 11-24 22:44:14 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:14 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:14 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> INFO 11-24 22:44:15 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:18,656 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:44:25,676 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> INFO 11-24 22:44:21 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> INFO 11-24 22:44:21 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> INFO 11-24 22:44:21 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> ERROR 11-24 22:44:21 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> INFO 11-24 22:44:21 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> INFO 11-24 22:44:21 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> INFO 11-24 22:44:22 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:25,677 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:44:25,681 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> INFO 11-24 22:44:21 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> INFO 11-24 22:44:21 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> INFO 11-24 22:44:21 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> ERROR 11-24 22:44:21 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> INFO 11-24 22:44:21 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> INFO 11-24 22:44:21 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> INFO 11-24 22:44:22 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:25,682 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:44:26,682 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:44:26,683 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:22 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:22 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:22 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> ERROR 11-24 22:44:22 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:22 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:22 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:23 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:26,684 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:22 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:22 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:22 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> ERROR 11-24 22:44:22 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:22 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:22 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> INFO 11-24 22:44:23 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:26,684 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:44:33,703 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:44:33,704 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:29 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:29 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:29 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> ERROR 11-24 22:44:29 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:29 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:29 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:30 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:33,706 >> 
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:29 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:29 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:29 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> ERROR 11-24 22:44:29 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:29 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:29 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> INFO 11-24 22:44:30 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:33,706 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:44:34,712 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> INFO 11-24 22:44:30 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> INFO 11-24 22:44:30 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> INFO 11-24 22:44:30 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> ERROR 11-24 22:44:30 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> INFO 11-24 22:44:30 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> INFO 11-24 22:44:30 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> INFO 11-24 22:44:31 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:34,713 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:44:34,723 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> INFO 11-24 22:44:30 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> INFO 11-24 22:44:30 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> INFO 11-24 22:44:30 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> ERROR 11-24 22:44:30 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> INFO 11-24 22:44:30 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> INFO 11-24 22:44:30 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> INFO 11-24 22:44:31 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:34,724 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:44:41,732 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> INFO 11-24 22:44:37 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> INFO 11-24 22:44:37 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> INFO 11-24 22:44:37 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> ERROR 11-24 22:44:37 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> INFO 11-24 22:44:37 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> INFO 11-24 22:44:37 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> INFO 11-24 22:44:38 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:41,733 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:44:41,741 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> INFO 11-24 22:44:37 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> INFO 11-24 22:44:37 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> INFO 11-24 22:44:37 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> ERROR 11-24 22:44:37 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> INFO 11-24 22:44:37 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> INFO 11-24 22:44:37 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> INFO 11-24 22:44:38 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:41,742 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:44:42,735 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> INFO 11-24 22:44:38 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> INFO 11-24 22:44:38 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> INFO 11-24 22:44:38 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> ERROR 11-24 22:44:38 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> INFO 11-24 22:44:38 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> INFO 11-24 22:44:38 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> INFO 11-24 22:44:39 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:42,736 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:44:42,750 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> INFO 11-24 22:44:38 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> INFO 11-24 22:44:38 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> INFO 11-24 22:44:38 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> ERROR 11-24 22:44:38 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> INFO 11-24 22:44:38 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> INFO 11-24 22:44:38 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> INFO 11-24 22:44:39 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:42,751 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:44:49,760 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> INFO 11-24 22:44:45 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> INFO 11-24 22:44:45 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> INFO 11-24 22:44:45 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> ERROR 11-24 22:44:45 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> INFO 11-24 22:44:45 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> INFO 11-24 22:44:45 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> INFO 11-24 22:44:46 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:49,761 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:44:49,765 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> INFO 11-24 22:44:45 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> INFO 11-24 22:44:45 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> INFO 11-24 22:44:45 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> ERROR 11-24 22:44:45 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> INFO 11-24 22:44:45 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> INFO 11-24 22:44:45 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> INFO 11-24 22:44:46 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:49,765 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:44:50,762 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> INFO 11-24 22:44:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> INFO 11-24 22:44:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> INFO 11-24 22:44:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> ERROR 11-24 22:44:46 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> INFO 11-24 22:44:46 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> INFO 11-24 22:44:46 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> INFO 11-24 22:44:47 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:50,763 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:44:50,776 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> INFO 11-24 22:44:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> INFO 11-24 22:44:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> INFO 11-24 22:44:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> ERROR 11-24 22:44:46 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> INFO 11-24 22:44:46 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> INFO 11-24 22:44:46 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> INFO 11-24 22:44:47 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:50,777 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:44:57,789 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:44:57,789 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:53 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:53 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> ERROR 11-24 22:44:53 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:53 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:53 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:54 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:44:57,792 >> 
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:53 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:53 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> ERROR 11-24 22:44:53 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:53 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:53 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> INFO 11-24 22:44:54 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:44:57,792 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:44:58,789 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> INFO 11-24 22:44:54 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> INFO 11-24 22:44:54 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> INFO 11-24 22:44:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> ERROR 11-24 22:44:54 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> INFO 11-24 22:44:54 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> INFO 11-24 22:44:54 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> INFO 11-24 22:44:55 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:44:58,790 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:44:58,802 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> INFO 11-24 22:44:54 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> INFO 11-24 22:44:54 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> INFO 11-24 22:44:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> ERROR 11-24 22:44:54 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> INFO 11-24 22:44:54 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> INFO 11-24 22:44:54 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> INFO 11-24 22:44:55 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:44:58,803 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:45:05,818 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:45:05,819 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:01 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:01 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:01 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> ERROR 11-24 22:45:01 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:01 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:01 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:02 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:05,820 >> 
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:01 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:01 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:01 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> ERROR 11-24 22:45:01 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:01 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:01 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> INFO 11-24 22:45:02 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:05,820 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:45:06,819 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> INFO 11-24 22:45:02 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> INFO 11-24 22:45:02 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> INFO 11-24 22:45:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> ERROR 11-24 22:45:02 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> INFO 11-24 22:45:02 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> INFO 11-24 22:45:02 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> INFO 11-24 22:45:03 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:06,820 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:45:06,826 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> INFO 11-24 22:45:02 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> INFO 11-24 22:45:02 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> INFO 11-24 22:45:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> ERROR 11-24 22:45:02 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> INFO 11-24 22:45:02 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> INFO 11-24 22:45:02 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> INFO 11-24 22:45:03 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:06,827 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:45:13,844 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:45:13,846 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:09 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:09 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> ERROR 11-24 22:45:09 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:09 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:09 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:10 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:13,904 >> 
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:09 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:09 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> ERROR 11-24 22:45:09 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:09 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:09 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> INFO 11-24 22:45:10 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:13,904 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:45:14,847 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> INFO 11-24 22:45:10 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> INFO 11-24 22:45:10 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> INFO 11-24 22:45:10 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> ERROR 11-24 22:45:10 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> INFO 11-24 22:45:10 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> INFO 11-24 22:45:10 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> INFO 11-24 22:45:11 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:14,848 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:45:14,851 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> INFO 11-24 22:45:10 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> INFO 11-24 22:45:10 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> INFO 11-24 22:45:10 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> ERROR 11-24 22:45:10 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> INFO 11-24 22:45:10 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> INFO 11-24 22:45:10 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> INFO 11-24 22:45:11 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:14,852 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:45:21,929 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:45:21,929 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:17 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:17 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:17 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> ERROR 11-24 22:45:17 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:17 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:17 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:18 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:21,929 >> 
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:17 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:17 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:17 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> ERROR 11-24 22:45:17 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:17 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:17 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> INFO 11-24 22:45:18 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:21,929 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:45:22,874 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> INFO 11-24 22:45:18 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> INFO 11-24 22:45:18 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> INFO 11-24 22:45:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> ERROR 11-24 22:45:18 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> INFO 11-24 22:45:18 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> INFO 11-24 22:45:18 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> INFO 11-24 22:45:19 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:22,875 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:45:22,875 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> INFO 11-24 22:45:18 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> INFO 11-24 22:45:18 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> INFO 11-24 22:45:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> ERROR 11-24 22:45:18 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> INFO 11-24 22:45:18 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> INFO 11-24 22:45:18 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> INFO 11-24 22:45:19 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:22,876 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:45:29,956 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> INFO 11-24 22:45:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> INFO 11-24 22:45:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> INFO 11-24 22:45:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> ERROR 11-24 22:45:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> INFO 11-24 22:45:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> INFO 11-24 22:45:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> INFO 11-24 22:45:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:29,956 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:45:29,956 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> INFO 11-24 22:45:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> INFO 11-24 22:45:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> INFO 11-24 22:45:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> ERROR 11-24 22:45:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> INFO 11-24 22:45:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> INFO 11-24 22:45:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> INFO 11-24 22:45:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:29,957 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:45:30,901 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> INFO 11-24 22:45:26 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> INFO 11-24 22:45:26 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> INFO 11-24 22:45:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> ERROR 11-24 22:45:26 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> INFO 11-24 22:45:26 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> INFO 11-24 22:45:26 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> INFO 11-24 22:45:27 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:30,902 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:45:30,902 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> INFO 11-24 22:45:26 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> INFO 11-24 22:45:26 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> INFO 11-24 22:45:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> ERROR 11-24 22:45:26 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> INFO 11-24 22:45:26 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> INFO 11-24 22:45:26 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> INFO 11-24 22:45:27 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:30,903 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:45:37,982 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> INFO 11-24 22:45:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> INFO 11-24 22:45:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> INFO 11-24 22:45:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> ERROR 11-24 22:45:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> INFO 11-24 22:45:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> INFO 11-24 22:45:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> INFO 11-24 22:45:34 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:37,983 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:45:37,984 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> INFO 11-24 22:45:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> INFO 11-24 22:45:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> INFO 11-24 22:45:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> ERROR 11-24 22:45:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> INFO 11-24 22:45:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> INFO 11-24 22:45:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> INFO 11-24 22:45:34 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:37,984 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:45:38,930 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:45:38,930 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:34 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:34 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> ERROR 11-24 22:45:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:38,931 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:34 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:34 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> ERROR 11-24 22:45:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> INFO 11-24 22:45:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:38,931 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:45:45,004 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> INFO 11-24 22:45:41 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> INFO 11-24 22:45:41 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> INFO 11-24 22:45:41 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> ERROR 11-24 22:45:42 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> INFO 11-24 22:45:42 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> INFO 11-24 22:45:42 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> INFO 11-24 22:45:42 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:45,005 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:45:46,009 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> INFO 11-24 22:45:42 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> INFO 11-24 22:45:42 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> INFO 11-24 22:45:42 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> ERROR 11-24 22:45:42 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> INFO 11-24 22:45:42 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> INFO 11-24 22:45:42 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> INFO 11-24 22:45:42 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:46,010 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:45:46,956 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:45:46,957 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:42 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:42 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:42 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> ERROR 11-24 22:45:43 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:43 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:43 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:43 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:46,957 >> 
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:42 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:42 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:42 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> ERROR 11-24 22:45:43 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:43 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:43 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> INFO 11-24 22:45:43 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:46,957 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:45:52,029 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> INFO 11-24 22:45:48 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> INFO 11-24 22:45:48 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> INFO 11-24 22:45:49 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> ERROR 11-24 22:45:49 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> INFO 11-24 22:45:49 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> INFO 11-24 22:45:49 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> INFO 11-24 22:45:49 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:52,029 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:45:53,032 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> INFO 11-24 22:45:49 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> INFO 11-24 22:45:49 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> INFO 11-24 22:45:49 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> ERROR 11-24 22:45:49 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> INFO 11-24 22:45:49 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> INFO 11-24 22:45:49 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> INFO 11-24 22:45:50 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:45:53,032 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:45:54,983 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> INFO 11-24 22:45:50 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> INFO 11-24 22:45:50 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> INFO 11-24 22:45:50 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> ERROR 11-24 22:45:51 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> INFO 11-24 22:45:51 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> INFO 11-24 22:45:51 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> INFO 11-24 22:45:51 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:45:54,984 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:45:54,989 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> INFO 11-24 22:45:50 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> INFO 11-24 22:45:50 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> INFO 11-24 22:45:50 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> ERROR 11-24 22:45:50 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> INFO 11-24 22:45:50 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> INFO 11-24 22:45:50 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> INFO 11-24 22:45:51 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:45:54,990 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:45:59,052 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> INFO 11-24 22:45:56 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> INFO 11-24 22:45:56 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> INFO 11-24 22:45:56 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> ERROR 11-24 22:45:56 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> INFO 11-24 22:45:56 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> INFO 11-24 22:45:56 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> INFO 11-24 22:45:56 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:45:59,053 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:46:00,055 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> INFO 11-24 22:45:56 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> INFO 11-24 22:45:56 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> INFO 11-24 22:45:56 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> ERROR 11-24 22:45:57 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> INFO 11-24 22:45:57 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> INFO 11-24 22:45:57 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> INFO 11-24 22:45:57 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:00,056 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:46:03,009 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> INFO 11-24 22:45:58 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> INFO 11-24 22:45:58 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> INFO 11-24 22:45:58 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> ERROR 11-24 22:45:59 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> INFO 11-24 22:45:59 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> INFO 11-24 22:45:59 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> INFO 11-24 22:45:59 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:03,010 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:46:03,015 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> INFO 11-24 22:45:58 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> INFO 11-24 22:45:58 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> INFO 11-24 22:45:58 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> ERROR 11-24 22:45:59 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> INFO 11-24 22:45:59 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> INFO 11-24 22:45:59 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> INFO 11-24 22:45:59 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:03,016 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:46:06,074 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> INFO 11-24 22:46:03 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> INFO 11-24 22:46:03 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> INFO 11-24 22:46:03 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> ERROR 11-24 22:46:03 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> INFO 11-24 22:46:03 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> INFO 11-24 22:46:03 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> INFO 11-24 22:46:03 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:06,075 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:46:07,078 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> INFO 11-24 22:46:04 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> INFO 11-24 22:46:04 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> INFO 11-24 22:46:04 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> ERROR 11-24 22:46:04 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> INFO 11-24 22:46:04 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> INFO 11-24 22:46:04 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> INFO 11-24 22:46:04 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:07,078 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:46:11,036 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> INFO 11-24 22:46:07 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> INFO 11-24 22:46:07 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> INFO 11-24 22:46:07 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> ERROR 11-24 22:46:07 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> INFO 11-24 22:46:07 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> INFO 11-24 22:46:07 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> INFO 11-24 22:46:07 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:11,037 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:46:11,042 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> INFO 11-24 22:46:07 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> INFO 11-24 22:46:07 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> INFO 11-24 22:46:07 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> ERROR 11-24 22:46:07 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> INFO 11-24 22:46:07 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> INFO 11-24 22:46:07 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> INFO 11-24 22:46:07 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:11,043 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:46:13,097 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> INFO 11-24 22:46:10 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> INFO 11-24 22:46:10 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> INFO 11-24 22:46:10 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> ERROR 11-24 22:46:10 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> INFO 11-24 22:46:10 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> INFO 11-24 22:46:10 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> INFO 11-24 22:46:10 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:13,098 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:46:14,101 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> INFO 11-24 22:46:11 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> INFO 11-24 22:46:11 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> INFO 11-24 22:46:11 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> ERROR 11-24 22:46:11 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> INFO 11-24 22:46:11 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> INFO 11-24 22:46:11 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> INFO 11-24 22:46:11 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:14,101 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:46:19,065 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> INFO 11-24 22:46:15 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> INFO 11-24 22:46:15 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> INFO 11-24 22:46:15 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> ERROR 11-24 22:46:15 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> INFO 11-24 22:46:15 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> INFO 11-24 22:46:15 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> INFO 11-24 22:46:15 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:19,066 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:46:19,066 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> INFO 11-24 22:46:15 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> INFO 11-24 22:46:15 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> INFO 11-24 22:46:15 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> ERROR 11-24 22:46:15 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> INFO 11-24 22:46:15 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> INFO 11-24 22:46:15 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> INFO 11-24 22:46:15 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:19,067 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:46:20,120 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> INFO 11-24 22:46:17 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> INFO 11-24 22:46:17 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> INFO 11-24 22:46:17 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> ERROR 11-24 22:46:17 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> INFO 11-24 22:46:17 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> INFO 11-24 22:46:17 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> INFO 11-24 22:46:17 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:20,121 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:46:21,124 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> INFO 11-24 22:46:18 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> INFO 11-24 22:46:18 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> INFO 11-24 22:46:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> ERROR 11-24 22:46:18 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> INFO 11-24 22:46:18 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> INFO 11-24 22:46:18 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> INFO 11-24 22:46:18 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:21,125 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:46:27,092 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> INFO 11-24 22:46:23 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> INFO 11-24 22:46:23 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> INFO 11-24 22:46:23 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> ERROR 11-24 22:46:23 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> INFO 11-24 22:46:23 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> INFO 11-24 22:46:23 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> INFO 11-24 22:46:23 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:27,093 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:46:27,094 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> INFO 11-24 22:46:23 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> INFO 11-24 22:46:23 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> INFO 11-24 22:46:23 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> ERROR 11-24 22:46:23 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> INFO 11-24 22:46:23 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> INFO 11-24 22:46:23 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> INFO 11-24 22:46:23 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:27,095 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:46:27,143 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> INFO 11-24 22:46:24 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> INFO 11-24 22:46:24 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> INFO 11-24 22:46:24 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> ERROR 11-24 22:46:24 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> INFO 11-24 22:46:24 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> INFO 11-24 22:46:24 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> INFO 11-24 22:46:24 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:27,144 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:46:29,150 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> INFO 11-24 22:46:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> INFO 11-24 22:46:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> INFO 11-24 22:46:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> ERROR 11-24 22:46:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> INFO 11-24 22:46:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> INFO 11-24 22:46:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> INFO 11-24 22:46:25 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:29,150 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:46:35,119 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:46:35,120 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> INFO 11-24 22:46:31 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> INFO 11-24 22:46:31 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> INFO 11-24 22:46:31 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> ERROR 11-24 22:46:31 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> INFO 11-24 22:46:31 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> INFO 11-24 22:46:31 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> INFO 11-24 22:46:31 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:35,120 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> INFO 11-24 22:46:31 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> INFO 11-24 22:46:31 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> INFO 11-24 22:46:31 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> ERROR 11-24 22:46:31 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> INFO 11-24 22:46:31 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> INFO 11-24 22:46:31 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> INFO 11-24 22:46:31 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:35,121 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:46:35,170 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> INFO 11-24 22:46:31 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> INFO 11-24 22:46:31 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> INFO 11-24 22:46:31 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> ERROR 11-24 22:46:31 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> INFO 11-24 22:46:31 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> INFO 11-24 22:46:31 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> INFO 11-24 22:46:31 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:35,170 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:46:37,177 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> INFO 11-24 22:46:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> INFO 11-24 22:46:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> INFO 11-24 22:46:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> ERROR 11-24 22:46:33 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> INFO 11-24 22:46:33 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> INFO 11-24 22:46:33 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> INFO 11-24 22:46:33 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:37,177 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:46:43,146 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:39 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:39 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> ERROR 11-24 22:46:39 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:39 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:39 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:40 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:43,148 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:46:43,148 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:39 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:39 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> ERROR 11-24 22:46:39 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:39 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:39 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> INFO 11-24 22:46:39 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:43,148 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:46:43,197 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> INFO 11-24 22:46:39 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> INFO 11-24 22:46:39 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> INFO 11-24 22:46:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> ERROR 11-24 22:46:39 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> INFO 11-24 22:46:39 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> INFO 11-24 22:46:39 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> INFO 11-24 22:46:40 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:43,197 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:46:44,200 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> INFO 11-24 22:46:41 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> INFO 11-24 22:46:41 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> INFO 11-24 22:46:41 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> ERROR 11-24 22:46:41 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> INFO 11-24 22:46:41 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> INFO 11-24 22:46:41 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> INFO 11-24 22:46:41 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:44,201 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:46:51,221 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> INFO 11-24 22:46:47 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> INFO 11-24 22:46:47 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> INFO 11-24 22:46:47 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> ERROR 11-24 22:46:47 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> INFO 11-24 22:46:47 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> INFO 11-24 22:46:47 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> INFO 11-24 22:46:48 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:51,222 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:46:52,178 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:46:52,178 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:47 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:47 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:47 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> ERROR 11-24 22:46:47 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:47 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:47 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:48 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:46:52,179 >> 
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:47 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:47 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:47 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> ERROR 11-24 22:46:47 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:47 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:47 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> INFO 11-24 22:46:48 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:46:52,179 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:46:52,225 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> INFO 11-24 22:46:48 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> INFO 11-24 22:46:48 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> INFO 11-24 22:46:48 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> ERROR 11-24 22:46:48 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> INFO 11-24 22:46:48 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> INFO 11-24 22:46:48 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> INFO 11-24 22:46:49 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:46:52,225 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:46:58,244 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> INFO 11-24 22:46:55 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> INFO 11-24 22:46:55 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> INFO 11-24 22:46:55 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> ERROR 11-24 22:46:55 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> INFO 11-24 22:46:55 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> INFO 11-24 22:46:55 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> INFO 11-24 22:46:55 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:46:58,245 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:47:00,204 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:56 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:56 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:56 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> ERROR 11-24 22:46:56 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:56 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:56 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:57 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:00,205 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:47:00,205 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:56 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:56 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:56 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> ERROR 11-24 22:46:56 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:56 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:56 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> INFO 11-24 22:46:57 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:00,205 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:47:00,249 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> INFO 11-24 22:46:56 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> INFO 11-24 22:46:56 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> INFO 11-24 22:46:56 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> ERROR 11-24 22:46:56 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> INFO 11-24 22:46:56 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> INFO 11-24 22:46:56 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> INFO 11-24 22:46:56 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:00,249 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:47:05,266 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> INFO 11-24 22:47:02 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> INFO 11-24 22:47:02 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> INFO 11-24 22:47:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> ERROR 11-24 22:47:02 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> INFO 11-24 22:47:02 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> INFO 11-24 22:47:02 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> INFO 11-24 22:47:02 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:05,267 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:47:08,232 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:47:08,233 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:04 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:04 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:04 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> ERROR 11-24 22:47:04 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:04 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:04 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:05 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:08,233 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:04 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:04 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:04 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> ERROR 11-24 22:47:04 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:04 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:04 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> INFO 11-24 22:47:05 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:08,233 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:47:08,280 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> INFO 11-24 22:47:04 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> INFO 11-24 22:47:04 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> INFO 11-24 22:47:04 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> ERROR 11-24 22:47:04 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> INFO 11-24 22:47:04 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> INFO 11-24 22:47:04 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> INFO 11-24 22:47:05 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:08,280 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:47:12,292 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> INFO 11-24 22:47:09 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> INFO 11-24 22:47:09 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> INFO 11-24 22:47:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> ERROR 11-24 22:47:09 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> INFO 11-24 22:47:09 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> INFO 11-24 22:47:09 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> INFO 11-24 22:47:09 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:12,302 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:47:16,259 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:47:16,260 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> INFO 11-24 22:47:12 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> INFO 11-24 22:47:12 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> INFO 11-24 22:47:12 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> ERROR 11-24 22:47:12 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> INFO 11-24 22:47:12 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> INFO 11-24 22:47:12 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> INFO 11-24 22:47:13 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:16,262 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> INFO 11-24 22:47:12 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> INFO 11-24 22:47:12 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> INFO 11-24 22:47:12 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> ERROR 11-24 22:47:12 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> INFO 11-24 22:47:12 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> INFO 11-24 22:47:12 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> INFO 11-24 22:47:13 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:16,263 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:47:16,307 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> INFO 11-24 22:47:12 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> INFO 11-24 22:47:12 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> INFO 11-24 22:47:12 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> ERROR 11-24 22:47:12 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> INFO 11-24 22:47:12 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> INFO 11-24 22:47:12 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> INFO 11-24 22:47:13 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:16,308 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:47:19,325 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> INFO 11-24 22:47:16 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> INFO 11-24 22:47:16 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> INFO 11-24 22:47:16 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> ERROR 11-24 22:47:16 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> INFO 11-24 22:47:16 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> INFO 11-24 22:47:16 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> INFO 11-24 22:47:16 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:19,325 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:47:24,289 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> INFO 11-24 22:47:20 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> INFO 11-24 22:47:20 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> INFO 11-24 22:47:20 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> ERROR 11-24 22:47:20 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> INFO 11-24 22:47:20 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> INFO 11-24 22:47:20 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> INFO 11-24 22:47:21 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:24,289 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:47:24,297 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> INFO 11-24 22:47:20 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> INFO 11-24 22:47:20 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> INFO 11-24 22:47:20 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> ERROR 11-24 22:47:20 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> INFO 11-24 22:47:20 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> INFO 11-24 22:47:20 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> INFO 11-24 22:47:21 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:24,298 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:47:24,336 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> INFO 11-24 22:47:20 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> INFO 11-24 22:47:20 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> INFO 11-24 22:47:20 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> ERROR 11-24 22:47:20 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> INFO 11-24 22:47:20 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> INFO 11-24 22:47:20 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> INFO 11-24 22:47:21 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:24,337 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:47:26,346 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> INFO 11-24 22:47:23 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> INFO 11-24 22:47:23 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> INFO 11-24 22:47:23 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> ERROR 11-24 22:47:23 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> INFO 11-24 22:47:23 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> INFO 11-24 22:47:23 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> INFO 11-24 22:47:24 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:26,346 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:47:32,316 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> INFO 11-24 22:47:28 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> INFO 11-24 22:47:28 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> INFO 11-24 22:47:28 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> ERROR 11-24 22:47:28 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> INFO 11-24 22:47:28 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> INFO 11-24 22:47:28 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> INFO 11-24 22:47:29 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:32,317 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:47:32,322 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> INFO 11-24 22:47:28 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> INFO 11-24 22:47:28 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> INFO 11-24 22:47:28 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> ERROR 11-24 22:47:28 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> INFO 11-24 22:47:28 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> INFO 11-24 22:47:28 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> INFO 11-24 22:47:29 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:32,323 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:47:32,362 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> INFO 11-24 22:47:28 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> INFO 11-24 22:47:28 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> INFO 11-24 22:47:28 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> ERROR 11-24 22:47:28 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> INFO 11-24 22:47:28 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> INFO 11-24 22:47:28 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> INFO 11-24 22:47:29 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:32,363 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:47:34,370 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> INFO 11-24 22:47:30 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> INFO 11-24 22:47:30 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> INFO 11-24 22:47:30 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> ERROR 11-24 22:47:30 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> INFO 11-24 22:47:30 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> INFO 11-24 22:47:30 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> INFO 11-24 22:47:30 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:34,371 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:47:40,343 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> INFO 11-24 22:47:36 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> INFO 11-24 22:47:36 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> INFO 11-24 22:47:36 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> ERROR 11-24 22:47:36 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> INFO 11-24 22:47:36 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> INFO 11-24 22:47:36 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> INFO 11-24 22:47:37 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:40,344 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:47:40,349 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> INFO 11-24 22:47:36 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> INFO 11-24 22:47:36 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> INFO 11-24 22:47:36 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> ERROR 11-24 22:47:36 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> INFO 11-24 22:47:36 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> INFO 11-24 22:47:36 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> INFO 11-24 22:47:37 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:40,350 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:47:40,388 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> INFO 11-24 22:47:36 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> INFO 11-24 22:47:36 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> INFO 11-24 22:47:36 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> ERROR 11-24 22:47:36 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> INFO 11-24 22:47:36 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> INFO 11-24 22:47:36 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> INFO 11-24 22:47:37 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:40,388 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:47:42,396 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> INFO 11-24 22:47:38 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> INFO 11-24 22:47:38 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> INFO 11-24 22:47:38 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> ERROR 11-24 22:47:38 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> INFO 11-24 22:47:38 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> INFO 11-24 22:47:38 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> INFO 11-24 22:47:39 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:42,397 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:47:48,374 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> INFO 11-24 22:47:44 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> INFO 11-24 22:47:44 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> INFO 11-24 22:47:44 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> ERROR 11-24 22:47:44 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> INFO 11-24 22:47:44 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> INFO 11-24 22:47:44 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> INFO 11-24 22:47:45 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:48,375 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:47:48,376 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> INFO 11-24 22:47:44 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> INFO 11-24 22:47:44 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> INFO 11-24 22:47:44 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> ERROR 11-24 22:47:44 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> INFO 11-24 22:47:44 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> INFO 11-24 22:47:44 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> INFO 11-24 22:47:45 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:48,377 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:47:48,418 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> INFO 11-24 22:47:44 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> INFO 11-24 22:47:44 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> INFO 11-24 22:47:44 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> ERROR 11-24 22:47:44 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> INFO 11-24 22:47:44 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> INFO 11-24 22:47:44 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> INFO 11-24 22:47:45 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:48,419 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:47:50,422 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> INFO 11-24 22:47:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> INFO 11-24 22:47:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> INFO 11-24 22:47:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> ERROR 11-24 22:47:46 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> INFO 11-24 22:47:46 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> INFO 11-24 22:47:46 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> INFO 11-24 22:47:47 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:50,423 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:47:56,402 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> INFO 11-24 22:47:52 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> INFO 11-24 22:47:52 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> INFO 11-24 22:47:52 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> ERROR 11-24 22:47:52 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> INFO 11-24 22:47:52 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> INFO 11-24 22:47:52 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> INFO 11-24 22:47:53 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:47:56,403 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:47:56,403 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> INFO 11-24 22:47:52 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> INFO 11-24 22:47:52 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> INFO 11-24 22:47:52 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> ERROR 11-24 22:47:52 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> INFO 11-24 22:47:52 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> INFO 11-24 22:47:52 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> INFO 11-24 22:47:53 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:47:56,405 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:47:56,444 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> INFO 11-24 22:47:52 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> INFO 11-24 22:47:52 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> INFO 11-24 22:47:52 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> ERROR 11-24 22:47:52 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> INFO 11-24 22:47:52 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> INFO 11-24 22:47:52 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> INFO 11-24 22:47:53 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:47:56,444 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:47:58,447 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> INFO 11-24 22:47:54 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> INFO 11-24 22:47:54 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> INFO 11-24 22:47:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> ERROR 11-24 22:47:54 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> INFO 11-24 22:47:54 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> INFO 11-24 22:47:54 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> INFO 11-24 22:47:55 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:47:58,448 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:48:04,428 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:48:04,428 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> INFO 11-24 22:48:00 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> INFO 11-24 22:48:00 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> INFO 11-24 22:48:00 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> ERROR 11-24 22:48:00 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> INFO 11-24 22:48:00 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> INFO 11-24 22:48:00 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> INFO 11-24 22:48:01 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:04,428 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> INFO 11-24 22:48:00 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> INFO 11-24 22:48:00 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> INFO 11-24 22:48:00 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> ERROR 11-24 22:48:00 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> INFO 11-24 22:48:00 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> INFO 11-24 22:48:00 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> INFO 11-24 22:48:01 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:04,429 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:48:04,469 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> INFO 11-24 22:48:00 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> INFO 11-24 22:48:00 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> INFO 11-24 22:48:00 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> ERROR 11-24 22:48:00 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> INFO 11-24 22:48:00 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> INFO 11-24 22:48:00 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> INFO 11-24 22:48:01 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:04,469 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:48:05,472 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> INFO 11-24 22:48:02 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> INFO 11-24 22:48:02 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> INFO 11-24 22:48:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> ERROR 11-24 22:48:02 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> INFO 11-24 22:48:02 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> INFO 11-24 22:48:02 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> INFO 11-24 22:48:03 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:05,473 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:48:12,453 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:48:12,454 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:08 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:08 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:08 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> ERROR 11-24 22:48:08 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:08 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:08 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:09 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:12,455 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:08 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:08 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:08 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> ERROR 11-24 22:48:08 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:08 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:08 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> INFO 11-24 22:48:09 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:12,455 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:48:12,494 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:48:12,494 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:08 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:08 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:08 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> ERROR 11-24 22:48:08 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:08 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:08 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:09 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:12,495 >> 
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:09 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:09 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> ERROR 11-24 22:48:09 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:09 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:09 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> INFO 11-24 22:48:10 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:12,495 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:48:21,484 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> INFO 11-24 22:48:16 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> INFO 11-24 22:48:16 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> INFO 11-24 22:48:16 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> ERROR 11-24 22:48:16 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> INFO 11-24 22:48:16 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> INFO 11-24 22:48:16 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> INFO 11-24 22:48:17 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:21,486 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:48:21,487 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> INFO 11-24 22:48:16 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> INFO 11-24 22:48:16 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> INFO 11-24 22:48:16 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> ERROR 11-24 22:48:16 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> INFO 11-24 22:48:16 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> INFO 11-24 22:48:16 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> INFO 11-24 22:48:17 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:21,488 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:48:21,521 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> INFO 11-24 22:48:16 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> INFO 11-24 22:48:16 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> INFO 11-24 22:48:16 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> ERROR 11-24 22:48:16 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> INFO 11-24 22:48:16 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> INFO 11-24 22:48:16 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> INFO 11-24 22:48:17 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:21,521 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:48:21,523 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> INFO 11-24 22:48:16 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> INFO 11-24 22:48:16 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> INFO 11-24 22:48:16 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> ERROR 11-24 22:48:16 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> INFO 11-24 22:48:16 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> INFO 11-24 22:48:16 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> INFO 11-24 22:48:17 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:21,524 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:48:30,514 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:48:30,515 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> ERROR 11-24 22:48:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:30,516 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> ERROR 11-24 22:48:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> INFO 11-24 22:48:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:30,516 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:48:30,548 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> INFO 11-24 22:48:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> INFO 11-24 22:48:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> INFO 11-24 22:48:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> ERROR 11-24 22:48:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> INFO 11-24 22:48:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> INFO 11-24 22:48:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> INFO 11-24 22:48:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:30,548 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:48:30,556 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> INFO 11-24 22:48:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> INFO 11-24 22:48:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> INFO 11-24 22:48:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> ERROR 11-24 22:48:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> INFO 11-24 22:48:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> INFO 11-24 22:48:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> INFO 11-24 22:48:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:30,556 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:48:39,546 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> INFO 11-24 22:48:34 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> INFO 11-24 22:48:34 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> INFO 11-24 22:48:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> ERROR 11-24 22:48:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> INFO 11-24 22:48:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> INFO 11-24 22:48:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> INFO 11-24 22:48:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:39,548 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:48:39,549 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> INFO 11-24 22:48:34 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> INFO 11-24 22:48:34 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> INFO 11-24 22:48:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> ERROR 11-24 22:48:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> INFO 11-24 22:48:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> INFO 11-24 22:48:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> INFO 11-24 22:48:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:39,550 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:48:39,576 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> INFO 11-24 22:48:34 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> INFO 11-24 22:48:34 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> INFO 11-24 22:48:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> ERROR 11-24 22:48:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> INFO 11-24 22:48:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> INFO 11-24 22:48:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> INFO 11-24 22:48:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:39,576 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:48:39,580 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> INFO 11-24 22:48:34 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> INFO 11-24 22:48:34 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> INFO 11-24 22:48:34 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> ERROR 11-24 22:48:34 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> INFO 11-24 22:48:34 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> INFO 11-24 22:48:34 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> INFO 11-24 22:48:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:39,580 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:48:47,575 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> INFO 11-24 22:48:43 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> INFO 11-24 22:48:43 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> INFO 11-24 22:48:43 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> ERROR 11-24 22:48:43 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> INFO 11-24 22:48:43 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> INFO 11-24 22:48:43 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> INFO 11-24 22:48:44 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:47,576 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:48:48,578 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> INFO 11-24 22:48:43 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> INFO 11-24 22:48:43 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> INFO 11-24 22:48:43 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> ERROR 11-24 22:48:43 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> INFO 11-24 22:48:43 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> INFO 11-24 22:48:43 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> INFO 11-24 22:48:44 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:48,579 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:48:48,604 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> INFO 11-24 22:48:43 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> INFO 11-24 22:48:43 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> INFO 11-24 22:48:43 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> ERROR 11-24 22:48:43 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> INFO 11-24 22:48:43 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> INFO 11-24 22:48:43 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> INFO 11-24 22:48:44 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:48,604 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:48:48,613 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> INFO 11-24 22:48:43 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> INFO 11-24 22:48:43 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> INFO 11-24 22:48:43 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> ERROR 11-24 22:48:43 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> INFO 11-24 22:48:43 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> INFO 11-24 22:48:43 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> INFO 11-24 22:48:44 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:48,613 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:48:54,600 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> INFO 11-24 22:48:51 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> INFO 11-24 22:48:51 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> INFO 11-24 22:48:51 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> ERROR 11-24 22:48:51 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> INFO 11-24 22:48:51 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> INFO 11-24 22:48:51 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> INFO 11-24 22:48:52 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:48:54,601 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:48:56,605 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> INFO 11-24 22:48:52 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> INFO 11-24 22:48:52 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> INFO 11-24 22:48:52 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> ERROR 11-24 22:48:52 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> INFO 11-24 22:48:52 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> INFO 11-24 22:48:52 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> INFO 11-24 22:48:53 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:48:56,606 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:48:56,626 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> INFO 11-24 22:48:52 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> INFO 11-24 22:48:52 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> INFO 11-24 22:48:52 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> ERROR 11-24 22:48:52 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> INFO 11-24 22:48:52 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> INFO 11-24 22:48:52 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> INFO 11-24 22:48:53 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:48:56,626 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:48:56,635 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> INFO 11-24 22:48:52 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> INFO 11-24 22:48:52 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> INFO 11-24 22:48:52 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> ERROR 11-24 22:48:52 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> INFO 11-24 22:48:52 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> INFO 11-24 22:48:52 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> INFO 11-24 22:48:53 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:48:56,635 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:49:01,622 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> INFO 11-24 22:48:58 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> INFO 11-24 22:48:58 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> INFO 11-24 22:48:58 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> ERROR 11-24 22:48:58 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> INFO 11-24 22:48:58 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> INFO 11-24 22:48:58 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> INFO 11-24 22:48:59 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:01,623 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:49:04,632 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> INFO 11-24 22:49:00 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> INFO 11-24 22:49:00 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> INFO 11-24 22:49:00 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> ERROR 11-24 22:49:00 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> INFO 11-24 22:49:00 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> INFO 11-24 22:49:00 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> INFO 11-24 22:49:01 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:04,633 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:49:04,650 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> INFO 11-24 22:49:00 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> INFO 11-24 22:49:00 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> INFO 11-24 22:49:00 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> ERROR 11-24 22:49:00 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> INFO 11-24 22:49:00 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> INFO 11-24 22:49:00 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> INFO 11-24 22:49:01 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:04,651 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:49:04,659 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> INFO 11-24 22:49:00 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> INFO 11-24 22:49:00 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> INFO 11-24 22:49:00 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> ERROR 11-24 22:49:00 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> INFO 11-24 22:49:00 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> INFO 11-24 22:49:00 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> INFO 11-24 22:49:01 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:04,659 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:49:08,645 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> INFO 11-24 22:49:05 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> INFO 11-24 22:49:05 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> INFO 11-24 22:49:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> ERROR 11-24 22:49:05 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> INFO 11-24 22:49:05 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> INFO 11-24 22:49:05 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> INFO 11-24 22:49:06 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:08,646 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:49:12,660 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> INFO 11-24 22:49:08 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> INFO 11-24 22:49:08 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> INFO 11-24 22:49:08 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> ERROR 11-24 22:49:08 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> INFO 11-24 22:49:08 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> INFO 11-24 22:49:08 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> INFO 11-24 22:49:09 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:12,660 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:49:12,675 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> INFO 11-24 22:49:08 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> INFO 11-24 22:49:08 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> INFO 11-24 22:49:08 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> ERROR 11-24 22:49:08 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> INFO 11-24 22:49:08 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> INFO 11-24 22:49:08 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> INFO 11-24 22:49:09 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:12,675 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:49:12,682 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> INFO 11-24 22:49:08 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> INFO 11-24 22:49:08 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> INFO 11-24 22:49:08 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> ERROR 11-24 22:49:08 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> INFO 11-24 22:49:08 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> INFO 11-24 22:49:08 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> INFO 11-24 22:49:09 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:12,682 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:49:15,670 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> INFO 11-24 22:49:12 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> INFO 11-24 22:49:12 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> INFO 11-24 22:49:12 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> ERROR 11-24 22:49:12 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> INFO 11-24 22:49:12 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> INFO 11-24 22:49:12 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> INFO 11-24 22:49:13 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:15,670 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:49:20,686 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> INFO 11-24 22:49:16 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> INFO 11-24 22:49:16 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> INFO 11-24 22:49:16 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> ERROR 11-24 22:49:16 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> INFO 11-24 22:49:16 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> INFO 11-24 22:49:16 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> INFO 11-24 22:49:17 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:20,687 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:49:20,705 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> INFO 11-24 22:49:16 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> INFO 11-24 22:49:16 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> INFO 11-24 22:49:16 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> ERROR 11-24 22:49:16 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> INFO 11-24 22:49:16 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> INFO 11-24 22:49:16 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> INFO 11-24 22:49:17 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:20,705 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:49:20,706 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> INFO 11-24 22:49:16 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> INFO 11-24 22:49:16 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> INFO 11-24 22:49:16 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> ERROR 11-24 22:49:16 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> INFO 11-24 22:49:16 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> INFO 11-24 22:49:16 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> INFO 11-24 22:49:17 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:20,706 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:49:22,693 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> INFO 11-24 22:49:19 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> INFO 11-24 22:49:19 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> INFO 11-24 22:49:19 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> ERROR 11-24 22:49:19 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> INFO 11-24 22:49:19 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> INFO 11-24 22:49:19 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> INFO 11-24 22:49:20 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:22,693 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:49:28,712 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> INFO 11-24 22:49:24 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> INFO 11-24 22:49:24 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> INFO 11-24 22:49:24 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> ERROR 11-24 22:49:24 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> INFO 11-24 22:49:24 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> INFO 11-24 22:49:24 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> INFO 11-24 22:49:25 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:28,714 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:49:28,728 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:24 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:24 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:24 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> ERROR 11-24 22:49:24 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:24 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:24 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:25 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:28,729 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:49:28,729 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:24 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:24 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:24 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> ERROR 11-24 22:49:24 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:24 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:24 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> INFO 11-24 22:49:25 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:28,729 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:49:29,716 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> INFO 11-24 22:49:26 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> INFO 11-24 22:49:26 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> INFO 11-24 22:49:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> ERROR 11-24 22:49:26 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> INFO 11-24 22:49:26 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> INFO 11-24 22:49:26 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> INFO 11-24 22:49:27 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:29,717 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:49:36,740 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:49:36,740 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:32 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:32 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:32 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> ERROR 11-24 22:49:32 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:32 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:32 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:33 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:36,741 >> 
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> ERROR 11-24 22:49:33 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:33 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:33 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> INFO 11-24 22:49:34 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:36,741 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:49:36,751 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> INFO 11-24 22:49:32 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> INFO 11-24 22:49:32 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> INFO 11-24 22:49:32 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> ERROR 11-24 22:49:32 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> INFO 11-24 22:49:32 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> INFO 11-24 22:49:32 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> INFO 11-24 22:49:33 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:36,751 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:49:36,757 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> INFO 11-24 22:49:32 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> INFO 11-24 22:49:32 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> INFO 11-24 22:49:32 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> ERROR 11-24 22:49:32 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> INFO 11-24 22:49:32 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> INFO 11-24 22:49:32 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> INFO 11-24 22:49:33 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:36,757 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:49:44,782 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> INFO 11-24 22:49:40 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> INFO 11-24 22:49:40 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> INFO 11-24 22:49:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> ERROR 11-24 22:49:40 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> INFO 11-24 22:49:40 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> INFO 11-24 22:49:40 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> INFO 11-24 22:49:41 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:44,783 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:49:45,771 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:49:45,771 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:40 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:40 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> ERROR 11-24 22:49:40 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:40 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:40 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:41 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:45,772 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:40 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:40 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> ERROR 11-24 22:49:40 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:40 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:40 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> INFO 11-24 22:49:41 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:45,772 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:49:45,783 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> INFO 11-24 22:49:40 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> INFO 11-24 22:49:40 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> INFO 11-24 22:49:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> ERROR 11-24 22:49:40 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> INFO 11-24 22:49:40 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> INFO 11-24 22:49:40 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> INFO 11-24 22:49:41 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:45,784 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:49:51,802 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> INFO 11-24 22:49:48 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> INFO 11-24 22:49:48 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> INFO 11-24 22:49:48 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> ERROR 11-24 22:49:48 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> INFO 11-24 22:49:48 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> INFO 11-24 22:49:48 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> INFO 11-24 22:49:49 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:51,803 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:49:53,796 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:49:53,796 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:49 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:49 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:49 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> ERROR 11-24 22:49:49 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:49 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:49 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:50 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:49:53,797 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:49 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:49 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:49 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> ERROR 11-24 22:49:49 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:49 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:49 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> INFO 11-24 22:49:50 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:49:53,797 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:49:53,807 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> INFO 11-24 22:49:49 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> INFO 11-24 22:49:49 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> INFO 11-24 22:49:49 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> ERROR 11-24 22:49:49 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> INFO 11-24 22:49:49 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> INFO 11-24 22:49:49 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> INFO 11-24 22:49:50 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:49:53,808 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:49:58,823 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> INFO 11-24 22:49:55 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> INFO 11-24 22:49:55 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> INFO 11-24 22:49:55 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> ERROR 11-24 22:49:55 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> INFO 11-24 22:49:55 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> INFO 11-24 22:49:55 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> INFO 11-24 22:49:56 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:49:58,824 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:50:01,821 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> INFO 11-24 22:49:57 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> INFO 11-24 22:49:57 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> INFO 11-24 22:49:57 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> ERROR 11-24 22:49:57 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> INFO 11-24 22:49:57 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> INFO 11-24 22:49:57 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> INFO 11-24 22:49:58 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:01,822 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:50:01,823 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> INFO 11-24 22:49:57 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> INFO 11-24 22:49:57 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> INFO 11-24 22:49:57 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> ERROR 11-24 22:49:57 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> INFO 11-24 22:49:57 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> INFO 11-24 22:49:57 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> INFO 11-24 22:49:58 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:01,824 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:50:01,831 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> INFO 11-24 22:49:57 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> INFO 11-24 22:49:57 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> INFO 11-24 22:49:57 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> ERROR 11-24 22:49:57 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> INFO 11-24 22:49:57 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> INFO 11-24 22:49:57 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> INFO 11-24 22:49:58 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:01,831 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:50:05,849 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> INFO 11-24 22:50:02 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> INFO 11-24 22:50:02 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> INFO 11-24 22:50:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> ERROR 11-24 22:50:02 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> INFO 11-24 22:50:02 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> INFO 11-24 22:50:02 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> INFO 11-24 22:50:03 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:05,850 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:50:09,849 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> INFO 11-24 22:50:05 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> INFO 11-24 22:50:05 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> INFO 11-24 22:50:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> ERROR 11-24 22:50:05 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> INFO 11-24 22:50:05 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> INFO 11-24 22:50:05 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> INFO 11-24 22:50:06 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:09,849 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:50:09,849 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> INFO 11-24 22:50:05 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> INFO 11-24 22:50:05 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> INFO 11-24 22:50:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> ERROR 11-24 22:50:05 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> INFO 11-24 22:50:05 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> INFO 11-24 22:50:05 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> INFO 11-24 22:50:06 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:09,850 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:50:09,858 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> INFO 11-24 22:50:05 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> INFO 11-24 22:50:05 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> INFO 11-24 22:50:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> ERROR 11-24 22:50:05 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> INFO 11-24 22:50:05 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> INFO 11-24 22:50:05 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> INFO 11-24 22:50:06 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:09,859 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:50:12,872 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> INFO 11-24 22:50:09 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> INFO 11-24 22:50:09 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> INFO 11-24 22:50:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> ERROR 11-24 22:50:09 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> INFO 11-24 22:50:09 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> INFO 11-24 22:50:09 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> INFO 11-24 22:50:10 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:12,873 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:50:17,877 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> INFO 11-24 22:50:13 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> INFO 11-24 22:50:13 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> INFO 11-24 22:50:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> ERROR 11-24 22:50:13 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> INFO 11-24 22:50:13 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> INFO 11-24 22:50:13 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> INFO 11-24 22:50:14 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:17,878 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:50:17,878 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> INFO 11-24 22:50:13 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> INFO 11-24 22:50:13 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> INFO 11-24 22:50:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> ERROR 11-24 22:50:13 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> INFO 11-24 22:50:13 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> INFO 11-24 22:50:13 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> INFO 11-24 22:50:14 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:17,879 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:50:17,886 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> INFO 11-24 22:50:13 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> INFO 11-24 22:50:13 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> INFO 11-24 22:50:13 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> ERROR 11-24 22:50:13 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> INFO 11-24 22:50:13 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> INFO 11-24 22:50:13 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> INFO 11-24 22:50:14 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:17,886 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:50:19,894 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> INFO 11-24 22:50:16 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> INFO 11-24 22:50:16 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> INFO 11-24 22:50:16 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> ERROR 11-24 22:50:16 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> INFO 11-24 22:50:16 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> INFO 11-24 22:50:16 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> INFO 11-24 22:50:17 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:19,895 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:50:25,905 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:50:25,905 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:21 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:21 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:21 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> ERROR 11-24 22:50:21 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:21 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:21 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:22 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:25,906 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:21 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:21 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:21 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> ERROR 11-24 22:50:21 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:21 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:21 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> INFO 11-24 22:50:22 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:25,906 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:50:25,912 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> INFO 11-24 22:50:21 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> INFO 11-24 22:50:21 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> INFO 11-24 22:50:21 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> ERROR 11-24 22:50:21 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> INFO 11-24 22:50:21 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> INFO 11-24 22:50:21 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> INFO 11-24 22:50:22 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:25,912 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:50:27,917 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> INFO 11-24 22:50:23 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> INFO 11-24 22:50:23 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> INFO 11-24 22:50:23 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> ERROR 11-24 22:50:23 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> INFO 11-24 22:50:23 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> INFO 11-24 22:50:23 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> INFO 11-24 22:50:24 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:27,917 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:50:33,932 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> INFO 11-24 22:50:29 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> INFO 11-24 22:50:29 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> INFO 11-24 22:50:29 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> ERROR 11-24 22:50:29 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> INFO 11-24 22:50:29 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> INFO 11-24 22:50:29 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> INFO 11-24 22:50:30 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:33,933 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:50:33,933 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> INFO 11-24 22:50:29 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> INFO 11-24 22:50:29 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> INFO 11-24 22:50:29 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> ERROR 11-24 22:50:29 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> INFO 11-24 22:50:29 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> INFO 11-24 22:50:29 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> INFO 11-24 22:50:30 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:33,934 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:50:33,935 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> INFO 11-24 22:50:29 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> INFO 11-24 22:50:29 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> INFO 11-24 22:50:29 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> ERROR 11-24 22:50:29 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> INFO 11-24 22:50:29 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> INFO 11-24 22:50:29 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> INFO 11-24 22:50:30 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:33,936 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:50:35,941 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> INFO 11-24 22:50:31 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> INFO 11-24 22:50:31 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> INFO 11-24 22:50:31 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> ERROR 11-24 22:50:31 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> INFO 11-24 22:50:31 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> INFO 11-24 22:50:31 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> INFO 11-24 22:50:32 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:35,942 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:50:41,960 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> INFO 11-24 22:50:37 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> INFO 11-24 22:50:37 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> INFO 11-24 22:50:37 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> ERROR 11-24 22:50:37 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> INFO 11-24 22:50:37 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> INFO 11-24 22:50:37 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> INFO 11-24 22:50:38 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:41,961 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:50:41,962 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:50:41,962 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:37 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:37 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:37 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> ERROR 11-24 22:50:37 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:37 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:37 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:38 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:41,963 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:37 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:37 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:37 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> ERROR 11-24 22:50:37 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:37 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:37 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> INFO 11-24 22:50:38 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:41,963 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:50:42,965 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> INFO 11-24 22:50:39 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> INFO 11-24 22:50:39 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> INFO 11-24 22:50:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> ERROR 11-24 22:50:39 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> INFO 11-24 22:50:39 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> INFO 11-24 22:50:39 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> INFO 11-24 22:50:40 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:42,966 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:50:49,988 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:50:49,989 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:50:49,989 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:45 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:45 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:45 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> ERROR 11-24 22:50:45 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:45 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:45 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:46 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:49,989 >> 
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:45 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:45 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:45 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> ERROR 11-24 22:50:45 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:45 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:45 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> INFO 11-24 22:50:46 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:49,989 >> 
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> INFO 11-24 22:50:45 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> INFO 11-24 22:50:45 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> INFO 11-24 22:50:45 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> ERROR 11-24 22:50:45 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> INFO 11-24 22:50:45 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> INFO 11-24 22:50:45 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> INFO 11-24 22:50:46 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:49,990 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:50:50,993 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> INFO 11-24 22:50:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> INFO 11-24 22:50:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> INFO 11-24 22:50:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> ERROR 11-24 22:50:46 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> INFO 11-24 22:50:46 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> INFO 11-24 22:50:46 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> INFO 11-24 22:50:47 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:50,994 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:50:58,017 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:50:58,017 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:53 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:53 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> ERROR 11-24 22:50:53 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:53 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:53 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:54 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:50:58,018 >> 
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:53 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:53 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> ERROR 11-24 22:50:53 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:53 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:53 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> INFO 11-24 22:50:54 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:50:58,018 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:50:59,018 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:50:59,018 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:53 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:53 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> ERROR 11-24 22:50:53 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:53 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:53 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:54 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:50:59,019 >> 
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:54 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:54 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> ERROR 11-24 22:50:54 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:54 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:54 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> INFO 11-24 22:50:55 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:50:59,019 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:51:06,043 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> INFO 11-24 22:51:01 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> INFO 11-24 22:51:01 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> INFO 11-24 22:51:01 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> ERROR 11-24 22:51:01 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> INFO 11-24 22:51:01 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> INFO 11-24 22:51:01 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> INFO 11-24 22:51:02 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:06,043 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:51:06,044 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> INFO 11-24 22:51:01 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> INFO 11-24 22:51:01 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> INFO 11-24 22:51:01 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> ERROR 11-24 22:51:01 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> INFO 11-24 22:51:01 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> INFO 11-24 22:51:01 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> INFO 11-24 22:51:02 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:06,045 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:51:07,046 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:51:07,046 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:02 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:02 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> ERROR 11-24 22:51:03 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:03 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:03 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:03 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:07,046 >> 
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:02 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:02 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:02 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> ERROR 11-24 22:51:02 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:02 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:02 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> INFO 11-24 22:51:03 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:07,046 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:51:14,072 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:09 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:09 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:10 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> ERROR 11-24 22:51:10 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:10 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:10 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:10 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:14,073 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:51:14,073 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:09 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:09 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:09 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> ERROR 11-24 22:51:09 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:09 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:09 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> INFO 11-24 22:51:10 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:14,073 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:51:15,073 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:10 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:10 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:10 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> ERROR 11-24 22:51:11 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:11 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:11 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:11 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:15,074 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:51:15,074 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:10 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:10 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:10 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> ERROR 11-24 22:51:11 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:11 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:11 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> INFO 11-24 22:51:11 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:15,074 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:51:21,094 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> INFO 11-24 22:51:17 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> INFO 11-24 22:51:17 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> INFO 11-24 22:51:17 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> ERROR 11-24 22:51:17 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> INFO 11-24 22:51:17 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> INFO 11-24 22:51:17 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> INFO 11-24 22:51:18 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:21,095 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:51:22,100 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> INFO 11-24 22:51:17 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> INFO 11-24 22:51:17 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> INFO 11-24 22:51:17 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> ERROR 11-24 22:51:17 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> INFO 11-24 22:51:17 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> INFO 11-24 22:51:17 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> INFO 11-24 22:51:18 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:22,101 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:51:23,100 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> INFO 11-24 22:51:18 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> INFO 11-24 22:51:18 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> INFO 11-24 22:51:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> ERROR 11-24 22:51:19 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> INFO 11-24 22:51:19 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> INFO 11-24 22:51:19 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> INFO 11-24 22:51:19 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:23,100 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:51:23,101 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> INFO 11-24 22:51:18 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> INFO 11-24 22:51:18 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> INFO 11-24 22:51:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> ERROR 11-24 22:51:19 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> INFO 11-24 22:51:19 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> INFO 11-24 22:51:19 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> INFO 11-24 22:51:19 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:23,101 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:51:28,121 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> INFO 11-24 22:51:24 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> INFO 11-24 22:51:24 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> INFO 11-24 22:51:24 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> ERROR 11-24 22:51:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> INFO 11-24 22:51:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> INFO 11-24 22:51:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> INFO 11-24 22:51:25 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:28,121 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:51:29,124 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> INFO 11-24 22:51:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> INFO 11-24 22:51:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> INFO 11-24 22:51:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> ERROR 11-24 22:51:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> INFO 11-24 22:51:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> INFO 11-24 22:51:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> INFO 11-24 22:51:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:29,125 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:51:31,131 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> ERROR 11-24 22:51:27 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:31,132 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:51:31,132 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> ERROR 11-24 22:51:27 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> INFO 11-24 22:51:27 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:31,132 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:51:35,148 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> INFO 11-24 22:51:32 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> INFO 11-24 22:51:32 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> INFO 11-24 22:51:32 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> ERROR 11-24 22:51:32 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> INFO 11-24 22:51:32 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> INFO 11-24 22:51:32 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> INFO 11-24 22:51:32 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:35,149 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:51:36,148 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> INFO 11-24 22:51:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> INFO 11-24 22:51:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> INFO 11-24 22:51:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> ERROR 11-24 22:51:33 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> INFO 11-24 22:51:33 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> INFO 11-24 22:51:33 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> INFO 11-24 22:51:33 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:36,149 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:51:39,156 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> INFO 11-24 22:51:35 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> INFO 11-24 22:51:35 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> INFO 11-24 22:51:35 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> ERROR 11-24 22:51:35 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> INFO 11-24 22:51:35 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> INFO 11-24 22:51:35 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> INFO 11-24 22:51:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:39,157 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:51:39,158 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> INFO 11-24 22:51:35 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> INFO 11-24 22:51:35 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> INFO 11-24 22:51:35 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> ERROR 11-24 22:51:35 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> INFO 11-24 22:51:35 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> INFO 11-24 22:51:35 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> INFO 11-24 22:51:35 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:39,158 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:51:42,168 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> INFO 11-24 22:51:39 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> INFO 11-24 22:51:39 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> INFO 11-24 22:51:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> ERROR 11-24 22:51:39 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> INFO 11-24 22:51:39 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> INFO 11-24 22:51:39 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> INFO 11-24 22:51:39 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:42,169 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:51:43,170 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> INFO 11-24 22:51:40 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> INFO 11-24 22:51:40 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> INFO 11-24 22:51:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> ERROR 11-24 22:51:40 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> INFO 11-24 22:51:40 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> INFO 11-24 22:51:40 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> INFO 11-24 22:51:40 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:43,171 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:51:46,180 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> INFO 11-24 22:51:43 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> INFO 11-24 22:51:43 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> INFO 11-24 22:51:43 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> ERROR 11-24 22:51:43 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> INFO 11-24 22:51:43 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> INFO 11-24 22:51:43 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> INFO 11-24 22:51:43 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:46,180 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:51:47,182 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> INFO 11-24 22:51:43 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> INFO 11-24 22:51:43 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> INFO 11-24 22:51:43 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> ERROR 11-24 22:51:43 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> INFO 11-24 22:51:43 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> INFO 11-24 22:51:43 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> INFO 11-24 22:51:43 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:47,183 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:51:49,189 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> INFO 11-24 22:51:46 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> INFO 11-24 22:51:46 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> INFO 11-24 22:51:46 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> ERROR 11-24 22:51:46 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> INFO 11-24 22:51:46 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> INFO 11-24 22:51:46 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> INFO 11-24 22:51:46 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:49,190 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:51:50,193 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> INFO 11-24 22:51:47 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> INFO 11-24 22:51:47 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> INFO 11-24 22:51:47 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> ERROR 11-24 22:51:47 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> INFO 11-24 22:51:47 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> INFO 11-24 22:51:47 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> INFO 11-24 22:51:47 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:50,193 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:51:53,202 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> INFO 11-24 22:51:50 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> INFO 11-24 22:51:50 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> INFO 11-24 22:51:50 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> ERROR 11-24 22:51:50 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> INFO 11-24 22:51:50 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> INFO 11-24 22:51:50 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> INFO 11-24 22:51:50 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:51:53,203 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:51:54,205 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> INFO 11-24 22:51:51 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> INFO 11-24 22:51:51 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> INFO 11-24 22:51:51 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> ERROR 11-24 22:51:51 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> INFO 11-24 22:51:51 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> INFO 11-24 22:51:51 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> INFO 11-24 22:51:51 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:51:54,206 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:51:56,212 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> INFO 11-24 22:51:53 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> INFO 11-24 22:51:53 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> INFO 11-24 22:51:53 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> ERROR 11-24 22:51:53 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> INFO 11-24 22:51:53 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> INFO 11-24 22:51:53 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> INFO 11-24 22:51:53 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:51:56,212 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:51:57,214 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> INFO 11-24 22:51:54 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> INFO 11-24 22:51:54 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> INFO 11-24 22:51:54 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> ERROR 11-24 22:51:54 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> INFO 11-24 22:51:54 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> INFO 11-24 22:51:54 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> INFO 11-24 22:51:54 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:51:57,215 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:52:00,223 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> INFO 11-24 22:51:57 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> INFO 11-24 22:51:57 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> INFO 11-24 22:51:57 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> ERROR 11-24 22:51:57 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> INFO 11-24 22:51:57 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> INFO 11-24 22:51:57 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> INFO 11-24 22:51:57 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:00,224 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:52:01,227 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> INFO 11-24 22:51:58 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> INFO 11-24 22:51:58 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> INFO 11-24 22:51:58 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> ERROR 11-24 22:51:58 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> INFO 11-24 22:51:58 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> INFO 11-24 22:51:58 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> INFO 11-24 22:51:58 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:01,228 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:52:03,235 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> INFO 11-24 22:52:00 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> INFO 11-24 22:52:00 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> INFO 11-24 22:52:00 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> ERROR 11-24 22:52:00 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> INFO 11-24 22:52:00 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> INFO 11-24 22:52:00 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> INFO 11-24 22:52:00 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:03,236 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:52:04,236 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> INFO 11-24 22:52:01 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> INFO 11-24 22:52:01 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> INFO 11-24 22:52:01 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> ERROR 11-24 22:52:01 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> INFO 11-24 22:52:01 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> INFO 11-24 22:52:01 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> INFO 11-24 22:52:01 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:04,237 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:52:07,247 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> INFO 11-24 22:52:04 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> INFO 11-24 22:52:04 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> INFO 11-24 22:52:04 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> ERROR 11-24 22:52:04 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> INFO 11-24 22:52:04 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> INFO 11-24 22:52:04 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> INFO 11-24 22:52:04 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:07,247 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:52:08,253 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> INFO 11-24 22:52:05 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> INFO 11-24 22:52:05 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> INFO 11-24 22:52:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> ERROR 11-24 22:52:05 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> INFO 11-24 22:52:05 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> INFO 11-24 22:52:05 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> INFO 11-24 22:52:05 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:08,254 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:52:10,260 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> INFO 11-24 22:52:07 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> INFO 11-24 22:52:07 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> INFO 11-24 22:52:07 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> ERROR 11-24 22:52:07 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> INFO 11-24 22:52:07 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> INFO 11-24 22:52:07 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> INFO 11-24 22:52:07 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:10,261 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:52:11,259 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> INFO 11-24 22:52:08 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> INFO 11-24 22:52:08 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> INFO 11-24 22:52:08 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> ERROR 11-24 22:52:08 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> INFO 11-24 22:52:08 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> INFO 11-24 22:52:08 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> INFO 11-24 22:52:08 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:11,260 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:52:14,269 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> INFO 11-24 22:52:11 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> INFO 11-24 22:52:11 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> INFO 11-24 22:52:11 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> ERROR 11-24 22:52:11 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> INFO 11-24 22:52:11 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> INFO 11-24 22:52:11 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> INFO 11-24 22:52:11 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:14,270 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:52:15,274 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> INFO 11-24 22:52:12 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> INFO 11-24 22:52:12 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> INFO 11-24 22:52:12 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> ERROR 11-24 22:52:12 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> INFO 11-24 22:52:12 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> INFO 11-24 22:52:12 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> INFO 11-24 22:52:12 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:15,274 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:52:17,280 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> INFO 11-24 22:52:14 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> INFO 11-24 22:52:14 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> INFO 11-24 22:52:14 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> ERROR 11-24 22:52:14 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> INFO 11-24 22:52:14 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> INFO 11-24 22:52:14 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> INFO 11-24 22:52:14 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:17,280 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:52:18,286 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> INFO 11-24 22:52:15 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> INFO 11-24 22:52:15 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> INFO 11-24 22:52:15 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> ERROR 11-24 22:52:15 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> INFO 11-24 22:52:15 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> INFO 11-24 22:52:15 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> INFO 11-24 22:52:15 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:18,286 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:52:21,293 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> INFO 11-24 22:52:18 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> INFO 11-24 22:52:18 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> INFO 11-24 22:52:18 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> ERROR 11-24 22:52:18 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> INFO 11-24 22:52:18 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> INFO 11-24 22:52:18 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> INFO 11-24 22:52:18 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:21,293 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:52:22,297 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> INFO 11-24 22:52:19 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> INFO 11-24 22:52:19 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> INFO 11-24 22:52:19 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> ERROR 11-24 22:52:19 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> INFO 11-24 22:52:19 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> INFO 11-24 22:52:19 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> INFO 11-24 22:52:19 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:22,298 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:52:24,302 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> INFO 11-24 22:52:21 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> INFO 11-24 22:52:21 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> INFO 11-24 22:52:21 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> ERROR 11-24 22:52:21 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> INFO 11-24 22:52:21 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> INFO 11-24 22:52:21 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> INFO 11-24 22:52:21 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:24,303 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:52:25,307 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> INFO 11-24 22:52:22 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> INFO 11-24 22:52:22 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> INFO 11-24 22:52:22 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> ERROR 11-24 22:52:22 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> INFO 11-24 22:52:22 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> INFO 11-24 22:52:22 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> INFO 11-24 22:52:22 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:25,308 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:52:28,314 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> INFO 11-24 22:52:25 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> INFO 11-24 22:52:25 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> INFO 11-24 22:52:25 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> ERROR 11-24 22:52:25 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> INFO 11-24 22:52:25 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> INFO 11-24 22:52:25 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> INFO 11-24 22:52:25 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:28,315 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:52:29,317 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> INFO 11-24 22:52:26 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> INFO 11-24 22:52:26 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> INFO 11-24 22:52:26 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> ERROR 11-24 22:52:26 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> INFO 11-24 22:52:26 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> INFO 11-24 22:52:26 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> INFO 11-24 22:52:26 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:29,318 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:52:31,322 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> INFO 11-24 22:52:28 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> INFO 11-24 22:52:28 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> INFO 11-24 22:52:28 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> ERROR 11-24 22:52:28 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> INFO 11-24 22:52:28 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> INFO 11-24 22:52:28 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> INFO 11-24 22:52:28 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:31,323 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:52:32,327 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> INFO 11-24 22:52:29 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> INFO 11-24 22:52:29 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> INFO 11-24 22:52:29 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> ERROR 11-24 22:52:29 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> INFO 11-24 22:52:29 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> INFO 11-24 22:52:29 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> INFO 11-24 22:52:29 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:32,328 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:52:35,340 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> INFO 11-24 22:52:32 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> INFO 11-24 22:52:32 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> INFO 11-24 22:52:32 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> ERROR 11-24 22:52:32 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> INFO 11-24 22:52:32 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> INFO 11-24 22:52:32 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> INFO 11-24 22:52:32 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:35,340 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:52:36,340 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> INFO 11-24 22:52:33 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> INFO 11-24 22:52:33 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> INFO 11-24 22:52:33 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> ERROR 11-24 22:52:33 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> INFO 11-24 22:52:33 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> INFO 11-24 22:52:33 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> INFO 11-24 22:52:33 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:36,340 >> 
[ERROR|vllm_server.py:207:2471912] 2024-11-24 22:52:38,344 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> INFO 11-24 22:52:35 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> INFO 11-24 22:52:35 api_server.py:150] args: Namespace(host='0.0.0.0', port=60991, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318313, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> INFO 11-24 22:52:35 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318313)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> ERROR 11-24 22:52:35 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> INFO 11-24 22:52:35 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> INFO 11-24 22:52:35 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> INFO 11-24 22:52:36 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     engine = cls(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471912] 2024-11-24 22:52:38,345 >> 
[ERROR|vllm_server.py:207:2471911] 2024-11-24 22:52:39,349 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> INFO 11-24 22:52:36 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> INFO 11-24 22:52:36 api_server.py:150] args: Namespace(host='0.0.0.0', port=4779, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318213, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> INFO 11-24 22:52:36 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318213)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> ERROR 11-24 22:52:36 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> INFO 11-24 22:52:36 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> INFO 11-24 22:52:36 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> INFO 11-24 22:52:36 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     engine = cls(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471911] 2024-11-24 22:52:39,350 >> 
[ERROR|vllm_server.py:207:2471913] 2024-11-24 22:52:42,359 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> INFO 11-24 22:52:39 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> INFO 11-24 22:52:39 api_server.py:150] args: Namespace(host='0.0.0.0', port=57413, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318413, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> INFO 11-24 22:52:39 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318413)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> ERROR 11-24 22:52:39 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> INFO 11-24 22:52:39 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> INFO 11-24 22:52:39 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> INFO 11-24 22:52:39 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     engine = cls(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471913] 2024-11-24 22:52:42,360 >> 
[ERROR|vllm_server.py:207:2471914] 2024-11-24 22:52:43,362 >> vLLM process has exited. Restarting...
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> vLLM Server log:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> INFO 11-24 22:52:40 api_server.py:149] vLLM API server version 0.4.0.post1
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> INFO 11-24 22:52:40 api_server.py:150] args: Namespace(host='0.0.0.0', port=8417, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='rho-1b-sft-GSM8K', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='bfloat16', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=2746318513, swap_space=8, gpu_memory_utilization=0.87, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=512, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> ################################################# UsageContext.OPENAI_API_SERVER
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> INFO 11-24 22:52:40 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='rho-1b-sft-GSM8K', tokenizer='rho-1b-sft-GSM8K', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=2746318513)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> ERROR 11-24 22:52:40 pynccl.py:53] Failed to load NCCL library from libnccl.so.2 .It is expected if you are not running on NVIDIA/AMD GPUs.Otherwise please set the environment variable VLLM_NCCL_SO_PATH to point to the correct nccl library path.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> INFO 11-24 22:52:40 pynccl_utils.py:17] Failed to import NCCL library: libnccl.so.2: cannot open shared object file: No such file or directory
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> INFO 11-24 22:52:40 pynccl_utils.py:18] It is expected if you are not running on NVIDIA GPUs.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> INFO 11-24 22:52:40 selector.py:16] Using FlashAttention backend.
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> Traceback (most recent call last):
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     return _run_code(code, main_globals, None,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Compiler/gcccore/python/3.10.13/lib/python3.10/runpy.py", line 86, in _run_code
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     exec(code, run_globals)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py", line 158, in <module>
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     engine = AsyncLLMEngine.from_engine_args(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 348, in from_engine_args
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     engine = cls(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 311, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     self.engine = self._init_engine(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py", line 422, in _init_engine
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     return engine_class(*args, **kwargs)
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/engine/llm_engine.py", line 110, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     self.model_executor = executor_class(model_config, cache_config,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 37, in __init__
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     self._init_worker()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/executor/gpu_executor.py", line 66, in _init_worker
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     self.driver_worker.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/worker.py", line 107, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     self.model_runner.load_model()
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/worker/model_runner.py", line 95, in load_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     self.model = get_model(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/model_loader.py", line 101, in get_model
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     model.load_weights(model_config.model, model_config.download_dir,
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/models/llama.py", line 377, in load_weights
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     for name, loaded_weight in hf_model_weights_iterator(
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>   File "/lustre06/project/6002409/imadlak/program/VinePPO/venv/lib/python3.10/site-packages/vllm/model_executor/weight_utils.py", line 265, in hf_model_weights_iterator
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >>     with safe_open(st_file, framework="pt") as f:
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> safetensors_rust.SafetensorError: Error while deserializing header: HeaderTooLarge
[ERROR|vllm_server.py:211:2471914] 2024-11-24 22:52:43,363 >> 
slurmstepd: error: *** JOB 37233326 ON ng30811 CANCELLED AT 2024-11-25T03:52:45 DUE TO TIME LIMIT ***
